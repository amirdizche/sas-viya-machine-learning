{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swat as sw\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Credit Card data\n",
    "df = pd.read_csv('../gan-testing/data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 29)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['Time', 'Class'], axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_X = df.drop(['Amount'], axis=1)\n",
    "orig_y = df.loc[:,\"Amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "orig_X_train, orig_X_test, orig_y_train, orig_y_test = train_test_split(orig_X, orig_y, test_size=0.3, random_state=123)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\R2_score on train dataset :  0.9109251064569308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "reg = LinearRegression().fit(orig_X_train, orig_y_train)\n",
    "predict_train=reg.predict(orig_X_train)\n",
    "r2_train=r2_score(orig_y_train, predict_train)\n",
    "print('\\R2_score on train dataset : ', r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\R2_score on test dataset :  0.9317383074771152\n"
     ]
    }
   ],
   "source": [
    "# predict the target on the test dataset\n",
    "predict_test = reg.predict(orig_X_test)\n",
    "#print('\\nTarget on test data',predict_test) \n",
    " \n",
    "r2_test=r2_score(orig_y_test, predict_test)\n",
    "print('\\R2_score on test dataset : ', r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199364, 29)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_GAN = pd.concat([orig_X_train, orig_y_train],axis=1)\n",
    "cc_GAN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
       "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
       "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_GAN.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sw.CAS('dl2073.clstr.rnd.sas.com',33789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Added action set 'generativeAdversarialNet'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; actionset</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>generativeAdversarialNet</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.504s</span> &#183; <span class=\"cas-user\">user 2.16s</span> &#183; <span class=\"cas-sys\">sys 2.33s</span> &#183; <span class=\"cas-memory\">mem 0.222MB</span></small></p>"
      ],
      "text/plain": [
       "[actionset]\n",
       "\n",
       " 'generativeAdversarialNet'\n",
       "\n",
       "+ Elapsed: 0.504s, user: 2.16s, sys: 2.33s, mem: 0.222mb"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.loadactionset('generativeAdversarialNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table CC_DATA in caslib CASUSER(alphel).\n",
      "NOTE: The table CC_DATA has been created in caslib CASUSER(alphel) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; caslib</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASUSER(alphel)</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; tableName</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CC_DATA</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; casTable</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASTable('CC_DATA', caslib='CASUSER(alphel)')</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 2.99s</span> &#183; <span class=\"cas-user\">user 3.44s</span> &#183; <span class=\"cas-sys\">sys 0.706s</span> &#183; <span class=\"cas-memory\">mem 130MB</span></small></p>"
      ],
      "text/plain": [
       "[caslib]\n",
       "\n",
       " 'CASUSER(alphel)'\n",
       "\n",
       "[tableName]\n",
       "\n",
       " 'CC_DATA'\n",
       "\n",
       "[casTable]\n",
       "\n",
       " CASTable('CC_DATA', caslib='CASUSER(alphel)')\n",
       "\n",
       "+ Elapsed: 2.99s, user: 3.44s, sys: 0.706s, mem: 130mb"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.upload(cc_GAN, casout=dict(name='cc_data', replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table CEN in caslib CASUSER(alphel).\n",
      "NOTE: The table CEN has been created in caslib CASUSER(alphel) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VarName</th>\n",
       "      <th>Centroid_i</th>\n",
       "      <th>weight</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amount</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05646</td>\n",
       "      <td>74.05</td>\n",
       "      <td>25.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amount</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05167</td>\n",
       "      <td>79.17</td>\n",
       "      <td>28.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amount</td>\n",
       "      <td>3</td>\n",
       "      <td>0.11023</td>\n",
       "      <td>32.76</td>\n",
       "      <td>12.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amount</td>\n",
       "      <td>4</td>\n",
       "      <td>0.08512</td>\n",
       "      <td>43.38</td>\n",
       "      <td>16.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amount</td>\n",
       "      <td>5</td>\n",
       "      <td>0.25107</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>V28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.24767</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>V28</td>\n",
       "      <td>5</td>\n",
       "      <td>0.12603</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>V28</td>\n",
       "      <td>6</td>\n",
       "      <td>0.12104</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>V28</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00903</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>V28</td>\n",
       "      <td>8</td>\n",
       "      <td>0.13324</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    VarName  Centroid_i   weight   Mean    Std\n",
       "0    Amount           1  0.05646  74.05  25.22\n",
       "1    Amount           2  0.05167  79.17  28.17\n",
       "2    Amount           3  0.11023  32.76  12.24\n",
       "3    Amount           4  0.08512  43.38  16.88\n",
       "4    Amount           5  0.25107   2.56   2.39\n",
       "..      ...         ...      ...    ...    ...\n",
       "201     V28           4  0.24767  -0.01   0.04\n",
       "202     V28           5  0.12603   0.08   0.12\n",
       "203     V28           6  0.12104   0.06   0.11\n",
       "204     V28           7  0.00903  -0.19   2.83\n",
       "205     V28           8  0.13324  -0.03   0.19\n",
       "\n",
       "[206 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cen = pd.read_csv(\"../gan-testing/data/cc_centroids.csv\")\n",
    "s.upload(cen, casout=dict(name='cen', replace=True))\n",
    "cen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using device: GPU 0.\n",
      "NOTE: Epoch i=1, ae_loss=  0.1391.\n",
      "NOTE: Epoch i=2, ae_loss=  0.1334.\n",
      "NOTE: Epoch i=3, ae_loss=  0.1281.\n",
      "NOTE: Epoch i=4, ae_loss=  0.1250.\n",
      "NOTE: Epoch i=5, ae_loss=  0.1250.\n",
      "NOTE: Epoch i=6, ae_loss=  0.1213.\n",
      "NOTE: Epoch i=7, ae_loss=  0.1202.\n",
      "NOTE: Epoch i=8, ae_loss=  0.1183.\n",
      "NOTE: Epoch i=9, ae_loss=  0.1171.\n",
      "NOTE: Epoch i=10, ae_loss=  0.1160.\n",
      "NOTE: Epoch i=11, ae_loss=  0.1126.\n",
      "NOTE: Epoch i=12, ae_loss=  0.1110.\n",
      "NOTE: Epoch i=13, ae_loss=  0.1063.\n",
      "NOTE: Epoch i=14, ae_loss=  0.1040.\n",
      "NOTE: Epoch i=15, ae_loss=  0.1014.\n",
      "NOTE: Epoch i=16, ae_loss=  0.0982.\n",
      "NOTE: Epoch i=17, ae_loss=  0.0963.\n",
      "NOTE: Epoch i=18, ae_loss=  0.0943.\n",
      "NOTE: Epoch i=19, ae_loss=  0.0918.\n",
      "NOTE: Epoch i=20, ae_loss=  0.0895.\n",
      "NOTE: Epoch i=21, ae_loss=  0.0903.\n",
      "NOTE: Epoch i=22, ae_loss=  0.0866.\n",
      "NOTE: Epoch i=23, ae_loss=  0.0852.\n",
      "NOTE: Epoch i=24, ae_loss=  0.0829.\n",
      "NOTE: Epoch i=25, ae_loss=  0.0823.\n",
      "NOTE: Epoch i=26, ae_loss=  0.0813.\n",
      "NOTE: Epoch i=27, ae_loss=  0.0809.\n",
      "NOTE: Epoch i=28, ae_loss=  0.0799.\n",
      "NOTE: Epoch i=29, ae_loss=  0.0772.\n",
      "NOTE: Epoch i=30, ae_loss=  0.0764.\n",
      "NOTE: Epoch i=31, ae_loss=  0.0780.\n",
      "NOTE: Epoch i=32, ae_loss=  0.0747.\n",
      "NOTE: Epoch i=33, ae_loss=  0.0737.\n",
      "NOTE: Epoch i=34, ae_loss=  0.0720.\n",
      "NOTE: Epoch i=35, ae_loss=  0.0720.\n",
      "NOTE: Epoch i=36, ae_loss=  0.0699.\n",
      "NOTE: Epoch i=37, ae_loss=  0.0712.\n",
      "NOTE: Epoch i=38, ae_loss=  0.0683.\n",
      "NOTE: Epoch i=39, ae_loss=  0.0691.\n",
      "NOTE: Epoch i=40, ae_loss=  0.0686.\n",
      "NOTE: Epoch i=41, ae_loss=  0.0669.\n",
      "NOTE: Epoch i=42, ae_loss=  0.0675.\n",
      "NOTE: Epoch i=43, ae_loss=  0.0661.\n",
      "NOTE: Epoch i=44, ae_loss=  0.0658.\n",
      "NOTE: Epoch i=45, ae_loss=  0.0654.\n",
      "NOTE: Epoch i=46, ae_loss=  0.0641.\n",
      "NOTE: Epoch i=47, ae_loss=  0.0643.\n",
      "NOTE: Epoch i=48, ae_loss=  0.0641.\n",
      "NOTE: Epoch i=49, ae_loss=  0.0633.\n",
      "NOTE: Epoch i=50, ae_loss=  0.0624.\n",
      "NOTE: Epoch i=51, ae_loss=  0.0623.\n",
      "NOTE: Epoch i=52, ae_loss=  0.0611.\n",
      "NOTE: Epoch i=53, ae_loss=  0.0618.\n",
      "NOTE: Epoch i=54, ae_loss=  0.0618.\n",
      "NOTE: Epoch i=55, ae_loss=  0.0601.\n",
      "NOTE: Epoch i=56, ae_loss=  0.0592.\n",
      "NOTE: Epoch i=57, ae_loss=  0.0608.\n",
      "NOTE: Epoch i=58, ae_loss=  0.0589.\n",
      "NOTE: Epoch i=59, ae_loss=  0.0593.\n",
      "NOTE: Epoch i=60, ae_loss=  0.0578.\n",
      "NOTE: Epoch i=61, ae_loss=  0.0588.\n",
      "NOTE: Epoch i=62, ae_loss=  0.0596.\n",
      "NOTE: Epoch i=63, ae_loss=  0.0576.\n",
      "NOTE: Epoch i=64, ae_loss=  0.0572.\n",
      "NOTE: Epoch i=65, ae_loss=  0.0569.\n",
      "NOTE: Epoch i=66, ae_loss=  0.0563.\n",
      "NOTE: Epoch i=67, ae_loss=  0.0554.\n",
      "NOTE: Epoch i=68, ae_loss=  0.0566.\n",
      "NOTE: Epoch i=69, ae_loss=  0.0565.\n",
      "NOTE: Epoch i=70, ae_loss=  0.0556.\n",
      "NOTE: Epoch i=71, ae_loss=  0.0569.\n",
      "NOTE: Epoch i=72, ae_loss=  0.0557.\n",
      "NOTE: Epoch i=73, ae_loss=  0.0546.\n",
      "NOTE: Epoch i=74, ae_loss=  0.0553.\n",
      "NOTE: Epoch i=75, ae_loss=  0.0547.\n",
      "NOTE: Epoch i=76, ae_loss=  0.0543.\n",
      "NOTE: Epoch i=77, ae_loss=  0.0535.\n",
      "NOTE: Epoch i=78, ae_loss=  0.0530.\n",
      "NOTE: Epoch i=79, ae_loss=  0.0520.\n",
      "NOTE: Epoch i=80, ae_loss=  0.0537.\n",
      "NOTE: Epoch i=81, ae_loss=  0.0521.\n",
      "NOTE: Epoch i=82, ae_loss=  0.0532.\n",
      "NOTE: Epoch i=83, ae_loss=  0.0523.\n",
      "NOTE: Epoch i=84, ae_loss=  0.0532.\n",
      "NOTE: Epoch i=85, ae_loss=  0.0523.\n",
      "NOTE: Epoch i=86, ae_loss=  0.0510.\n",
      "NOTE: Epoch i=87, ae_loss=  0.0504.\n",
      "NOTE: Epoch i=88, ae_loss=  0.0518.\n",
      "NOTE: Epoch i=89, ae_loss=  0.0510.\n",
      "NOTE: Epoch i=90, ae_loss=  0.0491.\n",
      "NOTE: Epoch i=91, ae_loss=  0.0494.\n",
      "NOTE: Epoch i=92, ae_loss=  0.0501.\n",
      "NOTE: Epoch i=93, ae_loss=  0.0496.\n",
      "NOTE: Epoch i=94, ae_loss=  0.0508.\n",
      "NOTE: Epoch i=95, ae_loss=  0.0498.\n",
      "NOTE: Epoch i=96, ae_loss=  0.0493.\n",
      "NOTE: Epoch i=97, ae_loss=  0.0494.\n",
      "NOTE: Epoch i=98, ae_loss=  0.0480.\n",
      "NOTE: Epoch i=99, ae_loss=  0.0488.\n",
      "NOTE: Epoch i=100, ae_loss=  0.0486.\n",
      "NOTE: Epoch i=101, ae_loss=  0.0477.\n",
      "NOTE: Epoch i=102, ae_loss=  0.0479.\n",
      "NOTE: Epoch i=103, ae_loss=  0.0471.\n",
      "NOTE: Epoch i=104, ae_loss=  0.0482.\n",
      "NOTE: Epoch i=105, ae_loss=  0.0481.\n",
      "NOTE: Epoch i=106, ae_loss=  0.0474.\n",
      "NOTE: Epoch i=107, ae_loss=  0.0474.\n",
      "NOTE: Epoch i=108, ae_loss=  0.0471.\n",
      "NOTE: Epoch i=109, ae_loss=  0.0465.\n",
      "NOTE: Epoch i=110, ae_loss=  0.0461.\n",
      "NOTE: Epoch i=111, ae_loss=  0.0468.\n",
      "NOTE: Epoch i=112, ae_loss=  0.0458.\n",
      "NOTE: Epoch i=113, ae_loss=  0.0458.\n",
      "NOTE: Epoch i=114, ae_loss=  0.0444.\n",
      "NOTE: Epoch i=115, ae_loss=  0.0451.\n",
      "NOTE: Epoch i=116, ae_loss=  0.0445.\n",
      "NOTE: Epoch i=117, ae_loss=  0.0453.\n",
      "NOTE: Epoch i=118, ae_loss=  0.0435.\n",
      "NOTE: Epoch i=119, ae_loss=  0.0446.\n",
      "NOTE: Epoch i=120, ae_loss=  0.0446.\n",
      "NOTE: Epoch i=121, ae_loss=  0.0436.\n",
      "NOTE: Epoch i=122, ae_loss=  0.0442.\n",
      "NOTE: Epoch i=123, ae_loss=  0.0455.\n",
      "NOTE: Epoch i=124, ae_loss=  0.0440.\n",
      "NOTE: Epoch i=125, ae_loss=  0.0430.\n",
      "NOTE: Epoch i=126, ae_loss=  0.0444.\n",
      "NOTE: Epoch i=127, ae_loss=  0.0422.\n",
      "NOTE: Epoch i=128, ae_loss=  0.0421.\n",
      "NOTE: Epoch i=129, ae_loss=  0.0424.\n",
      "NOTE: Epoch i=130, ae_loss=  0.0411.\n",
      "NOTE: Epoch i=131, ae_loss=  0.0425.\n",
      "NOTE: Epoch i=132, ae_loss=  0.0430.\n",
      "NOTE: Epoch i=133, ae_loss=  0.0420.\n",
      "NOTE: Epoch i=134, ae_loss=  0.0427.\n",
      "NOTE: Epoch i=135, ae_loss=  0.0421.\n",
      "NOTE: Epoch i=136, ae_loss=  0.0412.\n",
      "NOTE: Epoch i=137, ae_loss=  0.0412.\n",
      "NOTE: Epoch i=138, ae_loss=  0.0421.\n",
      "NOTE: Epoch i=139, ae_loss=  0.0410.\n",
      "NOTE: Epoch i=140, ae_loss=  0.0410.\n",
      "NOTE: Epoch i=141, ae_loss=  0.0418.\n",
      "NOTE: Epoch i=142, ae_loss=  0.0414.\n",
      "NOTE: Epoch i=143, ae_loss=  0.0400.\n",
      "NOTE: Epoch i=144, ae_loss=  0.0421.\n",
      "NOTE: Epoch i=145, ae_loss=  0.0423.\n",
      "NOTE: Epoch i=146, ae_loss=  0.0414.\n",
      "NOTE: Epoch i=147, ae_loss=  0.0415.\n",
      "NOTE: Epoch i=148, ae_loss=  0.0425.\n",
      "NOTE: Epoch i=149, ae_loss=  0.0414.\n",
      "NOTE: Epoch i=150, ae_loss=  0.0423.\n",
      "NOTE: Epoch i=151, ae_loss=  0.0423.\n",
      "NOTE: Epoch i=152, ae_loss=  0.0414.\n",
      "NOTE: Epoch i=153, ae_loss=  0.0425.\n",
      "NOTE: Epoch i=154, ae_loss=  0.0408.\n",
      "NOTE: Epoch i=155, ae_loss=  0.0415.\n",
      "NOTE: Epoch i=156, ae_loss=  0.0413.\n",
      "NOTE: Epoch i=157, ae_loss=  0.0414.\n",
      "NOTE: Epoch i=158, ae_loss=  0.0410.\n",
      "NOTE: Epoch i=159, ae_loss=  0.0404.\n",
      "NOTE: Epoch i=160, ae_loss=  0.0420.\n",
      "NOTE: Epoch i=161, ae_loss=  0.0412.\n",
      "NOTE: Epoch i=162, ae_loss=  0.0422.\n",
      "NOTE: Epoch i=163, ae_loss=  0.0401.\n",
      "NOTE: Epoch i=164, ae_loss=  0.0395.\n",
      "NOTE: Epoch i=165, ae_loss=  0.0413.\n",
      "NOTE: Epoch i=166, ae_loss=  0.0411.\n",
      "NOTE: Epoch i=167, ae_loss=  0.0407.\n",
      "NOTE: Epoch i=168, ae_loss=  0.0401.\n",
      "NOTE: Epoch i=169, ae_loss=  0.0404.\n",
      "NOTE: Epoch i=170, ae_loss=  0.0411.\n",
      "NOTE: Epoch i=171, ae_loss=  0.0412.\n",
      "NOTE: Epoch i=172, ae_loss=  0.0399.\n",
      "NOTE: Epoch i=173, ae_loss=  0.0395.\n",
      "NOTE: Epoch i=174, ae_loss=  0.0403.\n",
      "NOTE: Epoch i=175, ae_loss=  0.0398.\n",
      "NOTE: Epoch i=176, ae_loss=  0.0409.\n",
      "NOTE: Epoch i=177, ae_loss=  0.0407.\n",
      "NOTE: Epoch i=178, ae_loss=  0.0407.\n",
      "NOTE: Epoch i=179, ae_loss=  0.0402.\n",
      "NOTE: Epoch i=180, ae_loss=  0.0393.\n",
      "NOTE: Epoch i=181, ae_loss=  0.0409.\n",
      "NOTE: Epoch i=182, ae_loss=  0.0409.\n",
      "NOTE: Epoch i=183, ae_loss=  0.0405.\n",
      "NOTE: Epoch i=184, ae_loss=  0.0411.\n",
      "NOTE: Epoch i=185, ae_loss=  0.0399.\n",
      "NOTE: Epoch i=186, ae_loss=  0.0407.\n",
      "NOTE: Epoch i=187, ae_loss=  0.0408.\n",
      "NOTE: Epoch i=188, ae_loss=  0.0421.\n",
      "NOTE: Epoch i=189, ae_loss=  0.0390.\n",
      "NOTE: Epoch i=190, ae_loss=  0.0394.\n",
      "NOTE: Epoch i=191, ae_loss=  0.0411.\n",
      "NOTE: Epoch i=192, ae_loss=  0.0392.\n",
      "NOTE: Epoch i=193, ae_loss=  0.0398.\n",
      "NOTE: Epoch i=194, ae_loss=  0.0395.\n",
      "NOTE: Epoch i=195, ae_loss=  0.0390.\n",
      "NOTE: Epoch i=196, ae_loss=  0.0394.\n",
      "NOTE: Epoch i=197, ae_loss=  0.0398.\n",
      "NOTE: Epoch i=198, ae_loss=  0.0403.\n",
      "NOTE: Epoch i=199, ae_loss=  0.0406.\n",
      "NOTE: Epoch i=200, ae_loss=  0.0398.\n",
      "NOTE: Epoch i=201, ae_loss=  0.0406.\n",
      "NOTE: Epoch i=202, ae_loss=  0.0379.\n",
      "NOTE: Epoch i=203, ae_loss=  0.0398.\n",
      "NOTE: Epoch i=204, ae_loss=  0.0398.\n",
      "NOTE: Epoch i=205, ae_loss=  0.0399.\n",
      "NOTE: Epoch i=206, ae_loss=  0.0390.\n",
      "NOTE: Epoch i=207, ae_loss=  0.0397.\n",
      "NOTE: Epoch i=208, ae_loss=  0.0395.\n",
      "NOTE: Epoch i=209, ae_loss=  0.0385.\n",
      "NOTE: Epoch i=210, ae_loss=  0.0403.\n",
      "NOTE: Epoch i=211, ae_loss=  0.0400.\n",
      "NOTE: Epoch i=212, ae_loss=  0.0397.\n",
      "NOTE: Epoch i=213, ae_loss=  0.0401.\n",
      "NOTE: Epoch i=214, ae_loss=  0.0395.\n",
      "NOTE: Epoch i=215, ae_loss=  0.0392.\n",
      "NOTE: Epoch i=216, ae_loss=  0.0396.\n",
      "NOTE: Epoch i=217, ae_loss=  0.0393.\n",
      "NOTE: Epoch i=218, ae_loss=  0.0391.\n",
      "NOTE: Epoch i=219, ae_loss=  0.0395.\n",
      "NOTE: Epoch i=220, ae_loss=  0.0389.\n",
      "NOTE: Epoch i=221, ae_loss=  0.0398.\n",
      "NOTE: Epoch i=222, ae_loss=  0.0386.\n",
      "NOTE: Epoch i=223, ae_loss=  0.0401.\n",
      "NOTE: Epoch i=224, ae_loss=  0.0394.\n",
      "NOTE: Epoch i=225, ae_loss=  0.0405.\n",
      "NOTE: Epoch i=226, ae_loss=  0.0389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=227, ae_loss=  0.0388.\n",
      "NOTE: Epoch i=228, ae_loss=  0.0390.\n",
      "NOTE: Epoch i=229, ae_loss=  0.0385.\n",
      "NOTE: Epoch i=230, ae_loss=  0.0380.\n",
      "NOTE: Epoch i=231, ae_loss=  0.0393.\n",
      "NOTE: Epoch i=232, ae_loss=  0.0385.\n",
      "NOTE: Epoch i=233, ae_loss=  0.0385.\n",
      "NOTE: Epoch i=234, ae_loss=  0.0382.\n",
      "NOTE: Epoch i=235, ae_loss=  0.0384.\n",
      "NOTE: Epoch i=236, ae_loss=  0.0389.\n",
      "NOTE: Epoch i=237, ae_loss=  0.0381.\n",
      "NOTE: Epoch i=238, ae_loss=  0.0383.\n",
      "NOTE: Epoch i=239, ae_loss=  0.0387.\n",
      "NOTE: Epoch i=240, ae_loss=  0.0375.\n",
      "NOTE: Epoch i=241, ae_loss=  0.0382.\n",
      "NOTE: Epoch i=242, ae_loss=  0.0378.\n",
      "NOTE: Epoch i=243, ae_loss=  0.0366.\n",
      "NOTE: Epoch i=244, ae_loss=  0.0386.\n",
      "NOTE: Epoch i=245, ae_loss=  0.0385.\n",
      "NOTE: Epoch i=246, ae_loss=  0.0373.\n",
      "NOTE: Epoch i=247, ae_loss=  0.0372.\n",
      "NOTE: Epoch i=248, ae_loss=  0.0392.\n",
      "NOTE: Epoch i=249, ae_loss=  0.0387.\n",
      "NOTE: Epoch i=250, ae_loss=  0.0379.\n",
      "NOTE: Epoch i=251, ae_loss=  0.0385.\n",
      "NOTE: Epoch i=252, ae_loss=  0.0370.\n",
      "NOTE: Epoch i=253, ae_loss=  0.0377.\n",
      "NOTE: Epoch i=254, ae_loss=  0.0386.\n",
      "NOTE: Epoch i=255, ae_loss=  0.0381.\n",
      "NOTE: Epoch i=256, ae_loss=  0.0375.\n",
      "NOTE: Epoch i=257, ae_loss=  0.0381.\n",
      "NOTE: Epoch i=258, ae_loss=  0.0374.\n",
      "NOTE: Epoch i=259, ae_loss=  0.0372.\n",
      "NOTE: Epoch i=260, ae_loss=  0.0382.\n",
      "NOTE: Epoch i=261, ae_loss=  0.0370.\n",
      "NOTE: Epoch i=262, ae_loss=  0.0378.\n",
      "NOTE: Epoch i=263, ae_loss=  0.0366.\n",
      "NOTE: Epoch i=264, ae_loss=  0.0383.\n",
      "NOTE: Epoch i=265, ae_loss=  0.0383.\n",
      "NOTE: Epoch i=266, ae_loss=  0.0373.\n",
      "NOTE: Epoch i=267, ae_loss=  0.0389.\n",
      "NOTE: Epoch i=268, ae_loss=  0.0377.\n",
      "NOTE: Epoch i=269, ae_loss=  0.0384.\n",
      "NOTE: Epoch i=270, ae_loss=  0.0378.\n",
      "NOTE: Epoch i=271, ae_loss=  0.0381.\n",
      "NOTE: Epoch i=272, ae_loss=  0.0375.\n",
      "NOTE: Epoch i=273, ae_loss=  0.0389.\n",
      "NOTE: Epoch i=274, ae_loss=  0.0369.\n",
      "NOTE: Epoch i=275, ae_loss=  0.0384.\n",
      "NOTE: Epoch i=276, ae_loss=  0.0387.\n",
      "NOTE: Epoch i=277, ae_loss=  0.0373.\n",
      "NOTE: Epoch i=278, ae_loss=  0.0375.\n",
      "NOTE: Epoch i=279, ae_loss=  0.0377.\n",
      "NOTE: Epoch i=280, ae_loss=  0.0373.\n",
      "NOTE: Epoch i=281, ae_loss=  0.0385.\n",
      "NOTE: Epoch i=282, ae_loss=  0.0362.\n",
      "NOTE: Epoch i=283, ae_loss=  0.0373.\n",
      "NOTE: Epoch i=284, ae_loss=  0.0384.\n",
      "NOTE: Epoch i=285, ae_loss=  0.0371.\n",
      "NOTE: Epoch i=286, ae_loss=  0.0373.\n",
      "NOTE: Epoch i=287, ae_loss=  0.0372.\n",
      "NOTE: Epoch i=288, ae_loss=  0.0382.\n",
      "NOTE: Epoch i=289, ae_loss=  0.0365.\n",
      "NOTE: Epoch i=290, ae_loss=  0.0363.\n",
      "NOTE: Epoch i=291, ae_loss=  0.0379.\n",
      "NOTE: Epoch i=292, ae_loss=  0.0378.\n",
      "NOTE: Epoch i=293, ae_loss=  0.0396.\n",
      "NOTE: Epoch i=294, ae_loss=  0.0385.\n",
      "NOTE: Epoch i=295, ae_loss=  0.0382.\n",
      "NOTE: Epoch i=296, ae_loss=  0.0361.\n",
      "NOTE: Epoch i=297, ae_loss=  0.0377.\n",
      "NOTE: Epoch i=298, ae_loss=  0.0380.\n",
      "NOTE: Epoch i=299, ae_loss=  0.0377.\n",
      "NOTE: Epoch i=300, ae_loss=  0.0377.\n",
      "NOTE: Epoch i=1, g_loss=  0.7961, d_loss= -1.2164.\n",
      "NOTE: Epoch i=2, g_loss=  1.3558, d_loss= -1.8412.\n",
      "NOTE: Epoch i=3, g_loss=  1.3238, d_loss= -1.7289.\n",
      "NOTE: Epoch i=4, g_loss=  1.3365, d_loss= -1.3257.\n",
      "NOTE: Epoch i=5, g_loss=  1.0593, d_loss= -0.9005.\n",
      "NOTE: Epoch i=6, g_loss=  0.9765, d_loss= -0.9126.\n",
      "NOTE: Epoch i=7, g_loss=  0.9965, d_loss= -0.7264.\n",
      "NOTE: Epoch i=8, g_loss=  0.8753, d_loss= -0.7145.\n",
      "NOTE: Epoch i=9, g_loss=  0.9200, d_loss= -0.5224.\n",
      "NOTE: Epoch i=10, g_loss=  0.9832, d_loss= -0.6094.\n",
      "NOTE: Epoch i=11, g_loss=  1.0636, d_loss= -0.6489.\n",
      "NOTE: Epoch i=12, g_loss=  1.0583, d_loss= -0.5833.\n",
      "NOTE: Epoch i=13, g_loss=  1.0374, d_loss= -0.3076.\n",
      "NOTE: Epoch i=14, g_loss=  1.0261, d_loss= -0.4998.\n",
      "NOTE: Epoch i=15, g_loss=  1.0185, d_loss= -0.5594.\n",
      "NOTE: Epoch i=16, g_loss=  0.9284, d_loss= -0.2560.\n",
      "NOTE: Epoch i=17, g_loss=  0.8545, d_loss= -0.2984.\n",
      "NOTE: Epoch i=18, g_loss=  0.8964, d_loss= -0.4451.\n",
      "NOTE: Epoch i=19, g_loss=  0.8637, d_loss= -0.4499.\n",
      "NOTE: Epoch i=20, g_loss=  0.8427, d_loss= -0.2434.\n",
      "NOTE: Epoch i=21, g_loss=  0.6082, d_loss= -0.3825.\n",
      "NOTE: Epoch i=22, g_loss=  0.8894, d_loss= -0.2501.\n",
      "NOTE: Epoch i=23, g_loss=  0.9014, d_loss= -0.5334.\n",
      "NOTE: Epoch i=24, g_loss=  1.0472, d_loss= -0.2862.\n",
      "NOTE: Epoch i=25, g_loss=  0.8966, d_loss= -0.4125.\n",
      "NOTE: Epoch i=26, g_loss=  0.8648, d_loss= -0.2463.\n",
      "NOTE: Epoch i=27, g_loss=  0.8190, d_loss= -0.4323.\n",
      "NOTE: Epoch i=28, g_loss=  0.9067, d_loss= -0.2039.\n",
      "NOTE: Epoch i=29, g_loss=  0.8590, d_loss= -0.0993.\n",
      "NOTE: Epoch i=30, g_loss=  0.8857, d_loss= -0.1911.\n",
      "NOTE: Epoch i=31, g_loss=  1.1207, d_loss= -0.0727.\n",
      "NOTE: Epoch i=32, g_loss=  0.9631, d_loss= -0.2437.\n",
      "NOTE: Epoch i=33, g_loss=  1.1003, d_loss= -0.2263.\n",
      "NOTE: Epoch i=34, g_loss=  1.0188, d_loss= -0.6852.\n",
      "NOTE: Epoch i=35, g_loss=  0.8578, d_loss= -0.2583.\n",
      "NOTE: Epoch i=36, g_loss=  1.0082, d_loss= -0.1657.\n",
      "NOTE: Epoch i=37, g_loss=  1.2062, d_loss= -0.2619.\n",
      "NOTE: Epoch i=38, g_loss=  1.0593, d_loss= -0.2386.\n",
      "NOTE: Epoch i=39, g_loss=  1.0661, d_loss= -0.5747.\n",
      "NOTE: Epoch i=40, g_loss=  0.9265, d_loss= -0.3881.\n",
      "NOTE: Epoch i=41, g_loss=  0.8095, d_loss= -0.2244.\n",
      "NOTE: Epoch i=42, g_loss=  1.1506, d_loss= -0.4352.\n",
      "NOTE: Epoch i=43, g_loss=  0.9256, d_loss=  0.0878.\n",
      "NOTE: Epoch i=44, g_loss=  0.8753, d_loss= -0.3907.\n",
      "NOTE: Epoch i=45, g_loss=  1.0609, d_loss= -0.4686.\n",
      "NOTE: Epoch i=46, g_loss=  1.0692, d_loss= -0.3417.\n",
      "NOTE: Epoch i=47, g_loss=  1.1139, d_loss= -0.3764.\n",
      "NOTE: Epoch i=48, g_loss=  0.9143, d_loss= -0.1304.\n",
      "NOTE: Epoch i=49, g_loss=  0.9987, d_loss= -0.3872.\n",
      "NOTE: Epoch i=50, g_loss=  1.0832, d_loss= -0.1476.\n",
      "NOTE: Epoch i=51, g_loss=  0.9944, d_loss= -0.3783.\n",
      "NOTE: Epoch i=52, g_loss=  0.7587, d_loss= -0.4405.\n",
      "NOTE: Epoch i=53, g_loss=  0.8884, d_loss= -0.2267.\n",
      "NOTE: Epoch i=54, g_loss=  1.0279, d_loss= -0.4970.\n",
      "NOTE: Epoch i=55, g_loss=  1.2038, d_loss= -0.2957.\n",
      "NOTE: Epoch i=56, g_loss=  1.0506, d_loss= -0.2836.\n",
      "NOTE: Epoch i=57, g_loss=  1.0706, d_loss= -0.0625.\n",
      "NOTE: Epoch i=58, g_loss=  1.0946, d_loss= -0.2866.\n",
      "NOTE: Epoch i=59, g_loss=  0.7708, d_loss= -0.0340.\n",
      "NOTE: Epoch i=60, g_loss=  1.0662, d_loss= -0.3962.\n",
      "NOTE: Epoch i=61, g_loss=  1.0304, d_loss= -0.4491.\n",
      "NOTE: Epoch i=62, g_loss=  0.8301, d_loss= -0.0335.\n",
      "NOTE: Epoch i=63, g_loss=  0.8627, d_loss= -0.3517.\n",
      "NOTE: Epoch i=64, g_loss=  1.0682, d_loss= -0.2456.\n",
      "NOTE: Epoch i=65, g_loss=  0.9147, d_loss= -0.7081.\n",
      "NOTE: Epoch i=66, g_loss=  0.9199, d_loss= -0.0079.\n",
      "NOTE: Epoch i=67, g_loss=  0.8272, d_loss= -0.3972.\n",
      "NOTE: Epoch i=68, g_loss=  0.9223, d_loss= -0.2443.\n",
      "NOTE: Epoch i=69, g_loss=  0.8034, d_loss= -0.4613.\n",
      "NOTE: Epoch i=70, g_loss=  1.0455, d_loss= -0.4183.\n",
      "NOTE: Epoch i=71, g_loss=  1.0080, d_loss= -0.0380.\n",
      "NOTE: Epoch i=72, g_loss=  0.9855, d_loss= -0.2024.\n",
      "NOTE: Epoch i=73, g_loss=  1.0624, d_loss= -0.0952.\n",
      "NOTE: Epoch i=74, g_loss=  1.0504, d_loss= -0.4009.\n",
      "NOTE: Epoch i=75, g_loss=  1.0160, d_loss= -0.2073.\n",
      "NOTE: Epoch i=76, g_loss=  0.9690, d_loss= -0.2545.\n",
      "NOTE: Epoch i=77, g_loss=  1.1396, d_loss= -0.4232.\n",
      "NOTE: Epoch i=78, g_loss=  1.0560, d_loss= -0.3275.\n",
      "NOTE: Epoch i=79, g_loss=  0.9157, d_loss= -0.3060.\n",
      "NOTE: Epoch i=80, g_loss=  0.9226, d_loss= -0.3650.\n",
      "NOTE: Epoch i=81, g_loss=  1.1585, d_loss=  0.0889.\n",
      "NOTE: Epoch i=82, g_loss=  1.1336, d_loss= -0.2926.\n",
      "NOTE: Epoch i=83, g_loss=  1.1466, d_loss= -0.4802.\n",
      "NOTE: Epoch i=84, g_loss=  1.1024, d_loss= -0.1610.\n",
      "NOTE: Epoch i=85, g_loss=  1.0752, d_loss= -0.1864.\n",
      "NOTE: Epoch i=86, g_loss=  0.9175, d_loss= -0.3317.\n",
      "NOTE: Epoch i=87, g_loss=  1.1217, d_loss= -0.3860.\n",
      "NOTE: Epoch i=88, g_loss=  1.0097, d_loss= -0.3540.\n",
      "NOTE: Epoch i=89, g_loss=  0.9119, d_loss= -0.1947.\n",
      "NOTE: Epoch i=90, g_loss=  1.2213, d_loss= -0.4578.\n",
      "NOTE: Epoch i=91, g_loss=  0.9132, d_loss= -0.4433.\n",
      "NOTE: Epoch i=92, g_loss=  0.9692, d_loss= -0.1306.\n",
      "NOTE: Epoch i=93, g_loss=  0.8566, d_loss=  0.0340.\n",
      "NOTE: Epoch i=94, g_loss=  1.1567, d_loss= -0.2610.\n",
      "NOTE: Epoch i=95, g_loss=  1.2216, d_loss= -0.4435.\n",
      "NOTE: Epoch i=96, g_loss=  0.8841, d_loss= -0.2180.\n",
      "NOTE: Epoch i=97, g_loss=  0.7558, d_loss= -0.2419.\n",
      "NOTE: Epoch i=98, g_loss=  0.9066, d_loss= -0.1905.\n",
      "NOTE: Epoch i=99, g_loss=  1.2522, d_loss= -0.2594.\n",
      "NOTE: Epoch i=100, g_loss=  1.0099, d_loss= -0.4312.\n",
      "NOTE: Epoch i=101, g_loss=  1.0221, d_loss= -0.2674.\n",
      "NOTE: Epoch i=102, g_loss=  0.9704, d_loss= -0.4900.\n",
      "NOTE: Epoch i=103, g_loss=  0.9905, d_loss= -0.1331.\n",
      "NOTE: Epoch i=104, g_loss=  1.1394, d_loss= -0.3160.\n",
      "NOTE: Epoch i=105, g_loss=  0.9222, d_loss= -0.1124.\n",
      "NOTE: Epoch i=106, g_loss=  0.7571, d_loss= -0.0639.\n",
      "NOTE: Epoch i=107, g_loss=  0.9001, d_loss= -0.1435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=108, g_loss=  0.9461, d_loss= -0.4092.\n",
      "NOTE: Epoch i=109, g_loss=  1.0145, d_loss= -0.3322.\n",
      "NOTE: Epoch i=110, g_loss=  0.6914, d_loss= -0.5946.\n",
      "NOTE: Epoch i=111, g_loss=  1.0221, d_loss= -0.4472.\n",
      "NOTE: Epoch i=112, g_loss=  0.9298, d_loss= -0.2104.\n",
      "NOTE: Epoch i=113, g_loss=  0.7308, d_loss= -0.3668.\n",
      "NOTE: Epoch i=114, g_loss=  1.0462, d_loss= -0.4668.\n",
      "NOTE: Epoch i=115, g_loss=  0.8875, d_loss= -0.3786.\n",
      "NOTE: Epoch i=116, g_loss=  0.9889, d_loss= -0.7370.\n",
      "NOTE: Epoch i=117, g_loss=  1.1415, d_loss= -0.2194.\n",
      "NOTE: Epoch i=118, g_loss=  1.1728, d_loss= -0.1504.\n",
      "NOTE: Epoch i=119, g_loss=  0.9891, d_loss= -0.2631.\n",
      "NOTE: Epoch i=120, g_loss=  0.8742, d_loss= -0.2560.\n",
      "NOTE: Epoch i=121, g_loss=  1.0960, d_loss= -0.3152.\n",
      "NOTE: Epoch i=122, g_loss=  1.0500, d_loss= -0.4863.\n",
      "NOTE: Epoch i=123, g_loss=  0.8630, d_loss= -0.3423.\n",
      "NOTE: Epoch i=124, g_loss=  1.0001, d_loss= -0.2337.\n",
      "NOTE: Epoch i=125, g_loss=  0.8056, d_loss= -0.3461.\n",
      "NOTE: Epoch i=126, g_loss=  1.0907, d_loss= -0.3309.\n",
      "NOTE: Epoch i=127, g_loss=  0.9574, d_loss= -0.1279.\n",
      "NOTE: Epoch i=128, g_loss=  1.1063, d_loss= -0.1347.\n",
      "NOTE: Epoch i=129, g_loss=  1.1963, d_loss= -0.1541.\n",
      "NOTE: Epoch i=130, g_loss=  0.9583, d_loss= -0.2209.\n",
      "NOTE: Epoch i=131, g_loss=  1.2596, d_loss= -0.4101.\n",
      "NOTE: Epoch i=132, g_loss=  0.9403, d_loss= -0.2959.\n",
      "NOTE: Epoch i=133, g_loss=  0.7650, d_loss= -0.3920.\n",
      "NOTE: Epoch i=134, g_loss=  0.9699, d_loss= -0.1513.\n",
      "NOTE: Epoch i=135, g_loss=  0.9124, d_loss= -0.1827.\n",
      "NOTE: Epoch i=136, g_loss=  0.9971, d_loss= -0.2945.\n",
      "NOTE: Epoch i=137, g_loss=  0.9270, d_loss= -0.5481.\n",
      "NOTE: Epoch i=138, g_loss=  0.9400, d_loss= -0.3841.\n",
      "NOTE: Epoch i=139, g_loss=  1.0024, d_loss= -0.4292.\n",
      "NOTE: Epoch i=140, g_loss=  0.8881, d_loss= -0.3927.\n",
      "NOTE: Epoch i=141, g_loss=  1.0724, d_loss= -0.2415.\n",
      "NOTE: Epoch i=142, g_loss=  0.6598, d_loss= -0.4327.\n",
      "NOTE: Epoch i=143, g_loss=  0.6726, d_loss= -0.4084.\n",
      "NOTE: Epoch i=144, g_loss=  0.9723, d_loss= -0.4205.\n",
      "NOTE: Epoch i=145, g_loss=  0.9542, d_loss= -0.0349.\n",
      "NOTE: Epoch i=146, g_loss=  1.0086, d_loss= -0.1027.\n",
      "NOTE: Epoch i=147, g_loss=  1.0085, d_loss= -0.2700.\n",
      "NOTE: Epoch i=148, g_loss=  0.9102, d_loss= -0.5201.\n",
      "NOTE: Epoch i=149, g_loss=  0.9087, d_loss= -0.1781.\n",
      "NOTE: Epoch i=150, g_loss=  0.9781, d_loss= -0.1873.\n",
      "NOTE: Epoch i=151, g_loss=  1.0778, d_loss= -0.1818.\n",
      "NOTE: Epoch i=152, g_loss=  0.8817, d_loss= -0.3578.\n",
      "NOTE: Epoch i=153, g_loss=  0.9381, d_loss= -0.0652.\n",
      "NOTE: Epoch i=154, g_loss=  1.0992, d_loss= -0.3619.\n",
      "NOTE: Epoch i=155, g_loss=  0.8706, d_loss= -0.3556.\n",
      "NOTE: Epoch i=156, g_loss=  0.8406, d_loss= -0.0256.\n",
      "NOTE: Epoch i=157, g_loss=  0.9721, d_loss= -0.2389.\n",
      "NOTE: Epoch i=158, g_loss=  0.9777, d_loss= -0.2727.\n",
      "NOTE: Epoch i=159, g_loss=  1.1583, d_loss= -0.2089.\n",
      "NOTE: Epoch i=160, g_loss=  0.9663, d_loss= -0.3593.\n",
      "NOTE: Epoch i=161, g_loss=  0.9134, d_loss= -0.4956.\n",
      "NOTE: Epoch i=162, g_loss=  1.0086, d_loss= -0.0894.\n",
      "NOTE: Epoch i=163, g_loss=  0.9883, d_loss= -0.3368.\n",
      "NOTE: Epoch i=164, g_loss=  1.0772, d_loss= -0.1258.\n",
      "NOTE: Epoch i=165, g_loss=  0.8178, d_loss= -0.3501.\n",
      "NOTE: Epoch i=166, g_loss=  0.9138, d_loss= -0.3113.\n",
      "NOTE: Epoch i=167, g_loss=  1.0188, d_loss= -0.2906.\n",
      "NOTE: Epoch i=168, g_loss=  0.8807, d_loss= -0.1680.\n",
      "NOTE: Epoch i=169, g_loss=  0.9874, d_loss= -0.4951.\n",
      "NOTE: Epoch i=170, g_loss=  1.0879, d_loss= -0.4701.\n",
      "NOTE: Epoch i=171, g_loss=  0.8318, d_loss= -0.2953.\n",
      "NOTE: Epoch i=172, g_loss=  1.1173, d_loss= -0.2880.\n",
      "NOTE: Epoch i=173, g_loss=  1.1494, d_loss= -0.3176.\n",
      "NOTE: Epoch i=174, g_loss=  1.1508, d_loss= -0.2043.\n",
      "NOTE: Epoch i=175, g_loss=  1.0558, d_loss= -0.4354.\n",
      "NOTE: Epoch i=176, g_loss=  1.0582, d_loss= -0.1704.\n",
      "NOTE: Epoch i=177, g_loss=  0.8894, d_loss= -0.0917.\n",
      "NOTE: Epoch i=178, g_loss=  1.0778, d_loss= -0.3159.\n",
      "NOTE: Epoch i=179, g_loss=  1.1038, d_loss= -0.3643.\n",
      "NOTE: Epoch i=180, g_loss=  1.0089, d_loss= -0.1865.\n",
      "NOTE: Epoch i=181, g_loss=  1.1698, d_loss= -0.4161.\n",
      "NOTE: Epoch i=182, g_loss=  1.0669, d_loss= -0.1420.\n",
      "NOTE: Epoch i=183, g_loss=  0.8772, d_loss= -0.3245.\n",
      "NOTE: Epoch i=184, g_loss=  1.0802, d_loss= -0.2765.\n",
      "NOTE: Epoch i=185, g_loss=  0.8647, d_loss= -0.4032.\n",
      "NOTE: Epoch i=186, g_loss=  0.8425, d_loss= -0.2197.\n",
      "NOTE: Epoch i=187, g_loss=  1.0973, d_loss= -0.3687.\n",
      "NOTE: Epoch i=188, g_loss=  0.8148, d_loss= -0.3454.\n",
      "NOTE: Epoch i=189, g_loss=  1.0332, d_loss= -0.4686.\n",
      "NOTE: Epoch i=190, g_loss=  0.9657, d_loss= -0.4028.\n",
      "NOTE: Epoch i=191, g_loss=  0.8059, d_loss= -0.6568.\n",
      "NOTE: Epoch i=192, g_loss=  0.7929, d_loss= -0.3615.\n",
      "NOTE: Epoch i=193, g_loss=  0.9199, d_loss= -0.3017.\n",
      "NOTE: Epoch i=194, g_loss=  0.9223, d_loss= -0.3069.\n",
      "NOTE: Epoch i=195, g_loss=  0.8374, d_loss= -0.6094.\n",
      "NOTE: Epoch i=196, g_loss=  0.8355, d_loss= -0.4844.\n",
      "NOTE: Epoch i=197, g_loss=  0.9004, d_loss= -0.1281.\n",
      "NOTE: Epoch i=198, g_loss=  1.1072, d_loss= -0.2984.\n",
      "NOTE: Epoch i=199, g_loss=  0.9305, d_loss= -0.2171.\n",
      "NOTE: Epoch i=200, g_loss=  0.9946, d_loss= -0.1913.\n",
      "NOTE: Epoch i=201, g_loss=  1.0015, d_loss= -0.3871.\n",
      "NOTE: Epoch i=202, g_loss=  1.1397, d_loss= -0.5290.\n",
      "NOTE: Epoch i=203, g_loss=  1.0146, d_loss= -0.2612.\n",
      "NOTE: Epoch i=204, g_loss=  0.9696, d_loss= -0.4531.\n",
      "NOTE: Epoch i=205, g_loss=  0.9068, d_loss= -0.4362.\n",
      "NOTE: Epoch i=206, g_loss=  1.1731, d_loss= -0.5102.\n",
      "NOTE: Epoch i=207, g_loss=  1.0269, d_loss= -0.4137.\n",
      "NOTE: Epoch i=208, g_loss=  0.8650, d_loss= -0.3023.\n",
      "NOTE: Epoch i=209, g_loss=  0.9855, d_loss= -0.4392.\n",
      "NOTE: Epoch i=210, g_loss=  0.9990, d_loss= -0.3798.\n",
      "NOTE: Epoch i=211, g_loss=  0.8689, d_loss= -0.5292.\n",
      "NOTE: Epoch i=212, g_loss=  0.9298, d_loss= -0.2655.\n",
      "NOTE: Epoch i=213, g_loss=  1.0079, d_loss= -0.2740.\n",
      "NOTE: Epoch i=214, g_loss=  0.8606, d_loss= -0.1012.\n",
      "NOTE: Epoch i=215, g_loss=  0.9227, d_loss= -0.1810.\n",
      "NOTE: Epoch i=216, g_loss=  0.8517, d_loss= -0.1112.\n",
      "NOTE: Epoch i=217, g_loss=  0.9184, d_loss= -0.2232.\n",
      "NOTE: Epoch i=218, g_loss=  0.9718, d_loss= -0.4095.\n",
      "NOTE: Epoch i=219, g_loss=  0.9272, d_loss= -0.2504.\n",
      "NOTE: Epoch i=220, g_loss=  0.8784, d_loss= -0.1215.\n",
      "NOTE: Epoch i=221, g_loss=  0.7981, d_loss= -0.5467.\n",
      "NOTE: Epoch i=222, g_loss=  0.6613, d_loss= -0.4827.\n",
      "NOTE: Epoch i=223, g_loss=  0.7190, d_loss= -0.2821.\n",
      "NOTE: Epoch i=224, g_loss=  0.7514, d_loss= -0.2203.\n",
      "NOTE: Epoch i=225, g_loss=  0.8877, d_loss= -0.2871.\n",
      "NOTE: Epoch i=226, g_loss=  0.8307, d_loss= -0.2331.\n",
      "NOTE: Epoch i=227, g_loss=  0.9438, d_loss= -0.2335.\n",
      "NOTE: Epoch i=228, g_loss=  0.8098, d_loss= -0.2643.\n",
      "NOTE: Epoch i=229, g_loss=  0.8777, d_loss= -0.3719.\n",
      "NOTE: Epoch i=230, g_loss=  0.7543, d_loss= -0.4325.\n",
      "NOTE: Epoch i=231, g_loss=  0.9429, d_loss= -0.2188.\n",
      "NOTE: Epoch i=232, g_loss=  0.9154, d_loss= -0.4435.\n",
      "NOTE: Epoch i=233, g_loss=  0.8145, d_loss= -0.3228.\n",
      "NOTE: Epoch i=234, g_loss=  0.9366, d_loss= -0.4176.\n",
      "NOTE: Epoch i=235, g_loss=  0.8650, d_loss= -0.2489.\n",
      "NOTE: Epoch i=236, g_loss=  0.9048, d_loss= -0.4022.\n",
      "NOTE: Epoch i=237, g_loss=  0.9717, d_loss= -0.2752.\n",
      "NOTE: Epoch i=238, g_loss=  0.8810, d_loss= -0.2465.\n",
      "NOTE: Epoch i=239, g_loss=  0.9092, d_loss= -0.3534.\n",
      "NOTE: Epoch i=240, g_loss=  0.8198, d_loss= -0.2935.\n",
      "NOTE: Epoch i=241, g_loss=  0.8985, d_loss= -0.4220.\n",
      "NOTE: Epoch i=242, g_loss=  0.9733, d_loss= -0.5152.\n",
      "NOTE: Epoch i=243, g_loss=  0.9111, d_loss= -0.5828.\n",
      "NOTE: Epoch i=244, g_loss=  0.9007, d_loss= -0.1265.\n",
      "NOTE: Epoch i=245, g_loss=  0.7946, d_loss= -0.3303.\n",
      "NOTE: Epoch i=246, g_loss=  0.7535, d_loss= -0.2791.\n",
      "NOTE: Epoch i=247, g_loss=  0.9635, d_loss= -0.2648.\n",
      "NOTE: Epoch i=248, g_loss=  0.7501, d_loss= -0.2500.\n",
      "NOTE: Epoch i=249, g_loss=  0.8841, d_loss= -0.3569.\n",
      "NOTE: Epoch i=250, g_loss=  0.9598, d_loss= -0.3367.\n",
      "NOTE: Epoch i=251, g_loss=  0.6364, d_loss= -0.2474.\n",
      "NOTE: Epoch i=252, g_loss=  0.8481, d_loss= -0.5359.\n",
      "NOTE: Epoch i=253, g_loss=  0.9284, d_loss= -0.1135.\n",
      "NOTE: Epoch i=254, g_loss=  0.9459, d_loss= -0.3362.\n",
      "NOTE: Epoch i=255, g_loss=  0.7922, d_loss= -0.2623.\n",
      "NOTE: Epoch i=256, g_loss=  1.0110, d_loss= -0.3621.\n",
      "NOTE: Epoch i=257, g_loss=  0.8249, d_loss= -0.4750.\n",
      "NOTE: Epoch i=258, g_loss=  0.7767, d_loss= -0.2157.\n",
      "NOTE: Epoch i=259, g_loss=  0.8678, d_loss= -0.3241.\n",
      "NOTE: Epoch i=260, g_loss=  0.8010, d_loss= -0.1488.\n",
      "NOTE: Epoch i=261, g_loss=  0.7873, d_loss= -0.2695.\n",
      "NOTE: Epoch i=262, g_loss=  0.9389, d_loss= -0.3655.\n",
      "NOTE: Epoch i=263, g_loss=  0.9550, d_loss= -0.2202.\n",
      "NOTE: Epoch i=264, g_loss=  0.8137, d_loss= -0.2478.\n",
      "NOTE: Epoch i=265, g_loss=  0.7289, d_loss= -0.3645.\n",
      "NOTE: Epoch i=266, g_loss=  1.0320, d_loss= -0.2316.\n",
      "NOTE: Epoch i=267, g_loss=  0.9711, d_loss= -0.3086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=268, g_loss=  0.7877, d_loss= -0.3923.\n",
      "NOTE: Epoch i=269, g_loss=  0.7634, d_loss= -0.2380.\n",
      "NOTE: Epoch i=270, g_loss=  0.8763, d_loss= -0.2629.\n",
      "NOTE: Epoch i=271, g_loss=  0.8964, d_loss= -0.5514.\n",
      "NOTE: Epoch i=272, g_loss=  1.0641, d_loss= -0.4184.\n",
      "NOTE: Epoch i=273, g_loss=  0.8470, d_loss= -0.3855.\n",
      "NOTE: Epoch i=274, g_loss=  0.9102, d_loss= -0.2571.\n",
      "NOTE: Epoch i=275, g_loss=  0.9331, d_loss= -0.6300.\n",
      "NOTE: Epoch i=276, g_loss=  0.8541, d_loss= -0.1943.\n",
      "NOTE: Epoch i=277, g_loss=  0.8149, d_loss= -0.6056.\n",
      "NOTE: Epoch i=278, g_loss=  0.6681, d_loss= -0.4575.\n",
      "NOTE: Epoch i=279, g_loss=  0.9522, d_loss= -0.2344.\n",
      "NOTE: Epoch i=280, g_loss=  1.0545, d_loss= -0.6081.\n",
      "NOTE: Epoch i=281, g_loss=  0.9117, d_loss= -0.2916.\n",
      "NOTE: Epoch i=282, g_loss=  0.9757, d_loss= -0.1303.\n",
      "NOTE: Epoch i=283, g_loss=  1.1593, d_loss= -0.3854.\n",
      "NOTE: Epoch i=284, g_loss=  0.8309, d_loss= -0.3963.\n",
      "NOTE: Epoch i=285, g_loss=  0.9503, d_loss= -0.2288.\n",
      "NOTE: Epoch i=286, g_loss=  1.0061, d_loss= -0.2151.\n",
      "NOTE: Epoch i=287, g_loss=  1.1602, d_loss= -0.0710.\n",
      "NOTE: Epoch i=288, g_loss=  1.0902, d_loss= -0.3594.\n",
      "NOTE: Epoch i=289, g_loss=  0.7754, d_loss= -0.2523.\n",
      "NOTE: Epoch i=290, g_loss=  0.8543, d_loss= -0.3365.\n",
      "NOTE: Epoch i=291, g_loss=  1.0375, d_loss= -0.5096.\n",
      "NOTE: Epoch i=292, g_loss=  0.9257, d_loss= -0.4720.\n",
      "NOTE: Epoch i=293, g_loss=  0.8464, d_loss= -0.1886.\n",
      "NOTE: Epoch i=294, g_loss=  0.9496, d_loss= -0.3685.\n",
      "NOTE: Epoch i=295, g_loss=  0.9670, d_loss= -0.3786.\n",
      "NOTE: Epoch i=296, g_loss=  0.8497, d_loss= -0.6803.\n",
      "NOTE: Epoch i=297, g_loss=  1.0742, d_loss= -0.3203.\n",
      "NOTE: Epoch i=298, g_loss=  0.7949, d_loss= -0.3150.\n",
      "NOTE: Epoch i=299, g_loss=  0.9437, d_loss= -0.4457.\n",
      "NOTE: Epoch i=300, g_loss=  0.9188, d_loss= -0.2552.\n",
      "NOTE: 13764667 bytes were written to the table \"cpctStore\" in the caslib \"CASUSER(alphel)\".\n",
      "NOTE: tabularGanTrain action completed successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; IterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch Number\">EpochNumber</th>\n",
       "      <th title=\"Autoencoder Loss\">AutoencoderLoss</th>\n",
       "      <th title=\"Generator Loss\">GeneratorLoss</th>\n",
       "      <th title=\"Discriminator Loss\">DiscriminatorLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.139102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.133378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.128101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.125032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.124965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.849706</td>\n",
       "      <td>-0.680261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.074212</td>\n",
       "      <td>-0.320303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.794919</td>\n",
       "      <td>-0.314960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.943709</td>\n",
       "      <td>-0.445655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.918840</td>\n",
       "      <td>-0.255175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 4 columns</p>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"RowId\">RowId</th>\n",
       "      <th title=\"Description\">Description</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "      <th title=\"Numeric Value\">nValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMBEDDINGDIM</td>\n",
       "      <td>Generator Embedding Dimension</td>\n",
       "      <td>128</td>\n",
       "      <td>1.280000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MINIBATCHSIZE</td>\n",
       "      <td>Number of Observations in One Minibatch</td>\n",
       "      <td>500</td>\n",
       "      <td>5.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PACKSIZE</td>\n",
       "      <td>Number of Observations Group Together in Apply...</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REGDWEIGHT</td>\n",
       "      <td>Weight for Regularizing the Discriminator</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OPTIMIZERAE_BETA1</td>\n",
       "      <td>Exponential Decay Rate for the First Moment Es...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>9.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OPTIMIZERAE_BETA2</td>\n",
       "      <td>Exponential Decay Rate for the Second Moment E...</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>9.990000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OPTIMIZERAE_LEARNINGRATE</td>\n",
       "      <td>Learning Rate for the Autoencoder's Optimizer</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OPTIMIZERAE_NUMEPOCHS</td>\n",
       "      <td>Number of Epochs for the Autoencoder's Training</td>\n",
       "      <td>300</td>\n",
       "      <td>3.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OPTIMIZERAE_WEIGHTDECAY</td>\n",
       "      <td>Weight Decay for the Autoencoder's Optimizer</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>1.000000e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OPTIMIZERGAN_BETA1</td>\n",
       "      <td>Exponential Decay Rate for the First Moment Es...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OPTIMIZERGAN_BETA2</td>\n",
       "      <td>Exponential Decay Rate for the Second Moment E...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>9.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OPTIMIZERGAN_LEARNINGRATE</td>\n",
       "      <td>Learning Rate for the GAN Optimizer</td>\n",
       "      <td>2e-05</td>\n",
       "      <td>2.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>OPTIMIZERGAN_NUMEPOCHS</td>\n",
       "      <td>Number of Epochs for the GAN Training</td>\n",
       "      <td>300</td>\n",
       "      <td>3.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>OPTIMIZERGAN_WEIGHTDECAYD</td>\n",
       "      <td>Weight Decay for the Generator's Optimizer</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>OPTIMIZERGAN_WEIGHTDECAYG</td>\n",
       "      <td>Weight Decay for the Discriminator's Optimizer</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SEED</td>\n",
       "      <td>Seed for Random Initialization</td>\n",
       "      <td>12345</td>\n",
       "      <td>1.234500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>USELOGLEVELFREQ</td>\n",
       "      <td>Whether to Use Log Frequency of Categorical Le...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; NObs</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"RowId\">RowId</th>\n",
       "      <th title=\"Description\">Description</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NREAD</td>\n",
       "      <td>Number of Observations Read</td>\n",
       "      <td>199364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NUSED</td>\n",
       "      <td>Number of Observations Used</td>\n",
       "      <td>199364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Label\">Label</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(alphel)</td>\n",
       "      <td>out</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>CASTable('out', caslib='CASUSER(alphel)')</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 9.84e+03s</span> &#183; <span class=\"cas-user\">user 2.31e+04s</span> &#183; <span class=\"cas-sys\">sys 1.34e+03s</span> &#183; <span class=\"cas-memory\">mem 30.5MB</span></small></p>"
      ],
      "text/plain": [
       "[IterHistory]\n",
       "\n",
       "      EpochNumber  AutoencoderLoss  GeneratorLoss  DiscriminatorLoss\n",
       " 0              1         0.139102            NaN                NaN\n",
       " 1              2         0.133378            NaN                NaN\n",
       " 2              3         0.128101            NaN                NaN\n",
       " 3              4         0.125032            NaN                NaN\n",
       " 4              5         0.124965            NaN                NaN\n",
       " ..           ...              ...            ...                ...\n",
       " 595          296              NaN       0.849706          -0.680261\n",
       " 596          297              NaN       1.074212          -0.320303\n",
       " 597          298              NaN       0.794919          -0.314960\n",
       " 598          299              NaN       0.943709          -0.445655\n",
       " 599          300              NaN       0.918840          -0.255175\n",
       " \n",
       " [600 rows x 4 columns]\n",
       "\n",
       "[ModelInfo]\n",
       "\n",
       "                         RowId  \\\n",
       " 0                EMBEDDINGDIM   \n",
       " 1               MINIBATCHSIZE   \n",
       " 2                    PACKSIZE   \n",
       " 3                  REGDWEIGHT   \n",
       " 4           OPTIMIZERAE_BETA1   \n",
       " 5           OPTIMIZERAE_BETA2   \n",
       " 6    OPTIMIZERAE_LEARNINGRATE   \n",
       " 7       OPTIMIZERAE_NUMEPOCHS   \n",
       " 8     OPTIMIZERAE_WEIGHTDECAY   \n",
       " 9          OPTIMIZERGAN_BETA1   \n",
       " 10         OPTIMIZERGAN_BETA2   \n",
       " 11  OPTIMIZERGAN_LEARNINGRATE   \n",
       " 12     OPTIMIZERGAN_NUMEPOCHS   \n",
       " 13  OPTIMIZERGAN_WEIGHTDECAYD   \n",
       " 14  OPTIMIZERGAN_WEIGHTDECAYG   \n",
       " 15                       SEED   \n",
       " 16            USELOGLEVELFREQ   \n",
       " \n",
       "                                           Description     Value        nValue  \n",
       " 0                       Generator Embedding Dimension       128  1.280000e+02  \n",
       " 1             Number of Observations in One Minibatch       500  5.000000e+02  \n",
       " 2   Number of Observations Group Together in Apply...        10  1.000000e+01  \n",
       " 3           Weight for Regularizing the Discriminator        10  1.000000e+01  \n",
       " 4   Exponential Decay Rate for the First Moment Es...  0.900000  9.000000e-01  \n",
       " 5   Exponential Decay Rate for the Second Moment E...  0.999000  9.990000e-01  \n",
       " 6       Learning Rate for the Autoencoder's Optimizer     0.001  1.000000e-03  \n",
       " 7     Number of Epochs for the Autoencoder's Training       300  3.000000e+02  \n",
       " 8        Weight Decay for the Autoencoder's Optimizer     1e-08  1.000000e-08  \n",
       " 9   Exponential Decay Rate for the First Moment Es...  0.500000  5.000000e-01  \n",
       " 10  Exponential Decay Rate for the Second Moment E...  0.900000  9.000000e-01  \n",
       " 11                Learning Rate for the GAN Optimizer     2e-05  2.000000e-05  \n",
       " 12              Number of Epochs for the GAN Training       300  3.000000e+02  \n",
       " 13         Weight Decay for the Generator's Optimizer    0.0001  1.000000e-04  \n",
       " 14     Weight Decay for the Discriminator's Optimizer     1e-06  1.000000e-06  \n",
       " 15                     Seed for Random Initialization     12345  1.234500e+04  \n",
       " 16  Whether to Use Log Frequency of Categorical Le...      True  1.000000e+00  \n",
       "\n",
       "[NObs]\n",
       "\n",
       "    RowId                  Description   Value\n",
       " 0  NREAD  Number of Observations Read  199364\n",
       " 1  NUSED  Number of Observations Used  199364\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib Name Label  Rows  Columns  \\\n",
       " 0  CASUSER(alphel)  out           0       29   \n",
       " \n",
       "                                     casTable  \n",
       " 0  CASTable('out', caslib='CASUSER(alphel)')  \n",
       "\n",
       "+ Elapsed: 9.84e+03s, user: 2.31e+04s, sys: 1.34e+03s, mem: 30.5mb"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = s.tabularGanTrain(\n",
    "table = {\"name\":\"cc_data\"},\n",
    "    centroidsTable= \"cen\",\n",
    "    gpu = 1,\n",
    "    optimizerAe ={\"method\":'ADAM',\"numEpochs\":300},\n",
    "    optimizerGan ={\"method\":'ADAM',\"numEpochs\":300},\n",
    "    seed = 12345,\n",
    "    scoreSeed = 1234,\n",
    "    numSamples =300000,\n",
    "    saveState ={\"name\":\"cpctStore\", \"replace\":True},\n",
    "    casOut = {\"name\":\"out\", \"replace\":True}\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = s.fetch('out', to=400000, maxrows=400000)['Fetch']\n",
    "gloss = results.IterHistory['GeneratorLoss'].dropna().reset_index(drop=True)\n",
    "dloss = results.IterHistory['DiscriminatorLoss'].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Losses for CPCTGAN on CreditCard data')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(dloss)\n",
    "plt.plot(gloss)\n",
    "plt.legend(['D', 'G'])\n",
    "plt.title('Losses for CPCTGAN on CreditCard data')\n",
    "#plt.savefig('Original-CTGAN-Adult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_X = samples.drop([\"Amount\"],axis=1)\n",
    "fake_y = samples.loc[:,\"Amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 28)\n",
      "(300000,)\n",
      "(85443, 28)\n"
     ]
    }
   ],
   "source": [
    "# check the training data shape to agree with the original data (# of features/cols)\n",
    "print(fake_X.shape)\n",
    "print(fake_y.shape)\n",
    "print(orig_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\R2_score on train dataset :  0.10797218955896903\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression().fit(fake_X, fake_y)\n",
    "predict_train=reg.predict(fake_X)\n",
    "r2_train=r2_score(fake_y, predict_train)\n",
    "print('\\R2_score on train dataset : ', r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\R2_score on test dataset :  0.14773610422813166\n"
     ]
    }
   ],
   "source": [
    "# predict the target on the test dataset\n",
    "predict_test = reg.predict(orig_X_test)\n",
    "#print('\\nTarget on test data',predict_test) \n",
    " \n",
    "r2_test=r2_score(orig_y_test, predict_test)\n",
    "print('\\R2_score on test dataset : ', r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CTGAN on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctgan import CTGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G:  10.1397,Loss D: -29.9089\n",
      "Epoch 2, Loss G:  14.7937,Loss D: -83.2423\n",
      "Epoch 3, Loss G:  19.3291,Loss D: -164.3260\n",
      "Epoch 4, Loss G:  23.8256,Loss D: -263.0912\n",
      "Epoch 5, Loss G:  27.7296,Loss D: -415.3143\n",
      "Epoch 6, Loss G:  32.6189,Loss D: -526.2577\n",
      "Epoch 7, Loss G:  36.3843,Loss D: -689.7050\n",
      "Epoch 8, Loss G:  40.0467,Loss D: -831.0113\n",
      "Epoch 9, Loss G:  44.6874,Loss D: -1017.7595\n",
      "Epoch 10, Loss G:  47.3032,Loss D: -1218.7024\n",
      "Epoch 11, Loss G:  50.9409,Loss D: -1412.9937\n",
      "Epoch 12, Loss G:  51.7371,Loss D: -1621.9845\n",
      "Epoch 13, Loss G:  57.8070,Loss D: -1794.4327\n",
      "Epoch 14, Loss G:  59.9249,Loss D: -1967.3234\n",
      "Epoch 15, Loss G:  62.8084,Loss D: -2086.9167\n",
      "Epoch 16, Loss G:  63.9436,Loss D: -2393.0789\n",
      "Epoch 17, Loss G:  67.6362,Loss D: -2547.1797\n",
      "Epoch 18, Loss G:  68.0923,Loss D: -2566.8606\n",
      "Epoch 19, Loss G:  70.7952,Loss D: -2844.3838\n",
      "Epoch 20, Loss G:  73.0399,Loss D: -2949.3467\n",
      "Epoch 21, Loss G:  71.5208,Loss D: -2953.7708\n",
      "Epoch 22, Loss G:  73.0716,Loss D: -3117.1821\n",
      "Epoch 23, Loss G:  75.0886,Loss D: -3335.9854\n",
      "Epoch 24, Loss G:  77.3117,Loss D: -3339.8816\n",
      "Epoch 25, Loss G:  79.0612,Loss D: -3427.9353\n",
      "Epoch 26, Loss G:  78.8099,Loss D: -3384.5154\n",
      "Epoch 27, Loss G:  78.3320,Loss D: -3527.8997\n",
      "Epoch 28, Loss G:  75.2240,Loss D: -3492.1304\n",
      "Epoch 29, Loss G:  76.7885,Loss D: -3513.8135\n",
      "Epoch 30, Loss G:  79.3535,Loss D: -3463.3586\n",
      "Epoch 31, Loss G:  77.5790,Loss D: -3456.8452\n",
      "Epoch 32, Loss G:  80.0665,Loss D: -3526.4604\n",
      "Epoch 33, Loss G:  78.9035,Loss D: -3530.0410\n",
      "Epoch 34, Loss G:  76.7968,Loss D: -3439.9680\n",
      "Epoch 35, Loss G:  76.4763,Loss D: -3366.9084\n",
      "Epoch 36, Loss G:  79.5912,Loss D: -3389.9978\n",
      "Epoch 37, Loss G:  76.2955,Loss D: -3227.7239\n",
      "Epoch 38, Loss G:  73.0188,Loss D: -3178.2830\n",
      "Epoch 39, Loss G:  72.0668,Loss D: -2959.1340\n",
      "Epoch 40, Loss G:  73.9642,Loss D: -3003.5359\n",
      "Epoch 41, Loss G:  71.1525,Loss D: -2913.9790\n",
      "Epoch 42, Loss G:  69.2212,Loss D: -2738.1157\n",
      "Epoch 43, Loss G:  68.3523,Loss D: -2688.3774\n",
      "Epoch 44, Loss G:  69.0562,Loss D: -2651.1501\n",
      "Epoch 45, Loss G:  65.6515,Loss D: -2561.3750\n",
      "Epoch 46, Loss G:  64.1852,Loss D: -2390.4719\n",
      "Epoch 47, Loss G:  64.1193,Loss D: -2287.5288\n",
      "Epoch 48, Loss G:  61.9992,Loss D: -2220.5549\n",
      "Epoch 49, Loss G:  61.6247,Loss D: -2101.1609\n",
      "Epoch 50, Loss G:  58.2569,Loss D: -1984.2925\n",
      "Epoch 51, Loss G:  58.1795,Loss D: -1898.2035\n",
      "Epoch 52, Loss G:  57.2371,Loss D: -1837.6997\n",
      "Epoch 53, Loss G:  55.0571,Loss D: -1757.0262\n",
      "Epoch 54, Loss G:  53.5584,Loss D: -1649.9855\n",
      "Epoch 55, Loss G:  51.9157,Loss D: -1529.0868\n",
      "Epoch 56, Loss G:  52.7213,Loss D: -1511.9598\n",
      "Epoch 57, Loss G:  49.1416,Loss D: -1440.3660\n",
      "Epoch 58, Loss G:  50.0810,Loss D: -1404.0320\n",
      "Epoch 59, Loss G:  48.2239,Loss D: -1314.6392\n",
      "Epoch 60, Loss G:  47.1751,Loss D: -1190.1677\n",
      "Epoch 61, Loss G:  46.4942,Loss D: -1156.1833\n",
      "Epoch 62, Loss G:  43.4866,Loss D: -1079.6246\n",
      "Epoch 63, Loss G:  41.2394,Loss D: -999.0468\n",
      "Epoch 64, Loss G:  42.6012,Loss D: -967.0491\n",
      "Epoch 65, Loss G:  40.2258,Loss D: -879.0359\n",
      "Epoch 66, Loss G:  39.4189,Loss D: -845.5029\n",
      "Epoch 67, Loss G:  37.5086,Loss D: -755.1368\n",
      "Epoch 68, Loss G:  38.2968,Loss D: -727.0408\n",
      "Epoch 69, Loss G:  35.6869,Loss D: -677.9785\n",
      "Epoch 70, Loss G:  35.7519,Loss D: -669.9789\n",
      "Epoch 71, Loss G:  34.6884,Loss D: -590.5782\n",
      "Epoch 72, Loss G:  32.6896,Loss D: -566.4493\n",
      "Epoch 73, Loss G:  31.2971,Loss D: -515.8105\n",
      "Epoch 74, Loss G:  31.0261,Loss D: -494.4259\n",
      "Epoch 75, Loss G:  29.8260,Loss D: -457.6039\n",
      "Epoch 76, Loss G:  28.8182,Loss D: -422.2988\n",
      "Epoch 77, Loss G:  28.3931,Loss D: -377.2754\n",
      "Epoch 78, Loss G:  27.7358,Loss D: -361.6910\n",
      "Epoch 79, Loss G:  25.9995,Loss D: -351.7786\n",
      "Epoch 80, Loss G:  24.3847,Loss D: -321.8641\n",
      "Epoch 81, Loss G:  24.1412,Loss D: -301.5949\n",
      "Epoch 82, Loss G:  23.3883,Loss D: -274.8761\n",
      "Epoch 83, Loss G:  23.1383,Loss D: -269.3297\n",
      "Epoch 84, Loss G:  21.6259,Loss D: -236.2505\n",
      "Epoch 85, Loss G:  21.5721,Loss D: -227.8048\n",
      "Epoch 86, Loss G:  21.0093,Loss D: -211.7686\n",
      "Epoch 87, Loss G:  20.1979,Loss D: -202.6430\n",
      "Epoch 88, Loss G:  20.2337,Loss D: -194.0880\n",
      "Epoch 89, Loss G:  19.6311,Loss D: -177.5801\n",
      "Epoch 90, Loss G:  19.1538,Loss D: -164.6125\n",
      "Epoch 91, Loss G:  18.4376,Loss D: -152.5192\n",
      "Epoch 92, Loss G:  17.9436,Loss D: -147.8505\n",
      "Epoch 93, Loss G:  17.2680,Loss D: -135.6693\n",
      "Epoch 94, Loss G:  17.5127,Loss D: -129.2582\n",
      "Epoch 95, Loss G:  17.0590,Loss D: -123.2845\n",
      "Epoch 96, Loss G:  16.2085,Loss D: -117.5381\n",
      "Epoch 97, Loss G:  15.8838,Loss D: -110.4693\n",
      "Epoch 98, Loss G:  15.0616,Loss D: -100.2817\n",
      "Epoch 99, Loss G:  15.0387,Loss D: -95.6360\n",
      "Epoch 100, Loss G:  14.3323,Loss D: -95.0070\n",
      "Epoch 101, Loss G:  14.4878,Loss D: -89.6050\n",
      "Epoch 102, Loss G:  14.2349,Loss D: -84.5494\n",
      "Epoch 103, Loss G:  13.9878,Loss D: -79.6694\n",
      "Epoch 104, Loss G:  13.4908,Loss D: -75.3999\n",
      "Epoch 105, Loss G:  13.4180,Loss D: -71.7084\n",
      "Epoch 106, Loss G:  13.5186,Loss D: -71.1835\n",
      "Epoch 107, Loss G:  12.2514,Loss D: -67.7957\n",
      "Epoch 108, Loss G:  12.7703,Loss D: -64.4349\n",
      "Epoch 109, Loss G:  12.4402,Loss D: -60.6531\n",
      "Epoch 110, Loss G:  12.7543,Loss D: -64.4866\n",
      "Epoch 111, Loss G:  11.8645,Loss D: -56.3082\n",
      "Epoch 112, Loss G:  11.8305,Loss D: -54.1243\n",
      "Epoch 113, Loss G:  11.9236,Loss D: -54.2626\n",
      "Epoch 114, Loss G:  11.5392,Loss D: -53.0271\n",
      "Epoch 115, Loss G:  11.4246,Loss D: -50.4211\n",
      "Epoch 116, Loss G:  11.6290,Loss D: -46.6985\n",
      "Epoch 117, Loss G:  11.6294,Loss D: -47.0243\n",
      "Epoch 118, Loss G:  11.2870,Loss D: -44.1312\n",
      "Epoch 119, Loss G:  10.6452,Loss D: -40.7792\n",
      "Epoch 120, Loss G:  10.9862,Loss D: -41.8010\n",
      "Epoch 121, Loss G:  10.8409,Loss D: -40.3338\n",
      "Epoch 122, Loss G:  10.6210,Loss D: -40.7083\n",
      "Epoch 123, Loss G:  10.5122,Loss D: -37.3861\n",
      "Epoch 124, Loss G:  10.3611,Loss D: -37.6624\n",
      "Epoch 125, Loss G:  10.2790,Loss D: -35.5219\n",
      "Epoch 126, Loss G:  10.0721,Loss D: -34.6187\n",
      "Epoch 127, Loss G:  10.0426,Loss D: -33.4923\n",
      "Epoch 128, Loss G:  9.6692,Loss D: -31.2662\n",
      "Epoch 129, Loss G:  9.3016,Loss D: -29.8414\n",
      "Epoch 130, Loss G:  9.1348,Loss D: -30.6056\n",
      "Epoch 131, Loss G:  9.1916,Loss D: -28.2553\n",
      "Epoch 132, Loss G:  9.3471,Loss D: -27.6904\n",
      "Epoch 133, Loss G:  9.2739,Loss D: -26.4160\n",
      "Epoch 134, Loss G:  8.9129,Loss D: -24.8979\n",
      "Epoch 135, Loss G:  9.0865,Loss D: -25.7265\n",
      "Epoch 136, Loss G:  8.7767,Loss D: -23.5888\n",
      "Epoch 137, Loss G:  8.9142,Loss D: -25.1693\n",
      "Epoch 138, Loss G:  8.6372,Loss D: -23.5035\n",
      "Epoch 139, Loss G:  8.4586,Loss D: -22.9481\n",
      "Epoch 140, Loss G:  8.5514,Loss D: -21.2430\n",
      "Epoch 141, Loss G:  8.6151,Loss D: -21.6156\n",
      "Epoch 142, Loss G:  8.5714,Loss D: -20.4266\n",
      "Epoch 143, Loss G:  8.2988,Loss D: -20.8144\n",
      "Epoch 144, Loss G:  8.1071,Loss D: -19.9799\n",
      "Epoch 145, Loss G:  8.3555,Loss D: -19.5612\n",
      "Epoch 146, Loss G:  8.3169,Loss D: -18.8757\n",
      "Epoch 147, Loss G:  7.9101,Loss D: -17.8258\n",
      "Epoch 148, Loss G:  8.0547,Loss D: -16.2208\n",
      "Epoch 149, Loss G:  7.7545,Loss D: -17.0309\n",
      "Epoch 150, Loss G:  7.8085,Loss D: -16.6095\n",
      "Epoch 151, Loss G:  7.6344,Loss D: -17.0430\n",
      "Epoch 152, Loss G:  7.6563,Loss D: -15.5360\n",
      "Epoch 153, Loss G:  7.4264,Loss D: -15.7836\n",
      "Epoch 154, Loss G:  7.3189,Loss D: -14.6314\n",
      "Epoch 155, Loss G:  7.1081,Loss D: -13.5542\n",
      "Epoch 156, Loss G:  7.2804,Loss D: -13.3973\n",
      "Epoch 157, Loss G:  7.2060,Loss D: -12.6521\n",
      "Epoch 158, Loss G:  7.0855,Loss D: -13.0215\n",
      "Epoch 159, Loss G:  7.1425,Loss D: -12.9692\n",
      "Epoch 160, Loss G:  7.0720,Loss D: -12.3178\n",
      "Epoch 161, Loss G:  6.8525,Loss D: -12.0598\n",
      "Epoch 162, Loss G:  7.2438,Loss D: -12.5493\n",
      "Epoch 163, Loss G:  6.9600,Loss D: -11.6080\n",
      "Epoch 164, Loss G:  6.9958,Loss D: -11.8803\n",
      "Epoch 165, Loss G:  6.9039,Loss D: -10.8724\n",
      "Epoch 166, Loss G:  6.8344,Loss D: -10.6579\n",
      "Epoch 167, Loss G:  6.9084,Loss D: -10.7401\n",
      "Epoch 168, Loss G:  6.9404,Loss D: -10.6983\n",
      "Epoch 169, Loss G:  6.7019,Loss D: -10.2689\n",
      "Epoch 170, Loss G:  6.5901,Loss D: -9.6672\n",
      "Epoch 171, Loss G:  6.8017,Loss D: -9.7108\n",
      "Epoch 172, Loss G:  6.3554,Loss D: -9.1138\n",
      "Epoch 173, Loss G:  6.3712,Loss D: -9.5289\n",
      "Epoch 174, Loss G:  6.3765,Loss D: -9.1152\n",
      "Epoch 175, Loss G:  6.4783,Loss D: -9.3802\n",
      "Epoch 176, Loss G:  6.4214,Loss D: -8.9867\n",
      "Epoch 177, Loss G:  6.1522,Loss D: -9.0908\n",
      "Epoch 178, Loss G:  6.2596,Loss D: -8.4372\n",
      "Epoch 179, Loss G:  6.1070,Loss D: -7.9304\n",
      "Epoch 180, Loss G:  5.9809,Loss D: -8.3223\n",
      "Epoch 181, Loss G:  6.2373,Loss D: -8.6100\n",
      "Epoch 182, Loss G:  5.9194,Loss D: -8.1089\n",
      "Epoch 183, Loss G:  5.9695,Loss D: -8.2578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184, Loss G:  6.0554,Loss D: -7.5690\n",
      "Epoch 185, Loss G:  5.7420,Loss D: -7.0375\n",
      "Epoch 186, Loss G:  5.9606,Loss D: -7.4144\n",
      "Epoch 187, Loss G:  5.7684,Loss D: -6.7946\n",
      "Epoch 188, Loss G:  5.6791,Loss D: -6.3540\n",
      "Epoch 189, Loss G:  5.6855,Loss D: -6.8653\n",
      "Epoch 190, Loss G:  5.7551,Loss D: -6.7940\n",
      "Epoch 191, Loss G:  5.8035,Loss D: -6.9704\n",
      "Epoch 192, Loss G:  5.4543,Loss D: -5.9767\n",
      "Epoch 193, Loss G:  5.6559,Loss D: -6.2664\n",
      "Epoch 194, Loss G:  5.5449,Loss D: -6.9710\n",
      "Epoch 195, Loss G:  5.6095,Loss D: -6.2608\n",
      "Epoch 196, Loss G:  5.4496,Loss D: -5.9196\n",
      "Epoch 197, Loss G:  5.4982,Loss D: -6.1658\n",
      "Epoch 198, Loss G:  5.4057,Loss D: -6.1816\n",
      "Epoch 199, Loss G:  5.4854,Loss D: -5.7794\n",
      "Epoch 200, Loss G:  5.5046,Loss D: -5.8720\n",
      "Epoch 201, Loss G:  5.2928,Loss D: -6.3033\n",
      "Epoch 202, Loss G:  5.2786,Loss D: -5.6454\n",
      "Epoch 203, Loss G:  5.2639,Loss D: -5.2602\n",
      "Epoch 204, Loss G:  5.3528,Loss D: -5.3286\n",
      "Epoch 205, Loss G:  5.2291,Loss D: -5.6470\n",
      "Epoch 206, Loss G:  5.2079,Loss D: -5.1751\n",
      "Epoch 207, Loss G:  5.1385,Loss D: -5.3867\n",
      "Epoch 208, Loss G:  5.0166,Loss D: -5.1200\n",
      "Epoch 209, Loss G:  5.0456,Loss D: -5.3555\n",
      "Epoch 210, Loss G:  5.1309,Loss D: -4.8821\n",
      "Epoch 211, Loss G:  5.3504,Loss D: -5.3349\n",
      "Epoch 212, Loss G:  5.1083,Loss D: -4.8337\n",
      "Epoch 213, Loss G:  4.9811,Loss D: -4.7324\n",
      "Epoch 214, Loss G:  4.8996,Loss D: -4.6814\n",
      "Epoch 215, Loss G:  4.8606,Loss D: -4.6629\n",
      "Epoch 216, Loss G:  5.0019,Loss D: -5.1844\n",
      "Epoch 217, Loss G:  4.8769,Loss D: -4.8573\n",
      "Epoch 218, Loss G:  4.7688,Loss D: -4.3902\n",
      "Epoch 219, Loss G:  4.4629,Loss D: -4.1029\n",
      "Epoch 220, Loss G:  4.6272,Loss D: -4.5524\n",
      "Epoch 221, Loss G:  4.5926,Loss D: -4.4633\n",
      "Epoch 222, Loss G:  4.4901,Loss D: -4.7430\n",
      "Epoch 223, Loss G:  4.5170,Loss D: -4.1312\n",
      "Epoch 224, Loss G:  4.4836,Loss D: -4.1873\n",
      "Epoch 225, Loss G:  4.6199,Loss D: -4.4911\n",
      "Epoch 226, Loss G:  4.5340,Loss D: -4.2202\n",
      "Epoch 227, Loss G:  4.4381,Loss D: -3.9864\n",
      "Epoch 228, Loss G:  4.4657,Loss D: -4.2152\n",
      "Epoch 229, Loss G:  4.3843,Loss D: -4.0781\n",
      "Epoch 230, Loss G:  4.5315,Loss D: -4.2684\n",
      "Epoch 231, Loss G:  4.4457,Loss D: -3.9952\n",
      "Epoch 232, Loss G:  4.3302,Loss D: -4.0470\n",
      "Epoch 233, Loss G:  4.3300,Loss D: -3.7357\n",
      "Epoch 234, Loss G:  4.3931,Loss D: -3.9837\n",
      "Epoch 235, Loss G:  4.2796,Loss D: -3.7858\n",
      "Epoch 236, Loss G:  4.1397,Loss D: -3.6323\n",
      "Epoch 237, Loss G:  4.4634,Loss D: -4.1632\n",
      "Epoch 238, Loss G:  4.5530,Loss D: -3.6039\n",
      "Epoch 239, Loss G:  4.2348,Loss D: -3.7602\n",
      "Epoch 240, Loss G:  4.2210,Loss D: -4.0676\n",
      "Epoch 241, Loss G:  4.2206,Loss D: -4.1677\n",
      "Epoch 242, Loss G:  3.9863,Loss D: -3.5586\n",
      "Epoch 243, Loss G:  4.1730,Loss D: -3.8398\n",
      "Epoch 244, Loss G:  4.0173,Loss D: -3.5973\n",
      "Epoch 245, Loss G:  4.0098,Loss D: -3.6806\n",
      "Epoch 246, Loss G:  4.0368,Loss D: -3.1728\n",
      "Epoch 247, Loss G:  4.1223,Loss D: -3.5395\n",
      "Epoch 248, Loss G:  3.9827,Loss D: -3.5023\n",
      "Epoch 249, Loss G:  3.8298,Loss D: -3.5667\n",
      "Epoch 250, Loss G:  4.0811,Loss D: -3.8659\n",
      "Epoch 251, Loss G:  3.6157,Loss D: -3.2606\n",
      "Epoch 252, Loss G:  3.7999,Loss D: -3.7205\n",
      "Epoch 253, Loss G:  3.8645,Loss D: -3.5522\n",
      "Epoch 254, Loss G:  3.7272,Loss D: -3.3604\n",
      "Epoch 255, Loss G:  3.7018,Loss D: -3.3440\n",
      "Epoch 256, Loss G:  3.7498,Loss D: -3.4600\n",
      "Epoch 257, Loss G:  3.6466,Loss D: -3.0685\n",
      "Epoch 258, Loss G:  3.4756,Loss D: -2.8217\n",
      "Epoch 259, Loss G:  3.5624,Loss D: -3.2581\n",
      "Epoch 260, Loss G:  3.5383,Loss D: -3.3469\n",
      "Epoch 261, Loss G:  3.4052,Loss D: -3.3007\n",
      "Epoch 262, Loss G:  3.5130,Loss D: -3.0911\n",
      "Epoch 263, Loss G:  3.5087,Loss D: -3.3442\n",
      "Epoch 264, Loss G:  3.4052,Loss D: -3.4456\n",
      "Epoch 265, Loss G:  3.4415,Loss D: -3.0193\n",
      "Epoch 266, Loss G:  3.5619,Loss D: -3.2632\n",
      "Epoch 267, Loss G:  3.3267,Loss D: -3.1935\n",
      "Epoch 268, Loss G:  3.2597,Loss D: -3.1963\n",
      "Epoch 269, Loss G:  3.5162,Loss D: -3.2274\n",
      "Epoch 270, Loss G:  3.2258,Loss D: -3.1434\n",
      "Epoch 271, Loss G:  3.3192,Loss D: -3.3492\n",
      "Epoch 272, Loss G:  3.3777,Loss D: -3.2238\n",
      "Epoch 273, Loss G:  3.2674,Loss D: -3.1559\n",
      "Epoch 274, Loss G:  3.1446,Loss D: -2.7907\n",
      "Epoch 275, Loss G:  3.1099,Loss D: -2.9577\n",
      "Epoch 276, Loss G:  3.0041,Loss D: -3.0827\n",
      "Epoch 277, Loss G:  3.0374,Loss D: -3.5785\n",
      "Epoch 278, Loss G:  3.0038,Loss D: -2.8341\n",
      "Epoch 279, Loss G:  2.9450,Loss D: -2.9920\n",
      "Epoch 280, Loss G:  3.0723,Loss D: -2.9147\n",
      "Epoch 281, Loss G:  3.1347,Loss D: -2.7425\n",
      "Epoch 282, Loss G:  3.2840,Loss D: -3.0335\n",
      "Epoch 283, Loss G:  3.0344,Loss D: -2.6983\n",
      "Epoch 284, Loss G:  3.0774,Loss D: -3.1723\n",
      "Epoch 285, Loss G:  2.9093,Loss D: -2.6522\n",
      "Epoch 286, Loss G:  2.9326,Loss D: -2.6758\n",
      "Epoch 287, Loss G:  3.1061,Loss D: -2.8560\n",
      "Epoch 288, Loss G:  2.8812,Loss D: -2.8977\n",
      "Epoch 289, Loss G:  2.8600,Loss D: -2.9130\n",
      "Epoch 290, Loss G:  2.9538,Loss D: -2.6979\n",
      "Epoch 291, Loss G:  3.0198,Loss D: -2.9069\n",
      "Epoch 292, Loss G:  3.0062,Loss D: -2.8378\n",
      "Epoch 293, Loss G:  3.2410,Loss D: -2.3223\n",
      "Epoch 294, Loss G:  2.8002,Loss D: -2.8902\n",
      "Epoch 295, Loss G:  2.8359,Loss D: -2.5482\n",
      "Epoch 296, Loss G:  2.7874,Loss D: -2.5813\n",
      "Epoch 297, Loss G:  2.9158,Loss D: -2.7386\n",
      "Epoch 298, Loss G:  2.8579,Loss D: -2.6700\n",
      "Epoch 299, Loss G:  2.7920,Loss D: -3.0837\n",
      "Epoch 300, Loss G:  2.7488,Loss D: -2.7053\n"
     ]
    }
   ],
   "source": [
    "# train CTGAN and generate fake data\n",
    "# from synthesizer import CTGANSynthesizer\n",
    "ctgan = CTGANSynthesizer(verbose=True)\n",
    "ctgan.fit(cc_GAN, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adult_data_all.head()\n",
    "samples = ctgan.sample(300000)\n",
    "fake_X = samples.drop([\"Amount\"],axis=1)\n",
    "fake_X, fake_y = fake_X,samples.loc[:,\"Amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 28)\n",
      "(300000,)\n",
      "(85443, 28)\n"
     ]
    }
   ],
   "source": [
    "# check the training data shape to agree with the original data (# of features/cols)\n",
    "print(fake_X.shape)\n",
    "print(fake_y.shape)\n",
    "print(orig_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\R2_score on train dataset :  0.0013678218651916874\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression().fit(fake_X, fake_y)\n",
    "predict_train=reg.predict(fake_X)\n",
    "r2_train=r2_score(fake_y, predict_train)\n",
    "print('\\R2_score on train dataset : ', r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\R2_score on test dataset :  -0.8708630163471054\n"
     ]
    }
   ],
   "source": [
    "# predict the target on the test dataset\n",
    "predict_test = reg.predict(orig_X_test)\n",
    "#print('\\nTarget on test data',predict_test) \n",
    " \n",
    "r2_test=r2_score(orig_y_test, predict_test)\n",
    "print('\\R2_score on test dataset : ', r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
