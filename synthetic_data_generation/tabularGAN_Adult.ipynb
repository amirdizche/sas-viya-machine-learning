{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swat as sw\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>215419</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>64</td>\n",
       "      <td>?</td>\n",
       "      <td>321403</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Other-relative</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>374983</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>83891</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>5455</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>35</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>182148</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass  fnlwgt  education  education-num  \\\n",
       "0       39         State-gov   77516  Bachelors             13   \n",
       "1       50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2       38           Private  215646    HS-grad              9   \n",
       "3       53           Private  234721       11th              7   \n",
       "4       28           Private  338409  Bachelors             13   \n",
       "...    ...               ...     ...        ...            ...   \n",
       "48837   39           Private  215419  Bachelors             13   \n",
       "48838   64                 ?  321403    HS-grad              9   \n",
       "48839   38           Private  374983  Bachelors             13   \n",
       "48840   44           Private   83891  Bachelors             13   \n",
       "48841   35      Self-emp-inc  182148  Bachelors             13   \n",
       "\n",
       "           marital-status         occupation    relationship  \\\n",
       "0           Never-married       Adm-clerical   Not-in-family   \n",
       "1      Married-civ-spouse    Exec-managerial         Husband   \n",
       "2                Divorced  Handlers-cleaners   Not-in-family   \n",
       "3      Married-civ-spouse  Handlers-cleaners         Husband   \n",
       "4      Married-civ-spouse     Prof-specialty            Wife   \n",
       "...                   ...                ...             ...   \n",
       "48837            Divorced     Prof-specialty   Not-in-family   \n",
       "48838             Widowed                  ?  Other-relative   \n",
       "48839  Married-civ-spouse     Prof-specialty         Husband   \n",
       "48840            Divorced       Adm-clerical       Own-child   \n",
       "48841  Married-civ-spouse    Exec-managerial         Husband   \n",
       "\n",
       "                     race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0                   White    Male          2174             0              40   \n",
       "1                   White    Male             0             0              13   \n",
       "2                   White    Male             0             0              40   \n",
       "3                   Black    Male             0             0              40   \n",
       "4                   Black  Female             0             0              40   \n",
       "...                   ...     ...           ...           ...             ...   \n",
       "48837               White  Female             0             0              36   \n",
       "48838               Black    Male             0             0              40   \n",
       "48839               White    Male             0             0              50   \n",
       "48840  Asian-Pac-Islander    Male          5455             0              40   \n",
       "48841               White    Male             0             0              60   \n",
       "\n",
       "      native-country  label  \n",
       "0      United-States  <=50K  \n",
       "1      United-States  <=50K  \n",
       "2      United-States  <=50K  \n",
       "3      United-States  <=50K  \n",
       "4               Cuba  <=50K  \n",
       "...              ...    ...  \n",
       "48837  United-States  <=50K  \n",
       "48838  United-States  <=50K  \n",
       "48839  United-States  <=50K  \n",
       "48840  United-States  <=50K  \n",
       "48841  United-States   >50K  \n",
       "\n",
       "[48842 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_data = pd.read_csv('../gan-testing/data/adult.csv')\n",
    "adult_discrete_columns = \"workclass,education,marital-status,occupation,relationship,race,sex,native-country,label\".split(',')\n",
    "adult_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country  label  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove some extra charachters\n",
    "adult_data = adult_data.replace({'\\$': '', ',': ''}, regex=True)\n",
    "adult_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace the target >50K & <=50K with 1, 0\n",
    "adult_data.loc[adult_data['label']=='>50K', 'label'] = 1\n",
    "adult_data.loc[adult_data['label']=='<=50K', 'label'] = 0 # use this as input to the GAN\n",
    "unique_labels = adult_data['label'].unique()\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_data.replace('?', np.nan, inplace=True)\n",
    "adult_data=adult_data.fillna(adult_data.mean())\n",
    "adult_data = adult_data.apply(lambda x:x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>workclass_State-gov</th>\n",
       "      <th>workclass_Without-pay</th>\n",
       "      <th>education_10th</th>\n",
       "      <th>education_11th</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   workclass_Federal-gov  workclass_Local-gov  workclass_Never-worked  \\\n",
       "0                      0                    0                       0   \n",
       "1                      0                    0                       0   \n",
       "2                      0                    0                       0   \n",
       "3                      0                    0                       0   \n",
       "4                      0                    0                       0   \n",
       "\n",
       "   workclass_Private  workclass_Self-emp-inc  workclass_Self-emp-not-inc  \\\n",
       "0                  0                       0                           0   \n",
       "1                  0                       0                           1   \n",
       "2                  1                       0                           0   \n",
       "3                  1                       0                           0   \n",
       "4                  1                       0                           0   \n",
       "\n",
       "   workclass_State-gov  workclass_Without-pay  education_10th  education_11th  \\\n",
       "0                    1                      0               0               0   \n",
       "1                    0                      0               0               0   \n",
       "2                    0                      0               0               0   \n",
       "3                    0                      0               0               1   \n",
       "4                    0                      0               0               0   \n",
       "\n",
       "   ...  native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "0  ...                        0                           0   \n",
       "1  ...                        0                           0   \n",
       "2  ...                        0                           0   \n",
       "3  ...                        0                           0   \n",
       "4  ...                        0                           0   \n",
       "\n",
       "   native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                        0                     0                      0   \n",
       "1                        0                     0                      0   \n",
       "2                        0                     0                      0   \n",
       "3                        0                     0                      0   \n",
       "4                        0                     0                      0   \n",
       "\n",
       "   native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                        0                               0   \n",
       "1                        0                               0   \n",
       "2                        0                               0   \n",
       "3                        0                               0   \n",
       "4                        0                               0   \n",
       "\n",
       "   native-country_United-States  native-country_Vietnam  \\\n",
       "0                             1                       0   \n",
       "1                             1                       0   \n",
       "2                             1                       0   \n",
       "3                             1                       0   \n",
       "4                             0                       0   \n",
       "\n",
       "   native-country_Yugoslavia  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the Ruiwen's Encoder for ML utility and CTGAN transformer for GAN\n",
    "str_cols= [ 'workclass', 'education', 'marital-status', 'relationship','race', 'sex','native-country']\n",
    "num_cols = ['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week', 'label']\n",
    "dataframe = pd.DataFrame(adult_data.loc[:,str_cols])\n",
    "\n",
    "one_hot_columns = pd.DataFrame()\n",
    "for col_name, item in dataframe.iteritems():\n",
    "    \n",
    "    #print(col_name)\n",
    "    #print(item)\n",
    "    col = pd.get_dummies(item, prefix=col_name)\n",
    "    one_hot_columns =pd.concat([one_hot_columns,col],axis=1)\n",
    "one_hot_columns.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire data is concat of discrete and contiuous cols\n",
    "adult_data_all = pd.concat([one_hot_columns,adult_data.loc[:,num_cols]],axis=1)\n",
    "#adult_data_all.head()\n",
    "my_X = adult_data_all.drop([\"label\"],axis=1)\n",
    "orig_X, orig_y = my_X,adult_data_all.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "orig_X_train, orig_X_test, orig_y_train, orig_y_test = train_test_split(orig_X, orig_y, test_size=0.3, random_state=123)\n",
    "my_data1_train = pd.concat([orig_X_train,orig_y_train],axis=1)\n",
    "my_data1_test = pd.concat([orig_X_test,orig_y_test],axis=1)\n",
    "#my_data1_test.describe()\n",
    "orig_y_train = orig_y_train.astype('int')\n",
    "orig_y_test = orig_y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train ML models on the train set of the original data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "names = [\"Decision Tree\",\"Linear SVM\", \"Random Forest\", \"Logistic Regression\",\"MLP\"]\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5,random_state=0),\n",
    "    SVC(kernel = 'linear', max_iter=1000, C=0.025, random_state=0, probability=True),\n",
    "    RandomForestClassifier(n_estimators=200, random_state=0),\n",
    "    LogisticRegression(max_iter=1000, random_state=0),\n",
    "    MLPClassifier(alpha=1, max_iter=1000, random_state=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML scores for the original data:\n",
      "Decision Tree Acc:  0.8537500853067631 f-1:  0.6328593455542231 AUC: 0.8863975382887572\n",
      "Linear SVM Acc:  0.2956391182692964 f-1:  0.368089144676422 AUC: 0.509806196049388\n",
      "Random Forest Acc:  0.8495188698560022 f-1:  0.6575555210436402 AUC: 0.8977741254446227\n",
      "Logistic Regression Acc:  0.80017743806729 f-1:  0.3851322973540529 AUC: 0.5769572159245973\n",
      "MLP Acc:  0.7840715211902 f-1:  0.43419170243204575 AUC: 0.668529384827481\n"
     ]
    }
   ],
   "source": [
    "print('ML scores for the original data:')\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(orig_X_train, orig_y_train)\n",
    "    score = clf.score(orig_X_test, orig_y_test)\n",
    "    y_pred = clf.predict(orig_X_test)\n",
    "    Acc = accuracy_score(orig_y_test, y_pred)\n",
    "    fscore = f1_score(orig_y_test, y_pred, average='binary')\n",
    "    AUC = roc_auc_score(orig_y_test, clf.predict_proba(orig_X_test)[:, 1])\n",
    "    print(name,'Acc: ', Acc, 'f-1: ', fscore, 'AUC:', AUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['workclass',\n",
       " 'education',\n",
       " 'marital-status',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'native-country',\n",
       " 'label']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # now input the data to CPCTGAN and train\n",
    "# adult_discrete_columns = adult_discrete_columns[:-1]\n",
    "adult_discrete_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the train data for gan with same seed as the ML utility\n",
    "GAN_X = adult_data.drop([\"label\"],axis=1)\n",
    "GAN_orig_X, GAN_orig_y = GAN_X,adult_data.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "GAN_X_train, GAN_X_test, GAN_y_train, GAN_y_test = train_test_split(GAN_orig_X, GAN_orig_y, test_size=0.3, random_state=123)\n",
    "GAN_data_train = pd.concat([GAN_X_train,GAN_y_train],axis=1)\n",
    "GAN_data_test = pd.concat([GAN_X_test,GAN_y_test],axis=1)\n",
    "#my_data1_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sw.CAS('dl2073.clstr.rnd.sas.com',33789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Added action set 'generativeAdversarialNet'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; actionset</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>generativeAdversarialNet</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.334s</span> &#183; <span class=\"cas-user\">user 3.55s</span> &#183; <span class=\"cas-sys\">sys 2.91s</span> &#183; <span class=\"cas-memory\">mem 0.222MB</span></small></p>"
      ],
      "text/plain": [
       "[actionset]\n",
       "\n",
       " 'generativeAdversarialNet'\n",
       "\n",
       "+ Elapsed: 0.334s, user: 3.55s, sys: 2.91s, mem: 0.222mb"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.loadactionset('generativeAdversarialNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table GAN_DATA_TRAIN in caslib CASUSER(alphel).\n",
      "NOTE: The table GAN_DATA_TRAIN has been created in caslib CASUSER(alphel) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; caslib</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASUSER(alphel)</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; tableName</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>GAN_DATA_TRAIN</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; casTable</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASTable('GAN_DATA_TRAIN', caslib='CASUSER(alphel)')</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.0966s</span> &#183; <span class=\"cas-user\">user 0.261s</span> &#183; <span class=\"cas-sys\">sys 0.0435s</span> &#183; <span class=\"cas-memory\">mem 126MB</span></small></p>"
      ],
      "text/plain": [
       "[caslib]\n",
       "\n",
       " 'CASUSER(alphel)'\n",
       "\n",
       "[tableName]\n",
       "\n",
       " 'GAN_DATA_TRAIN'\n",
       "\n",
       "[casTable]\n",
       "\n",
       " CASTable('GAN_DATA_TRAIN', caslib='CASUSER(alphel)')\n",
       "\n",
       "+ Elapsed: 0.0966s, user: 0.261s, sys: 0.0435s, mem: 126mb"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.upload(GAN_data_train, casout=dict(name='GAN_data_train', replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table CEN in caslib CASUSER(alphel).\n",
      "NOTE: The table CEN has been created in caslib CASUSER(alphel) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VarName</th>\n",
       "      <th>Centroid-i</th>\n",
       "      <th>weight</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19322</td>\n",
       "      <td>27.77</td>\n",
       "      <td>11.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>2</td>\n",
       "      <td>0.46569</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05051</td>\n",
       "      <td>53.80</td>\n",
       "      <td>6.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>4</td>\n",
       "      <td>0.13165</td>\n",
       "      <td>52.02</td>\n",
       "      <td>20.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05105</td>\n",
       "      <td>53.95</td>\n",
       "      <td>7.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>6</td>\n",
       "      <td>0.10788</td>\n",
       "      <td>48.49</td>\n",
       "      <td>2.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>age</td>\n",
       "      <td>1</td>\n",
       "      <td>0.14270</td>\n",
       "      <td>31.37</td>\n",
       "      <td>8.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>age</td>\n",
       "      <td>2</td>\n",
       "      <td>0.18711</td>\n",
       "      <td>26.35</td>\n",
       "      <td>5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>age</td>\n",
       "      <td>3</td>\n",
       "      <td>0.16313</td>\n",
       "      <td>43.14</td>\n",
       "      <td>12.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>age</td>\n",
       "      <td>4</td>\n",
       "      <td>0.14163</td>\n",
       "      <td>36.40</td>\n",
       "      <td>10.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>age</td>\n",
       "      <td>5</td>\n",
       "      <td>0.21957</td>\n",
       "      <td>49.69</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>age</td>\n",
       "      <td>6</td>\n",
       "      <td>0.14585</td>\n",
       "      <td>29.30</td>\n",
       "      <td>7.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19857</td>\n",
       "      <td>134121.87</td>\n",
       "      <td>61429.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>2</td>\n",
       "      <td>0.07079</td>\n",
       "      <td>361429.52</td>\n",
       "      <td>185644.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>3</td>\n",
       "      <td>0.21485</td>\n",
       "      <td>287922.30</td>\n",
       "      <td>90902.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>4</td>\n",
       "      <td>0.15934</td>\n",
       "      <td>160820.85</td>\n",
       "      <td>66425.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>5</td>\n",
       "      <td>0.20436</td>\n",
       "      <td>127945.67</td>\n",
       "      <td>60218.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>6</td>\n",
       "      <td>0.15208</td>\n",
       "      <td>175072.62</td>\n",
       "      <td>71618.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>education-num</td>\n",
       "      <td>1</td>\n",
       "      <td>0.21242</td>\n",
       "      <td>7.33</td>\n",
       "      <td>3.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>education-num</td>\n",
       "      <td>2</td>\n",
       "      <td>0.47952</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>education-num</td>\n",
       "      <td>3</td>\n",
       "      <td>0.30806</td>\n",
       "      <td>11.98</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>capital-gain</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91891</td>\n",
       "      <td>0.05</td>\n",
       "      <td>42.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>capital-gain</td>\n",
       "      <td>2</td>\n",
       "      <td>0.02360</td>\n",
       "      <td>6896.89</td>\n",
       "      <td>1386.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>capital-gain</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00491</td>\n",
       "      <td>99413.59</td>\n",
       "      <td>7609.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>capital-gain</td>\n",
       "      <td>4</td>\n",
       "      <td>0.02150</td>\n",
       "      <td>14477.55</td>\n",
       "      <td>5509.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>capital-gain</td>\n",
       "      <td>5</td>\n",
       "      <td>0.03108</td>\n",
       "      <td>3114.35</td>\n",
       "      <td>1197.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>capital-loss</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03581</td>\n",
       "      <td>1834.96</td>\n",
       "      <td>208.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>capital-loss</td>\n",
       "      <td>2</td>\n",
       "      <td>0.95209</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>capital-loss</td>\n",
       "      <td>3</td>\n",
       "      <td>0.01120</td>\n",
       "      <td>2103.41</td>\n",
       "      <td>542.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>capital-loss</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00090</td>\n",
       "      <td>542.76</td>\n",
       "      <td>229.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>label</td>\n",
       "      <td>1</td>\n",
       "      <td>0.23984</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>label</td>\n",
       "      <td>2</td>\n",
       "      <td>0.76016</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           VarName  Centroid-i   weight       Mean        Std\n",
       "0   hours-per-week           1  0.19322      27.77      11.92\n",
       "1   hours-per-week           2  0.46569      40.00       0.10\n",
       "2   hours-per-week           3  0.05051      53.80       6.97\n",
       "3   hours-per-week           4  0.13165      52.02      20.72\n",
       "4   hours-per-week           5  0.05105      53.95       7.23\n",
       "5   hours-per-week           6  0.10788      48.49       2.39\n",
       "6              age           1  0.14270      31.37       8.39\n",
       "7              age           2  0.18711      26.35       5.90\n",
       "8              age           3  0.16313      43.14      12.26\n",
       "9              age           4  0.14163      36.40      10.26\n",
       "10             age           5  0.21957      49.69      13.50\n",
       "11             age           6  0.14585      29.30       7.47\n",
       "12          fnlwgt           1  0.19857  134121.87   61429.73\n",
       "13          fnlwgt           2  0.07079  361429.52  185644.38\n",
       "14          fnlwgt           3  0.21485  287922.30   90902.78\n",
       "15          fnlwgt           4  0.15934  160820.85   66425.34\n",
       "16          fnlwgt           5  0.20436  127945.67   60218.04\n",
       "17          fnlwgt           6  0.15208  175072.62   71618.94\n",
       "18   education-num           1  0.21242       7.33       3.24\n",
       "19   education-num           2  0.47952       9.30       0.47\n",
       "20   education-num           3  0.30806      11.98       2.08\n",
       "21    capital-gain           1  0.91891       0.05      42.18\n",
       "22    capital-gain           2  0.02360    6896.89    1386.30\n",
       "23    capital-gain           3  0.00491   99413.59    7609.05\n",
       "24    capital-gain           4  0.02150   14477.55    5509.19\n",
       "25    capital-gain           5  0.03108    3114.35    1197.44\n",
       "26    capital-loss           1  0.03581    1834.96     208.73\n",
       "27    capital-loss           2  0.95209       0.00       2.32\n",
       "28    capital-loss           3  0.01120    2103.41     542.26\n",
       "29    capital-loss           4  0.00090     542.76     229.63\n",
       "30           label           1  0.23984       1.00       0.01\n",
       "31           label           2  0.76016       0.00       0.00"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cen = pd.read_csv(\"../gan-testing/data/adult_centroids.csv\")\n",
    "s.upload(cen, casout=dict(name='cen', replace=True))\n",
    "cen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using device: GPU 0.\n",
      "NOTE: Epoch i=1, ae_loss=  0.0495.\n",
      "NOTE: Epoch i=2, ae_loss=  0.0378.\n",
      "NOTE: Epoch i=3, ae_loss=  0.0293.\n",
      "NOTE: Epoch i=4, ae_loss=  0.0231.\n",
      "NOTE: Epoch i=5, ae_loss=  0.0203.\n",
      "NOTE: Epoch i=6, ae_loss=  0.0194.\n",
      "NOTE: Epoch i=7, ae_loss=  0.0180.\n",
      "NOTE: Epoch i=8, ae_loss=  0.0164.\n",
      "NOTE: Epoch i=9, ae_loss=  0.0154.\n",
      "NOTE: Epoch i=10, ae_loss=  0.0138.\n",
      "NOTE: Epoch i=11, ae_loss=  0.0137.\n",
      "NOTE: Epoch i=12, ae_loss=  0.0138.\n",
      "NOTE: Epoch i=13, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=14, ae_loss=  0.0125.\n",
      "NOTE: Epoch i=15, ae_loss=  0.0120.\n",
      "NOTE: Epoch i=16, ae_loss=  0.0117.\n",
      "NOTE: Epoch i=17, ae_loss=  0.0119.\n",
      "NOTE: Epoch i=18, ae_loss=  0.0110.\n",
      "NOTE: Epoch i=19, ae_loss=  0.0117.\n",
      "NOTE: Epoch i=20, ae_loss=  0.0112.\n",
      "NOTE: Epoch i=21, ae_loss=  0.0119.\n",
      "NOTE: Epoch i=22, ae_loss=  0.0101.\n",
      "NOTE: Epoch i=23, ae_loss=  0.0111.\n",
      "NOTE: Epoch i=24, ae_loss=  0.0097.\n",
      "NOTE: Epoch i=25, ae_loss=  0.0096.\n",
      "NOTE: Epoch i=26, ae_loss=  0.0104.\n",
      "NOTE: Epoch i=27, ae_loss=  0.0098.\n",
      "NOTE: Epoch i=28, ae_loss=  0.0092.\n",
      "NOTE: Epoch i=29, ae_loss=  0.0092.\n",
      "NOTE: Epoch i=30, ae_loss=  0.0094.\n",
      "NOTE: Epoch i=31, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=32, ae_loss=  0.0088.\n",
      "NOTE: Epoch i=33, ae_loss=  0.0096.\n",
      "NOTE: Epoch i=34, ae_loss=  0.0090.\n",
      "NOTE: Epoch i=35, ae_loss=  0.0090.\n",
      "NOTE: Epoch i=36, ae_loss=  0.0095.\n",
      "NOTE: Epoch i=37, ae_loss=  0.0091.\n",
      "NOTE: Epoch i=38, ae_loss=  0.0101.\n",
      "NOTE: Epoch i=39, ae_loss=  0.0092.\n",
      "NOTE: Epoch i=40, ae_loss=  0.0092.\n",
      "NOTE: Epoch i=41, ae_loss=  0.0093.\n",
      "NOTE: Epoch i=42, ae_loss=  0.0081.\n",
      "NOTE: Epoch i=43, ae_loss=  0.0088.\n",
      "NOTE: Epoch i=44, ae_loss=  0.0088.\n",
      "NOTE: Epoch i=45, ae_loss=  0.0085.\n",
      "NOTE: Epoch i=46, ae_loss=  0.0094.\n",
      "NOTE: Epoch i=47, ae_loss=  0.0084.\n",
      "NOTE: Epoch i=48, ae_loss=  0.0090.\n",
      "NOTE: Epoch i=49, ae_loss=  0.0081.\n",
      "NOTE: Epoch i=50, ae_loss=  0.0089.\n",
      "NOTE: Epoch i=51, ae_loss=  0.0083.\n",
      "NOTE: Epoch i=52, ae_loss=  0.0086.\n",
      "NOTE: Epoch i=53, ae_loss=  0.0087.\n",
      "NOTE: Epoch i=54, ae_loss=  0.0084.\n",
      "NOTE: Epoch i=55, ae_loss=  0.0082.\n",
      "NOTE: Epoch i=56, ae_loss=  0.0086.\n",
      "NOTE: Epoch i=57, ae_loss=  0.0087.\n",
      "NOTE: Epoch i=58, ae_loss=  0.0082.\n",
      "NOTE: Epoch i=59, ae_loss=  0.0079.\n",
      "NOTE: Epoch i=60, ae_loss=  0.0082.\n",
      "NOTE: Epoch i=61, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=62, ae_loss=  0.0086.\n",
      "NOTE: Epoch i=63, ae_loss=  0.0089.\n",
      "NOTE: Epoch i=64, ae_loss=  0.0082.\n",
      "NOTE: Epoch i=65, ae_loss=  0.0081.\n",
      "NOTE: Epoch i=66, ae_loss=  0.0090.\n",
      "NOTE: Epoch i=67, ae_loss=  0.0081.\n",
      "NOTE: Epoch i=68, ae_loss=  0.0086.\n",
      "NOTE: Epoch i=69, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=70, ae_loss=  0.0081.\n",
      "NOTE: Epoch i=71, ae_loss=  0.0084.\n",
      "NOTE: Epoch i=72, ae_loss=  0.0081.\n",
      "NOTE: Epoch i=73, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=74, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=75, ae_loss=  0.0087.\n",
      "NOTE: Epoch i=76, ae_loss=  0.0083.\n",
      "NOTE: Epoch i=77, ae_loss=  0.0083.\n",
      "NOTE: Epoch i=78, ae_loss=  0.0082.\n",
      "NOTE: Epoch i=79, ae_loss=  0.0083.\n",
      "NOTE: Epoch i=80, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=81, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=82, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=83, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=84, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=85, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=86, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=87, ae_loss=  0.0088.\n",
      "NOTE: Epoch i=88, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=89, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=90, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=91, ae_loss=  0.0077.\n",
      "NOTE: Epoch i=92, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=93, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=94, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=95, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=96, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=97, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=98, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=99, ae_loss=  0.0085.\n",
      "NOTE: Epoch i=100, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=101, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=102, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=103, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=104, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=105, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=106, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=107, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=108, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=109, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=110, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=111, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=112, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=113, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=114, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=115, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=116, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=117, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=118, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=119, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=120, ae_loss=  0.0072.\n",
      "NOTE: Epoch i=121, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=122, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=123, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=124, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=125, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=126, ae_loss=  0.0078.\n",
      "NOTE: Epoch i=127, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=128, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=129, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=130, ae_loss=  0.0076.\n",
      "NOTE: Epoch i=131, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=132, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=133, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=134, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=135, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=136, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=137, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=138, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=139, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=140, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=141, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=142, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=143, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=144, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=145, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=146, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=147, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=148, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=149, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=150, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=151, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=152, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=153, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=154, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=155, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=156, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=157, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=158, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=159, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=160, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=161, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=162, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=163, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=164, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=165, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=166, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=167, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=168, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=169, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=170, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=171, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=172, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=173, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=174, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=175, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=176, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=177, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=178, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=179, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=180, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=181, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=182, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=183, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=184, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=185, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=186, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=187, ae_loss=  0.0073.\n",
      "NOTE: Epoch i=188, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=189, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=190, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=191, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=192, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=193, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=194, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=195, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=196, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=197, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=198, ae_loss=  0.0074.\n",
      "NOTE: Epoch i=199, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=200, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=201, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=202, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=203, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=204, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=205, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=206, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=207, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=208, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=209, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=210, ae_loss=  0.0070.\n",
      "NOTE: Epoch i=211, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=212, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=213, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=214, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=215, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=216, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=217, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=218, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=219, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=220, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=221, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=222, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=223, ae_loss=  0.0069.\n",
      "NOTE: Epoch i=224, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=225, ae_loss=  0.0070.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=226, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=227, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=228, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=229, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=230, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=231, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=232, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=233, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=234, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=235, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=236, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=237, ae_loss=  0.0075.\n",
      "NOTE: Epoch i=238, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=239, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=240, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=241, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=242, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=243, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=244, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=245, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=246, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=247, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=248, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=249, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=250, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=251, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=252, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=253, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=254, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=255, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=256, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=257, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=258, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=259, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=260, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=261, ae_loss=  0.0067.\n",
      "NOTE: Epoch i=262, ae_loss=  0.0068.\n",
      "NOTE: Epoch i=263, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=264, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=265, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=266, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=267, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=268, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=269, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=270, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=271, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=272, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=273, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=274, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=275, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=276, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=277, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=278, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=279, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=280, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=281, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=282, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=283, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=284, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=285, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=286, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=287, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=288, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=289, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=290, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=291, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=292, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=293, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=294, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=295, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=296, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=297, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=298, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=299, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=300, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=301, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=302, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=303, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=304, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=305, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=306, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=307, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=308, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=309, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=310, ae_loss=  0.0066.\n",
      "NOTE: Epoch i=311, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=312, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=313, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=314, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=315, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=316, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=317, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=318, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=319, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=320, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=321, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=322, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=323, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=324, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=325, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=326, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=327, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=328, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=329, ae_loss=  0.0051.\n",
      "NOTE: Epoch i=330, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=331, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=332, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=333, ae_loss=  0.0049.\n",
      "NOTE: Epoch i=334, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=335, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=336, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=337, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=338, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=339, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=340, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=341, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=342, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=343, ae_loss=  0.0051.\n",
      "NOTE: Epoch i=344, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=345, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=346, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=347, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=348, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=349, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=350, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=351, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=352, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=353, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=354, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=355, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=356, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=357, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=358, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=359, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=360, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=361, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=362, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=363, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=364, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=365, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=366, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=367, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=368, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=369, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=370, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=371, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=372, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=373, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=374, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=375, ae_loss=  0.0049.\n",
      "NOTE: Epoch i=376, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=377, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=378, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=379, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=380, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=381, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=382, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=383, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=384, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=385, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=386, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=387, ae_loss=  0.0051.\n",
      "NOTE: Epoch i=388, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=389, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=390, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=391, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=392, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=393, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=394, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=395, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=396, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=397, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=398, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=399, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=400, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=401, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=402, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=403, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=404, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=405, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=406, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=407, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=408, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=409, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=410, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=411, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=412, ae_loss=  0.0051.\n",
      "NOTE: Epoch i=413, ae_loss=  0.0050.\n",
      "NOTE: Epoch i=414, ae_loss=  0.0050.\n",
      "NOTE: Epoch i=415, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=416, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=417, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=418, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=419, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=420, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=421, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=422, ae_loss=  0.0050.\n",
      "NOTE: Epoch i=423, ae_loss=  0.0051.\n",
      "NOTE: Epoch i=424, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=425, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=426, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=427, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=428, ae_loss=  0.0063.\n",
      "NOTE: Epoch i=429, ae_loss=  0.0065.\n",
      "NOTE: Epoch i=430, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=431, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=432, ae_loss=  0.0049.\n",
      "NOTE: Epoch i=433, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=434, ae_loss=  0.0050.\n",
      "NOTE: Epoch i=435, ae_loss=  0.0049.\n",
      "NOTE: Epoch i=436, ae_loss=  0.0050.\n",
      "NOTE: Epoch i=437, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=438, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=439, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=440, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=441, ae_loss=  0.0062.\n",
      "NOTE: Epoch i=442, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=443, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=444, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=445, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=446, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=447, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=448, ae_loss=  0.0048.\n",
      "NOTE: Epoch i=449, ae_loss=  0.0049.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=450, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=451, ae_loss=  0.0051.\n",
      "NOTE: Epoch i=452, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=453, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=454, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=455, ae_loss=  0.0059.\n",
      "NOTE: Epoch i=456, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=457, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=458, ae_loss=  0.0050.\n",
      "NOTE: Epoch i=459, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=460, ae_loss=  0.0061.\n",
      "NOTE: Epoch i=461, ae_loss=  0.0046.\n",
      "NOTE: Epoch i=462, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=463, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=464, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=465, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=466, ae_loss=  0.0046.\n",
      "NOTE: Epoch i=467, ae_loss=  0.0054.\n",
      "NOTE: Epoch i=468, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=469, ae_loss=  0.0057.\n",
      "NOTE: Epoch i=470, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=471, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=472, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=473, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=474, ae_loss=  0.0050.\n",
      "NOTE: Epoch i=475, ae_loss=  0.0050.\n",
      "NOTE: Epoch i=476, ae_loss=  0.0050.\n",
      "NOTE: Epoch i=477, ae_loss=  0.0050.\n",
      "NOTE: Epoch i=478, ae_loss=  0.0049.\n",
      "NOTE: Epoch i=479, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=480, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=481, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=482, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=483, ae_loss=  0.0060.\n",
      "NOTE: Epoch i=484, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=485, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=486, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=487, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=488, ae_loss=  0.0050.\n",
      "NOTE: Epoch i=489, ae_loss=  0.0056.\n",
      "NOTE: Epoch i=490, ae_loss=  0.0047.\n",
      "NOTE: Epoch i=491, ae_loss=  0.0071.\n",
      "NOTE: Epoch i=492, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=493, ae_loss=  0.0052.\n",
      "NOTE: Epoch i=494, ae_loss=  0.0051.\n",
      "NOTE: Epoch i=495, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=496, ae_loss=  0.0064.\n",
      "NOTE: Epoch i=497, ae_loss=  0.0049.\n",
      "NOTE: Epoch i=498, ae_loss=  0.0055.\n",
      "NOTE: Epoch i=499, ae_loss=  0.0053.\n",
      "NOTE: Epoch i=500, ae_loss=  0.0058.\n",
      "NOTE: Epoch i=1, g_loss=  2.0782, d_loss= -0.0153.\n",
      "NOTE: Epoch i=2, g_loss=  2.0754, d_loss= -0.0748.\n",
      "NOTE: Epoch i=3, g_loss=  2.1256, d_loss= -0.2496.\n",
      "NOTE: Epoch i=4, g_loss=  2.2254, d_loss= -0.3704.\n",
      "NOTE: Epoch i=5, g_loss=  2.1940, d_loss= -0.4684.\n",
      "NOTE: Epoch i=6, g_loss=  2.1662, d_loss= -0.4870.\n",
      "NOTE: Epoch i=7, g_loss=  2.2175, d_loss= -0.8318.\n",
      "NOTE: Epoch i=8, g_loss=  2.2443, d_loss= -0.7013.\n",
      "NOTE: Epoch i=9, g_loss=  2.2812, d_loss= -0.8875.\n",
      "NOTE: Epoch i=10, g_loss=  2.1990, d_loss= -0.8515.\n",
      "NOTE: Epoch i=11, g_loss=  2.2683, d_loss= -0.8438.\n",
      "NOTE: Epoch i=12, g_loss=  2.3263, d_loss= -0.7940.\n",
      "NOTE: Epoch i=13, g_loss=  2.4286, d_loss= -0.8799.\n",
      "NOTE: Epoch i=14, g_loss=  2.2952, d_loss= -0.7295.\n",
      "NOTE: Epoch i=15, g_loss=  2.5349, d_loss= -0.8108.\n",
      "NOTE: Epoch i=16, g_loss=  2.5006, d_loss= -0.9446.\n",
      "NOTE: Epoch i=17, g_loss=  2.6230, d_loss= -0.8646.\n",
      "NOTE: Epoch i=18, g_loss=  2.6570, d_loss= -0.5864.\n",
      "NOTE: Epoch i=19, g_loss=  2.6013, d_loss= -0.7964.\n",
      "NOTE: Epoch i=20, g_loss=  2.7437, d_loss= -0.5644.\n",
      "NOTE: Epoch i=21, g_loss=  2.7295, d_loss= -0.5372.\n",
      "NOTE: Epoch i=22, g_loss=  2.7154, d_loss= -0.5611.\n",
      "NOTE: Epoch i=23, g_loss=  2.8306, d_loss= -0.5121.\n",
      "NOTE: Epoch i=24, g_loss=  2.8720, d_loss= -0.7030.\n",
      "NOTE: Epoch i=25, g_loss=  2.8404, d_loss= -0.3746.\n",
      "NOTE: Epoch i=26, g_loss=  3.0303, d_loss= -0.5472.\n",
      "NOTE: Epoch i=27, g_loss=  3.0925, d_loss= -0.3688.\n",
      "NOTE: Epoch i=28, g_loss=  3.0816, d_loss= -0.5985.\n",
      "NOTE: Epoch i=29, g_loss=  3.0765, d_loss= -0.4348.\n",
      "NOTE: Epoch i=30, g_loss=  3.0632, d_loss= -0.4548.\n",
      "NOTE: Epoch i=31, g_loss=  2.9512, d_loss= -0.1643.\n",
      "NOTE: Epoch i=32, g_loss=  2.9855, d_loss= -0.4653.\n",
      "NOTE: Epoch i=33, g_loss=  2.7186, d_loss= -0.3037.\n",
      "NOTE: Epoch i=34, g_loss=  2.3908, d_loss= -0.2034.\n",
      "NOTE: Epoch i=35, g_loss=  2.2789, d_loss= -0.0718.\n",
      "NOTE: Epoch i=36, g_loss=  1.6655, d_loss=  0.0195.\n",
      "NOTE: Epoch i=37, g_loss=  1.6012, d_loss= -0.1030.\n",
      "NOTE: Epoch i=38, g_loss=  1.2414, d_loss= -0.0999.\n",
      "NOTE: Epoch i=39, g_loss=  1.0605, d_loss= -0.0583.\n",
      "NOTE: Epoch i=40, g_loss=  0.9375, d_loss=  0.1152.\n",
      "NOTE: Epoch i=41, g_loss=  0.3794, d_loss=  0.0420.\n",
      "NOTE: Epoch i=42, g_loss=  0.2263, d_loss= -0.0516.\n",
      "NOTE: Epoch i=43, g_loss=  0.1412, d_loss=  0.0376.\n",
      "NOTE: Epoch i=44, g_loss= -0.2102, d_loss= -0.1448.\n",
      "NOTE: Epoch i=45, g_loss= -0.4353, d_loss=  0.2167.\n",
      "NOTE: Epoch i=46, g_loss= -0.7217, d_loss= -0.0228.\n",
      "NOTE: Epoch i=47, g_loss= -0.6752, d_loss=  0.0354.\n",
      "NOTE: Epoch i=48, g_loss= -0.7544, d_loss=  0.3176.\n",
      "NOTE: Epoch i=49, g_loss= -0.9077, d_loss=  0.1635.\n",
      "NOTE: Epoch i=50, g_loss= -0.9488, d_loss=  0.0368.\n",
      "NOTE: Epoch i=51, g_loss= -0.9003, d_loss=  0.0037.\n",
      "NOTE: Epoch i=52, g_loss= -1.0115, d_loss=  0.0932.\n",
      "NOTE: Epoch i=53, g_loss= -0.8543, d_loss= -0.0060.\n",
      "NOTE: Epoch i=54, g_loss= -0.8236, d_loss= -0.0360.\n",
      "NOTE: Epoch i=55, g_loss= -0.8851, d_loss= -0.0794.\n",
      "NOTE: Epoch i=56, g_loss= -0.9408, d_loss= -0.1320.\n",
      "NOTE: Epoch i=57, g_loss= -0.7282, d_loss= -0.1162.\n",
      "NOTE: Epoch i=58, g_loss= -0.5386, d_loss= -0.0953.\n",
      "NOTE: Epoch i=59, g_loss= -0.4818, d_loss= -0.3213.\n",
      "NOTE: Epoch i=60, g_loss= -0.3344, d_loss= -0.0225.\n",
      "NOTE: Epoch i=61, g_loss= -0.2860, d_loss= -0.1898.\n",
      "NOTE: Epoch i=62, g_loss= -0.1742, d_loss= -0.1571.\n",
      "NOTE: Epoch i=63, g_loss=  0.1690, d_loss= -0.1487.\n",
      "NOTE: Epoch i=64, g_loss=  0.3517, d_loss= -0.0037.\n",
      "NOTE: Epoch i=65, g_loss=  0.5535, d_loss= -0.1280.\n",
      "NOTE: Epoch i=66, g_loss=  0.6534, d_loss= -0.0014.\n",
      "NOTE: Epoch i=67, g_loss=  0.8987, d_loss= -0.1406.\n",
      "NOTE: Epoch i=68, g_loss=  1.1091, d_loss= -0.1175.\n",
      "NOTE: Epoch i=69, g_loss=  1.3208, d_loss= -0.2689.\n",
      "NOTE: Epoch i=70, g_loss=  1.3940, d_loss= -0.3069.\n",
      "NOTE: Epoch i=71, g_loss=  1.4489, d_loss= -0.2120.\n",
      "NOTE: Epoch i=72, g_loss=  1.4730, d_loss= -0.1080.\n",
      "NOTE: Epoch i=73, g_loss=  1.4849, d_loss= -0.3051.\n",
      "NOTE: Epoch i=74, g_loss=  1.7277, d_loss= -0.1623.\n",
      "NOTE: Epoch i=75, g_loss=  1.6016, d_loss= -0.1863.\n",
      "NOTE: Epoch i=76, g_loss=  1.4028, d_loss= -0.1516.\n",
      "NOTE: Epoch i=77, g_loss=  1.4002, d_loss= -0.0619.\n",
      "NOTE: Epoch i=78, g_loss=  1.2823, d_loss= -0.0252.\n",
      "NOTE: Epoch i=79, g_loss=  1.2789, d_loss= -0.0915.\n",
      "NOTE: Epoch i=80, g_loss=  1.2387, d_loss= -0.0755.\n",
      "NOTE: Epoch i=81, g_loss=  1.0615, d_loss= -0.0844.\n",
      "NOTE: Epoch i=82, g_loss=  0.9635, d_loss=  0.0221.\n",
      "NOTE: Epoch i=83, g_loss=  0.9563, d_loss= -0.2917.\n",
      "NOTE: Epoch i=84, g_loss=  0.7570, d_loss=  0.0680.\n",
      "NOTE: Epoch i=85, g_loss=  0.5735, d_loss=  0.0078.\n",
      "NOTE: Epoch i=86, g_loss=  0.4031, d_loss=  0.0218.\n",
      "NOTE: Epoch i=87, g_loss=  0.2098, d_loss= -0.0907.\n",
      "NOTE: Epoch i=88, g_loss=  0.0118, d_loss= -0.0422.\n",
      "NOTE: Epoch i=89, g_loss= -0.3975, d_loss= -0.2054.\n",
      "NOTE: Epoch i=90, g_loss= -0.4123, d_loss= -0.1165.\n",
      "NOTE: Epoch i=91, g_loss= -0.7427, d_loss= -0.2889.\n",
      "NOTE: Epoch i=92, g_loss= -0.9116, d_loss= -0.4513.\n",
      "NOTE: Epoch i=93, g_loss= -0.9149, d_loss= -0.0367.\n",
      "NOTE: Epoch i=94, g_loss= -1.0344, d_loss=  0.0277.\n",
      "NOTE: Epoch i=95, g_loss= -0.8704, d_loss=  0.1541.\n",
      "NOTE: Epoch i=96, g_loss= -1.0127, d_loss=  0.0960.\n",
      "NOTE: Epoch i=97, g_loss= -0.7153, d_loss= -0.0091.\n",
      "NOTE: Epoch i=98, g_loss= -0.5214, d_loss= -0.0086.\n",
      "NOTE: Epoch i=99, g_loss= -0.2458, d_loss=  0.0663.\n",
      "NOTE: Epoch i=100, g_loss=  0.0108, d_loss= -0.0147.\n",
      "NOTE: Epoch i=101, g_loss=  0.0931, d_loss= -0.0805.\n",
      "NOTE: Epoch i=102, g_loss=  0.3395, d_loss= -0.0042.\n",
      "NOTE: Epoch i=103, g_loss=  0.6424, d_loss= -0.0514.\n",
      "NOTE: Epoch i=104, g_loss=  0.7156, d_loss= -0.0596.\n",
      "NOTE: Epoch i=105, g_loss=  0.8435, d_loss=  0.0267.\n",
      "NOTE: Epoch i=106, g_loss=  1.0647, d_loss= -0.1414.\n",
      "NOTE: Epoch i=107, g_loss=  1.1666, d_loss= -0.2557.\n",
      "NOTE: Epoch i=108, g_loss=  1.3352, d_loss= -0.0458.\n",
      "NOTE: Epoch i=109, g_loss=  1.0220, d_loss= -0.0374.\n",
      "NOTE: Epoch i=110, g_loss=  1.0431, d_loss= -0.0050.\n",
      "NOTE: Epoch i=111, g_loss=  1.1483, d_loss=  0.1178.\n",
      "NOTE: Epoch i=112, g_loss=  1.0756, d_loss= -0.0660.\n",
      "NOTE: Epoch i=113, g_loss=  0.9662, d_loss=  0.1388.\n",
      "NOTE: Epoch i=114, g_loss=  0.8220, d_loss= -0.1459.\n",
      "NOTE: Epoch i=115, g_loss=  0.6984, d_loss= -0.0532.\n",
      "NOTE: Epoch i=116, g_loss=  0.5848, d_loss= -0.1978.\n",
      "NOTE: Epoch i=117, g_loss=  0.2341, d_loss=  0.0296.\n",
      "NOTE: Epoch i=118, g_loss=  0.0938, d_loss= -0.1317.\n",
      "NOTE: Epoch i=119, g_loss=  0.1827, d_loss= -0.2402.\n",
      "NOTE: Epoch i=120, g_loss=  0.0689, d_loss= -0.0878.\n",
      "NOTE: Epoch i=121, g_loss=  0.0428, d_loss= -0.0622.\n",
      "NOTE: Epoch i=122, g_loss=  0.1810, d_loss= -0.1097.\n",
      "NOTE: Epoch i=123, g_loss=  0.1608, d_loss= -0.2206.\n",
      "NOTE: Epoch i=124, g_loss=  0.3023, d_loss= -0.2765.\n",
      "NOTE: Epoch i=125, g_loss=  0.4674, d_loss=  0.1312.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=126, g_loss=  0.6079, d_loss=  0.0305.\n",
      "NOTE: Epoch i=127, g_loss=  0.9209, d_loss= -0.1238.\n",
      "NOTE: Epoch i=128, g_loss=  0.9482, d_loss= -0.0740.\n",
      "NOTE: Epoch i=129, g_loss=  0.9777, d_loss= -0.1693.\n",
      "NOTE: Epoch i=130, g_loss=  1.1318, d_loss= -0.1013.\n",
      "NOTE: Epoch i=131, g_loss=  1.3120, d_loss= -0.1797.\n",
      "NOTE: Epoch i=132, g_loss=  1.3739, d_loss= -0.0940.\n",
      "NOTE: Epoch i=133, g_loss=  1.3707, d_loss= -0.0776.\n",
      "NOTE: Epoch i=134, g_loss=  1.2097, d_loss= -0.3083.\n",
      "NOTE: Epoch i=135, g_loss=  1.1325, d_loss=  0.0268.\n",
      "NOTE: Epoch i=136, g_loss=  1.0349, d_loss= -0.3362.\n",
      "NOTE: Epoch i=137, g_loss=  0.9072, d_loss=  0.1278.\n",
      "NOTE: Epoch i=138, g_loss=  0.8128, d_loss= -0.1848.\n",
      "NOTE: Epoch i=139, g_loss=  0.7343, d_loss=  0.0009.\n",
      "NOTE: Epoch i=140, g_loss=  0.7893, d_loss= -0.2975.\n",
      "NOTE: Epoch i=141, g_loss=  0.6235, d_loss= -0.1008.\n",
      "NOTE: Epoch i=142, g_loss=  0.5937, d_loss= -0.0566.\n",
      "NOTE: Epoch i=143, g_loss=  0.7939, d_loss= -0.0567.\n",
      "NOTE: Epoch i=144, g_loss=  0.8305, d_loss= -0.1321.\n",
      "NOTE: Epoch i=145, g_loss=  1.0025, d_loss= -0.1341.\n",
      "NOTE: Epoch i=146, g_loss=  1.0309, d_loss= -0.3144.\n",
      "NOTE: Epoch i=147, g_loss=  1.1338, d_loss= -0.0499.\n",
      "NOTE: Epoch i=148, g_loss=  1.1650, d_loss= -0.2582.\n",
      "NOTE: Epoch i=149, g_loss=  1.1313, d_loss= -0.0120.\n",
      "NOTE: Epoch i=150, g_loss=  1.4071, d_loss=  0.1171.\n",
      "NOTE: Epoch i=151, g_loss=  1.5168, d_loss= -0.0705.\n",
      "NOTE: Epoch i=152, g_loss=  1.3829, d_loss= -0.0973.\n",
      "NOTE: Epoch i=153, g_loss=  1.0418, d_loss= -0.1671.\n",
      "NOTE: Epoch i=154, g_loss=  1.1400, d_loss=  0.0708.\n",
      "NOTE: Epoch i=155, g_loss=  1.0057, d_loss= -0.2550.\n",
      "NOTE: Epoch i=156, g_loss=  1.0135, d_loss= -0.1404.\n",
      "NOTE: Epoch i=157, g_loss=  1.1062, d_loss= -0.0731.\n",
      "NOTE: Epoch i=158, g_loss=  0.9206, d_loss= -0.0006.\n",
      "NOTE: Epoch i=159, g_loss=  1.0634, d_loss= -0.1123.\n",
      "NOTE: Epoch i=160, g_loss=  1.1957, d_loss= -0.0999.\n",
      "NOTE: Epoch i=161, g_loss=  1.1852, d_loss= -0.1635.\n",
      "NOTE: Epoch i=162, g_loss=  1.1197, d_loss=  0.1653.\n",
      "NOTE: Epoch i=163, g_loss=  1.0862, d_loss= -0.1781.\n",
      "NOTE: Epoch i=164, g_loss=  1.2907, d_loss= -0.1627.\n",
      "NOTE: Epoch i=165, g_loss=  1.1507, d_loss= -0.0113.\n",
      "NOTE: Epoch i=166, g_loss=  1.2613, d_loss= -0.1362.\n",
      "NOTE: Epoch i=167, g_loss=  1.4122, d_loss= -0.1008.\n",
      "NOTE: Epoch i=168, g_loss=  1.5035, d_loss= -0.0239.\n",
      "NOTE: Epoch i=169, g_loss=  1.3779, d_loss= -0.0723.\n",
      "NOTE: Epoch i=170, g_loss=  0.9387, d_loss= -0.0501.\n",
      "NOTE: Epoch i=171, g_loss=  0.9594, d_loss= -0.1828.\n",
      "NOTE: Epoch i=172, g_loss=  1.2199, d_loss= -0.0613.\n",
      "NOTE: Epoch i=173, g_loss=  1.0870, d_loss= -0.1348.\n",
      "NOTE: Epoch i=174, g_loss=  1.0730, d_loss=  0.0695.\n",
      "NOTE: Epoch i=175, g_loss=  1.1304, d_loss= -0.2879.\n",
      "NOTE: Epoch i=176, g_loss=  1.0534, d_loss= -0.0899.\n",
      "NOTE: Epoch i=177, g_loss=  1.1723, d_loss= -0.0997.\n",
      "NOTE: Epoch i=178, g_loss=  1.2090, d_loss= -0.1002.\n",
      "NOTE: Epoch i=179, g_loss=  1.3924, d_loss= -0.2378.\n",
      "NOTE: Epoch i=180, g_loss=  1.1281, d_loss= -0.0007.\n",
      "NOTE: Epoch i=181, g_loss=  1.1014, d_loss= -0.0534.\n",
      "NOTE: Epoch i=182, g_loss=  1.0383, d_loss=  0.0621.\n",
      "NOTE: Epoch i=183, g_loss=  1.2732, d_loss= -0.0605.\n",
      "NOTE: Epoch i=184, g_loss=  1.0542, d_loss= -0.0750.\n",
      "NOTE: Epoch i=185, g_loss=  1.2164, d_loss=  0.0353.\n",
      "NOTE: Epoch i=186, g_loss=  1.1768, d_loss=  0.0636.\n",
      "NOTE: Epoch i=187, g_loss=  1.0040, d_loss=  0.1201.\n",
      "NOTE: Epoch i=188, g_loss=  1.1999, d_loss= -0.1135.\n",
      "NOTE: Epoch i=189, g_loss=  1.0847, d_loss= -0.1511.\n",
      "NOTE: Epoch i=190, g_loss=  1.2642, d_loss= -0.2934.\n",
      "NOTE: Epoch i=191, g_loss=  1.0600, d_loss=  0.0342.\n",
      "NOTE: Epoch i=192, g_loss=  1.3133, d_loss=  0.1093.\n",
      "NOTE: Epoch i=193, g_loss=  1.3320, d_loss= -0.2975.\n",
      "NOTE: Epoch i=194, g_loss=  1.2386, d_loss=  0.0574.\n",
      "NOTE: Epoch i=195, g_loss=  1.2497, d_loss=  0.0608.\n",
      "NOTE: Epoch i=196, g_loss=  1.1084, d_loss= -0.0091.\n",
      "NOTE: Epoch i=197, g_loss=  1.3530, d_loss=  0.0776.\n",
      "NOTE: Epoch i=198, g_loss=  1.2119, d_loss=  0.0609.\n",
      "NOTE: Epoch i=199, g_loss=  1.3055, d_loss= -0.0568.\n",
      "NOTE: Epoch i=200, g_loss=  1.2217, d_loss= -0.2898.\n",
      "NOTE: Epoch i=201, g_loss=  1.2395, d_loss= -0.1970.\n",
      "NOTE: Epoch i=202, g_loss=  1.3274, d_loss=  0.0159.\n",
      "NOTE: Epoch i=203, g_loss=  1.1670, d_loss= -0.1029.\n",
      "NOTE: Epoch i=204, g_loss=  1.5089, d_loss= -0.1335.\n",
      "NOTE: Epoch i=205, g_loss=  1.3537, d_loss= -0.1338.\n",
      "NOTE: Epoch i=206, g_loss=  1.3839, d_loss= -0.1722.\n",
      "NOTE: Epoch i=207, g_loss=  1.2373, d_loss= -0.1217.\n",
      "NOTE: Epoch i=208, g_loss=  1.2116, d_loss= -0.0386.\n",
      "NOTE: Epoch i=209, g_loss=  1.3301, d_loss= -0.0122.\n",
      "NOTE: Epoch i=210, g_loss=  1.2031, d_loss= -0.0444.\n",
      "NOTE: Epoch i=211, g_loss=  1.2751, d_loss=  0.0635.\n",
      "NOTE: Epoch i=212, g_loss=  1.3252, d_loss= -0.2094.\n",
      "NOTE: Epoch i=213, g_loss=  1.3344, d_loss= -0.1425.\n",
      "NOTE: Epoch i=214, g_loss=  1.4028, d_loss= -0.3550.\n",
      "NOTE: Epoch i=215, g_loss=  1.2848, d_loss= -0.0832.\n",
      "NOTE: Epoch i=216, g_loss=  1.4388, d_loss= -0.0142.\n",
      "NOTE: Epoch i=217, g_loss=  1.2640, d_loss=  0.0357.\n",
      "NOTE: Epoch i=218, g_loss=  1.2492, d_loss= -0.0716.\n",
      "NOTE: Epoch i=219, g_loss=  1.3136, d_loss= -0.1193.\n",
      "NOTE: Epoch i=220, g_loss=  1.3936, d_loss= -0.0947.\n",
      "NOTE: Epoch i=221, g_loss=  1.3217, d_loss= -0.1562.\n",
      "NOTE: Epoch i=222, g_loss=  1.3964, d_loss= -0.0735.\n",
      "NOTE: Epoch i=223, g_loss=  1.3811, d_loss= -0.1617.\n",
      "NOTE: Epoch i=224, g_loss=  1.2989, d_loss= -0.0871.\n",
      "NOTE: Epoch i=225, g_loss=  1.5229, d_loss= -0.0067.\n",
      "NOTE: Epoch i=226, g_loss=  1.4542, d_loss=  0.1292.\n",
      "NOTE: Epoch i=227, g_loss=  1.3397, d_loss= -0.4148.\n",
      "NOTE: Epoch i=228, g_loss=  1.2906, d_loss= -0.1973.\n",
      "NOTE: Epoch i=229, g_loss=  1.4049, d_loss= -0.2163.\n",
      "NOTE: Epoch i=230, g_loss=  1.4014, d_loss= -0.3491.\n",
      "NOTE: Epoch i=231, g_loss=  1.3880, d_loss= -0.3019.\n",
      "NOTE: Epoch i=232, g_loss=  1.2983, d_loss= -0.2705.\n",
      "NOTE: Epoch i=233, g_loss=  1.5505, d_loss= -0.3016.\n",
      "NOTE: Epoch i=234, g_loss=  1.3043, d_loss= -0.1610.\n",
      "NOTE: Epoch i=235, g_loss=  1.4917, d_loss= -0.0245.\n",
      "NOTE: Epoch i=236, g_loss=  1.4418, d_loss= -0.3604.\n",
      "NOTE: Epoch i=237, g_loss=  1.6146, d_loss= -0.2284.\n",
      "NOTE: Epoch i=238, g_loss=  1.5560, d_loss= -0.1259.\n",
      "NOTE: Epoch i=239, g_loss=  1.4196, d_loss=  0.3262.\n",
      "NOTE: Epoch i=240, g_loss=  1.4413, d_loss= -0.0328.\n",
      "NOTE: Epoch i=241, g_loss=  1.4048, d_loss=  0.1561.\n",
      "NOTE: Epoch i=242, g_loss=  1.4820, d_loss= -0.2484.\n",
      "NOTE: Epoch i=243, g_loss=  1.2967, d_loss= -0.0003.\n",
      "NOTE: Epoch i=244, g_loss=  1.5789, d_loss= -0.2477.\n",
      "NOTE: Epoch i=245, g_loss=  1.4948, d_loss= -0.1932.\n",
      "NOTE: Epoch i=246, g_loss=  1.4652, d_loss= -0.2534.\n",
      "NOTE: Epoch i=247, g_loss=  1.2929, d_loss= -0.2599.\n",
      "NOTE: Epoch i=248, g_loss=  1.3899, d_loss= -0.0302.\n",
      "NOTE: Epoch i=249, g_loss=  1.3894, d_loss= -0.1942.\n",
      "NOTE: Epoch i=250, g_loss=  1.4237, d_loss= -0.0396.\n",
      "NOTE: Epoch i=251, g_loss=  1.4738, d_loss= -0.1107.\n",
      "NOTE: Epoch i=252, g_loss=  1.3836, d_loss= -0.2432.\n",
      "NOTE: Epoch i=253, g_loss=  1.3811, d_loss= -0.3122.\n",
      "NOTE: Epoch i=254, g_loss=  1.3978, d_loss= -0.2617.\n",
      "NOTE: Epoch i=255, g_loss=  1.3230, d_loss= -0.1169.\n",
      "NOTE: Epoch i=256, g_loss=  1.1914, d_loss= -0.0472.\n",
      "NOTE: Epoch i=257, g_loss=  1.2975, d_loss=  0.1561.\n",
      "NOTE: Epoch i=258, g_loss=  1.5267, d_loss= -0.0770.\n",
      "NOTE: Epoch i=259, g_loss=  1.4804, d_loss=  0.0350.\n",
      "NOTE: Epoch i=260, g_loss=  1.3137, d_loss=  0.0555.\n",
      "NOTE: Epoch i=261, g_loss=  1.5426, d_loss= -0.4059.\n",
      "NOTE: Epoch i=262, g_loss=  1.4399, d_loss=  0.0622.\n",
      "NOTE: Epoch i=263, g_loss=  1.5215, d_loss= -0.1182.\n",
      "NOTE: Epoch i=264, g_loss=  1.3946, d_loss=  0.0220.\n",
      "NOTE: Epoch i=265, g_loss=  1.5075, d_loss= -0.1117.\n",
      "NOTE: Epoch i=266, g_loss=  1.4832, d_loss= -0.1448.\n",
      "NOTE: Epoch i=267, g_loss=  1.7170, d_loss= -0.0093.\n",
      "NOTE: Epoch i=268, g_loss=  1.5548, d_loss= -0.1432.\n",
      "NOTE: Epoch i=269, g_loss=  1.6159, d_loss= -0.1470.\n",
      "NOTE: Epoch i=270, g_loss=  1.5821, d_loss= -0.1041.\n",
      "NOTE: Epoch i=271, g_loss=  1.5963, d_loss= -0.1088.\n",
      "NOTE: Epoch i=272, g_loss=  1.6526, d_loss= -0.2486.\n",
      "NOTE: Epoch i=273, g_loss=  1.5515, d_loss= -0.0175.\n",
      "NOTE: Epoch i=274, g_loss=  1.5693, d_loss= -0.2490.\n",
      "NOTE: Epoch i=275, g_loss=  1.5668, d_loss= -0.2646.\n",
      "NOTE: Epoch i=276, g_loss=  1.5816, d_loss= -0.2591.\n",
      "NOTE: Epoch i=277, g_loss=  1.5586, d_loss= -0.1008.\n",
      "NOTE: Epoch i=278, g_loss=  1.6112, d_loss= -0.2037.\n",
      "NOTE: Epoch i=279, g_loss=  1.5140, d_loss= -0.3161.\n",
      "NOTE: Epoch i=280, g_loss=  1.5474, d_loss= -0.1312.\n",
      "NOTE: Epoch i=281, g_loss=  1.5583, d_loss=  0.0558.\n",
      "NOTE: Epoch i=282, g_loss=  1.6445, d_loss= -0.0452.\n",
      "NOTE: Epoch i=283, g_loss=  1.4894, d_loss= -0.1693.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=284, g_loss=  1.6334, d_loss= -0.0895.\n",
      "NOTE: Epoch i=285, g_loss=  1.4680, d_loss= -0.2269.\n",
      "NOTE: Epoch i=286, g_loss=  1.5628, d_loss= -0.1676.\n",
      "NOTE: Epoch i=287, g_loss=  1.4248, d_loss= -0.2223.\n",
      "NOTE: Epoch i=288, g_loss=  1.5286, d_loss= -0.0552.\n",
      "NOTE: Epoch i=289, g_loss=  1.4662, d_loss=  0.1274.\n",
      "NOTE: Epoch i=290, g_loss=  1.5974, d_loss= -0.2771.\n",
      "NOTE: Epoch i=291, g_loss=  1.3954, d_loss= -0.1003.\n",
      "NOTE: Epoch i=292, g_loss=  1.6258, d_loss=  0.2224.\n",
      "NOTE: Epoch i=293, g_loss=  1.5923, d_loss=  0.0460.\n",
      "NOTE: Epoch i=294, g_loss=  1.6112, d_loss= -0.0494.\n",
      "NOTE: Epoch i=295, g_loss=  1.6493, d_loss= -0.1987.\n",
      "NOTE: Epoch i=296, g_loss=  1.6990, d_loss=  0.1082.\n",
      "NOTE: Epoch i=297, g_loss=  1.4387, d_loss= -0.3196.\n",
      "NOTE: Epoch i=298, g_loss=  1.2837, d_loss= -0.2765.\n",
      "NOTE: Epoch i=299, g_loss=  1.6081, d_loss= -0.0472.\n",
      "NOTE: Epoch i=300, g_loss=  1.6310, d_loss= -0.0922.\n",
      "NOTE: Epoch i=301, g_loss=  1.6276, d_loss= -0.3002.\n",
      "NOTE: Epoch i=302, g_loss=  1.5985, d_loss= -0.0371.\n",
      "NOTE: Epoch i=303, g_loss=  1.5456, d_loss= -0.1007.\n",
      "NOTE: Epoch i=304, g_loss=  1.5338, d_loss= -0.1494.\n",
      "NOTE: Epoch i=305, g_loss=  1.7107, d_loss= -0.3611.\n",
      "NOTE: Epoch i=306, g_loss=  1.4813, d_loss= -0.1808.\n",
      "NOTE: Epoch i=307, g_loss=  1.4172, d_loss=  0.0179.\n",
      "NOTE: Epoch i=308, g_loss=  1.6856, d_loss= -0.3056.\n",
      "NOTE: Epoch i=309, g_loss=  1.5816, d_loss= -0.1723.\n",
      "NOTE: Epoch i=310, g_loss=  1.5106, d_loss=  0.1491.\n",
      "NOTE: Epoch i=311, g_loss=  1.5117, d_loss= -0.0267.\n",
      "NOTE: Epoch i=312, g_loss=  1.4395, d_loss= -0.2121.\n",
      "NOTE: Epoch i=313, g_loss=  1.7367, d_loss= -0.2994.\n",
      "NOTE: Epoch i=314, g_loss=  1.5859, d_loss= -0.2688.\n",
      "NOTE: Epoch i=315, g_loss=  1.5481, d_loss=  0.0596.\n",
      "NOTE: Epoch i=316, g_loss=  1.6537, d_loss=  0.1929.\n",
      "NOTE: Epoch i=317, g_loss=  1.5784, d_loss= -0.0042.\n",
      "NOTE: Epoch i=318, g_loss=  1.6509, d_loss=  0.1021.\n",
      "NOTE: Epoch i=319, g_loss=  1.5157, d_loss= -0.1192.\n",
      "NOTE: Epoch i=320, g_loss=  1.6580, d_loss= -0.1250.\n",
      "NOTE: Epoch i=321, g_loss=  1.7044, d_loss= -0.0157.\n",
      "NOTE: Epoch i=322, g_loss=  1.5471, d_loss=  0.1427.\n",
      "NOTE: Epoch i=323, g_loss=  1.5700, d_loss= -0.0980.\n",
      "NOTE: Epoch i=324, g_loss=  1.6036, d_loss= -0.0263.\n",
      "NOTE: Epoch i=325, g_loss=  1.4047, d_loss= -0.1882.\n",
      "NOTE: Epoch i=326, g_loss=  1.6757, d_loss= -0.0816.\n",
      "NOTE: Epoch i=327, g_loss=  1.5480, d_loss= -0.1218.\n",
      "NOTE: Epoch i=328, g_loss=  1.5728, d_loss= -0.0420.\n",
      "NOTE: Epoch i=329, g_loss=  1.5788, d_loss=  0.0693.\n",
      "NOTE: Epoch i=330, g_loss=  1.5664, d_loss= -0.1036.\n",
      "NOTE: Epoch i=331, g_loss=  1.5678, d_loss= -0.1855.\n",
      "NOTE: Epoch i=332, g_loss=  1.6390, d_loss= -0.0946.\n",
      "NOTE: Epoch i=333, g_loss=  1.7085, d_loss= -0.1233.\n",
      "NOTE: Epoch i=334, g_loss=  1.7902, d_loss=  0.0334.\n",
      "NOTE: Epoch i=335, g_loss=  1.6435, d_loss= -0.2319.\n",
      "NOTE: Epoch i=336, g_loss=  1.5470, d_loss= -0.1637.\n",
      "NOTE: Epoch i=337, g_loss=  1.7568, d_loss= -0.2867.\n",
      "NOTE: Epoch i=338, g_loss=  1.5770, d_loss= -0.1245.\n",
      "NOTE: Epoch i=339, g_loss=  1.6155, d_loss=  0.1102.\n",
      "NOTE: Epoch i=340, g_loss=  1.4920, d_loss= -0.0201.\n",
      "NOTE: Epoch i=341, g_loss=  1.4904, d_loss= -0.1911.\n",
      "NOTE: Epoch i=342, g_loss=  1.4003, d_loss= -0.0274.\n",
      "NOTE: Epoch i=343, g_loss=  1.7838, d_loss= -0.2320.\n",
      "NOTE: Epoch i=344, g_loss=  1.5331, d_loss= -0.2567.\n",
      "NOTE: Epoch i=345, g_loss=  1.6680, d_loss= -0.3203.\n",
      "NOTE: Epoch i=346, g_loss=  1.8377, d_loss= -0.1865.\n",
      "NOTE: Epoch i=347, g_loss=  1.6151, d_loss= -0.1171.\n",
      "NOTE: Epoch i=348, g_loss=  1.6648, d_loss= -0.2842.\n",
      "NOTE: Epoch i=349, g_loss=  1.6669, d_loss= -0.1375.\n",
      "NOTE: Epoch i=350, g_loss=  1.3795, d_loss= -0.3982.\n",
      "NOTE: Epoch i=351, g_loss=  1.6391, d_loss= -0.1626.\n",
      "NOTE: Epoch i=352, g_loss=  1.7327, d_loss= -0.0637.\n",
      "NOTE: Epoch i=353, g_loss=  1.5257, d_loss= -0.1563.\n",
      "NOTE: Epoch i=354, g_loss=  1.6579, d_loss= -0.1347.\n",
      "NOTE: Epoch i=355, g_loss=  1.7982, d_loss= -0.3227.\n",
      "NOTE: Epoch i=356, g_loss=  1.5564, d_loss= -0.0941.\n",
      "NOTE: Epoch i=357, g_loss=  1.5109, d_loss= -0.3291.\n",
      "NOTE: Epoch i=358, g_loss=  1.6314, d_loss= -0.2749.\n",
      "NOTE: Epoch i=359, g_loss=  1.3852, d_loss= -0.1375.\n",
      "NOTE: Epoch i=360, g_loss=  1.5733, d_loss= -0.3083.\n",
      "NOTE: Epoch i=361, g_loss=  1.4192, d_loss= -0.1481.\n",
      "NOTE: Epoch i=362, g_loss=  1.2830, d_loss= -0.3577.\n",
      "NOTE: Epoch i=363, g_loss=  1.5342, d_loss= -0.0239.\n",
      "NOTE: Epoch i=364, g_loss=  1.5334, d_loss= -0.1426.\n",
      "NOTE: Epoch i=365, g_loss=  1.7863, d_loss=  0.0648.\n",
      "NOTE: Epoch i=366, g_loss=  1.4545, d_loss= -0.3527.\n",
      "NOTE: Epoch i=367, g_loss=  1.5717, d_loss= -0.0367.\n",
      "NOTE: Epoch i=368, g_loss=  1.3956, d_loss= -0.0297.\n",
      "NOTE: Epoch i=369, g_loss=  1.6197, d_loss= -0.0858.\n",
      "NOTE: Epoch i=370, g_loss=  1.4916, d_loss= -0.1469.\n",
      "NOTE: Epoch i=371, g_loss=  1.4213, d_loss= -0.2718.\n",
      "NOTE: Epoch i=372, g_loss=  1.6216, d_loss= -0.0024.\n",
      "NOTE: Epoch i=373, g_loss=  1.4679, d_loss= -0.3340.\n",
      "NOTE: Epoch i=374, g_loss=  1.5355, d_loss= -0.0103.\n",
      "NOTE: Epoch i=375, g_loss=  1.4125, d_loss= -0.1878.\n",
      "NOTE: Epoch i=376, g_loss=  1.5536, d_loss= -0.0613.\n",
      "NOTE: Epoch i=377, g_loss=  1.6823, d_loss= -0.0519.\n",
      "NOTE: Epoch i=378, g_loss=  1.5914, d_loss=  0.0799.\n",
      "NOTE: Epoch i=379, g_loss=  1.5616, d_loss=  0.0786.\n",
      "NOTE: Epoch i=380, g_loss=  1.5453, d_loss= -0.1295.\n",
      "NOTE: Epoch i=381, g_loss=  1.4746, d_loss= -0.0851.\n",
      "NOTE: Epoch i=382, g_loss=  1.7347, d_loss= -0.3745.\n",
      "NOTE: Epoch i=383, g_loss=  1.5225, d_loss= -0.2151.\n",
      "NOTE: Epoch i=384, g_loss=  1.6834, d_loss= -0.0968.\n",
      "NOTE: Epoch i=385, g_loss=  1.5836, d_loss= -0.1912.\n",
      "NOTE: Epoch i=386, g_loss=  1.6012, d_loss= -0.0533.\n",
      "NOTE: Epoch i=387, g_loss=  1.7045, d_loss= -0.1274.\n",
      "NOTE: Epoch i=388, g_loss=  1.5352, d_loss=  0.0368.\n",
      "NOTE: Epoch i=389, g_loss=  1.5781, d_loss= -0.1981.\n",
      "NOTE: Epoch i=390, g_loss=  1.7813, d_loss=  0.1851.\n",
      "NOTE: Epoch i=391, g_loss=  1.6181, d_loss= -0.1172.\n",
      "NOTE: Epoch i=392, g_loss=  1.6121, d_loss= -0.1377.\n",
      "NOTE: Epoch i=393, g_loss=  1.5160, d_loss= -0.2570.\n",
      "NOTE: Epoch i=394, g_loss=  1.6579, d_loss= -0.1451.\n",
      "NOTE: Epoch i=395, g_loss=  1.6030, d_loss= -0.0181.\n",
      "NOTE: Epoch i=396, g_loss=  1.7014, d_loss= -0.1873.\n",
      "NOTE: Epoch i=397, g_loss=  1.6457, d_loss= -0.1553.\n",
      "NOTE: Epoch i=398, g_loss=  1.6611, d_loss= -0.0709.\n",
      "NOTE: Epoch i=399, g_loss=  1.6987, d_loss= -0.3175.\n",
      "NOTE: Epoch i=400, g_loss=  1.4891, d_loss= -0.0290.\n",
      "NOTE: Epoch i=401, g_loss=  1.9412, d_loss= -0.2428.\n",
      "NOTE: Epoch i=402, g_loss=  1.9203, d_loss= -0.1712.\n",
      "NOTE: Epoch i=403, g_loss=  1.7837, d_loss= -0.0749.\n",
      "NOTE: Epoch i=404, g_loss=  1.7742, d_loss= -0.4064.\n",
      "NOTE: Epoch i=405, g_loss=  1.6988, d_loss= -0.3315.\n",
      "NOTE: Epoch i=406, g_loss=  1.7051, d_loss= -0.1969.\n",
      "NOTE: Epoch i=407, g_loss=  1.6185, d_loss= -0.2878.\n",
      "NOTE: Epoch i=408, g_loss=  1.6250, d_loss=  0.0476.\n",
      "NOTE: Epoch i=409, g_loss=  1.5516, d_loss=  0.1186.\n",
      "NOTE: Epoch i=410, g_loss=  1.6302, d_loss= -0.0558.\n",
      "NOTE: Epoch i=411, g_loss=  1.4864, d_loss= -0.0548.\n",
      "NOTE: Epoch i=412, g_loss=  1.7169, d_loss= -0.3868.\n",
      "NOTE: Epoch i=413, g_loss=  1.5959, d_loss=  0.0909.\n",
      "NOTE: Epoch i=414, g_loss=  1.5886, d_loss=  0.2340.\n",
      "NOTE: Epoch i=415, g_loss=  1.7722, d_loss=  0.0691.\n",
      "NOTE: Epoch i=416, g_loss=  1.3660, d_loss= -0.1416.\n",
      "NOTE: Epoch i=417, g_loss=  1.3940, d_loss=  0.1870.\n",
      "NOTE: Epoch i=418, g_loss=  1.5062, d_loss= -0.1143.\n",
      "NOTE: Epoch i=419, g_loss=  1.6146, d_loss= -0.0137.\n",
      "NOTE: Epoch i=420, g_loss=  1.5396, d_loss= -0.1297.\n",
      "NOTE: Epoch i=421, g_loss=  1.6710, d_loss=  0.1546.\n",
      "NOTE: Epoch i=422, g_loss=  1.2942, d_loss= -0.3821.\n",
      "NOTE: Epoch i=423, g_loss=  1.3501, d_loss=  0.0521.\n",
      "NOTE: Epoch i=424, g_loss=  1.6781, d_loss= -0.1457.\n",
      "NOTE: Epoch i=425, g_loss=  1.6294, d_loss=  0.2975.\n",
      "NOTE: Epoch i=426, g_loss=  1.6901, d_loss= -0.1929.\n",
      "NOTE: Epoch i=427, g_loss=  1.6519, d_loss= -0.1034.\n",
      "NOTE: Epoch i=428, g_loss=  1.5289, d_loss= -0.1003.\n",
      "NOTE: Epoch i=429, g_loss=  1.5521, d_loss= -0.2464.\n",
      "NOTE: Epoch i=430, g_loss=  1.3658, d_loss= -0.0945.\n",
      "NOTE: Epoch i=431, g_loss=  1.6547, d_loss= -0.0866.\n",
      "NOTE: Epoch i=432, g_loss=  1.5897, d_loss= -0.1850.\n",
      "NOTE: Epoch i=433, g_loss=  1.5684, d_loss= -0.2219.\n",
      "NOTE: Epoch i=434, g_loss=  1.7271, d_loss=  0.0827.\n",
      "NOTE: Epoch i=435, g_loss=  1.6731, d_loss= -0.1722.\n",
      "NOTE: Epoch i=436, g_loss=  1.4072, d_loss=  0.0939.\n",
      "NOTE: Epoch i=437, g_loss=  1.5733, d_loss= -0.1591.\n",
      "NOTE: Epoch i=438, g_loss=  1.4090, d_loss= -0.1782.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=439, g_loss=  1.6182, d_loss= -0.1224.\n",
      "NOTE: Epoch i=440, g_loss=  1.5223, d_loss= -0.2441.\n",
      "NOTE: Epoch i=441, g_loss=  1.3338, d_loss=  0.0990.\n",
      "NOTE: Epoch i=442, g_loss=  1.6498, d_loss= -0.3134.\n",
      "NOTE: Epoch i=443, g_loss=  1.5156, d_loss= -0.1485.\n",
      "NOTE: Epoch i=444, g_loss=  1.5976, d_loss= -0.1554.\n",
      "NOTE: Epoch i=445, g_loss=  1.5050, d_loss= -0.3183.\n",
      "NOTE: Epoch i=446, g_loss=  1.5846, d_loss= -0.2501.\n",
      "NOTE: Epoch i=447, g_loss=  1.3801, d_loss=  0.1832.\n",
      "NOTE: Epoch i=448, g_loss=  1.4169, d_loss=  0.0368.\n",
      "NOTE: Epoch i=449, g_loss=  1.7638, d_loss= -0.0697.\n",
      "NOTE: Epoch i=450, g_loss=  1.8974, d_loss= -0.2150.\n",
      "NOTE: Epoch i=451, g_loss=  1.5901, d_loss= -0.2484.\n",
      "NOTE: Epoch i=452, g_loss=  1.8255, d_loss=  0.0174.\n",
      "NOTE: Epoch i=453, g_loss=  1.6440, d_loss= -0.1964.\n",
      "NOTE: Epoch i=454, g_loss=  1.5786, d_loss= -0.1310.\n",
      "NOTE: Epoch i=455, g_loss=  1.7350, d_loss= -0.1890.\n",
      "NOTE: Epoch i=456, g_loss=  1.4843, d_loss= -0.1735.\n",
      "NOTE: Epoch i=457, g_loss=  1.6773, d_loss=  0.0451.\n",
      "NOTE: Epoch i=458, g_loss=  1.6412, d_loss= -0.0009.\n",
      "NOTE: Epoch i=459, g_loss=  1.5460, d_loss= -0.0339.\n",
      "NOTE: Epoch i=460, g_loss=  1.5493, d_loss=  0.0152.\n",
      "NOTE: Epoch i=461, g_loss=  1.6242, d_loss= -0.3204.\n",
      "NOTE: Epoch i=462, g_loss=  1.6172, d_loss=  0.1059.\n",
      "NOTE: Epoch i=463, g_loss=  1.5153, d_loss=  0.3419.\n",
      "NOTE: Epoch i=464, g_loss=  1.4472, d_loss= -0.2853.\n",
      "NOTE: Epoch i=465, g_loss=  1.5634, d_loss= -0.1719.\n",
      "NOTE: Epoch i=466, g_loss=  1.6684, d_loss= -0.0222.\n",
      "NOTE: Epoch i=467, g_loss=  1.7213, d_loss=  0.0198.\n",
      "NOTE: Epoch i=468, g_loss=  1.5928, d_loss= -0.2426.\n",
      "NOTE: Epoch i=469, g_loss=  1.6791, d_loss=  0.0006.\n",
      "NOTE: Epoch i=470, g_loss=  1.6065, d_loss= -0.2628.\n",
      "NOTE: Epoch i=471, g_loss=  1.5315, d_loss= -0.2692.\n",
      "NOTE: Epoch i=472, g_loss=  1.5877, d_loss= -0.1980.\n",
      "NOTE: Epoch i=473, g_loss=  1.6218, d_loss= -0.2324.\n",
      "NOTE: Epoch i=474, g_loss=  1.5563, d_loss= -0.1482.\n",
      "NOTE: Epoch i=475, g_loss=  1.5704, d_loss= -0.1710.\n",
      "NOTE: Epoch i=476, g_loss=  1.7072, d_loss= -0.5297.\n",
      "NOTE: Epoch i=477, g_loss=  1.5989, d_loss= -0.3437.\n",
      "NOTE: Epoch i=478, g_loss=  1.8798, d_loss= -0.0273.\n",
      "NOTE: Epoch i=479, g_loss=  1.7688, d_loss= -0.1008.\n",
      "NOTE: Epoch i=480, g_loss=  1.7512, d_loss= -0.2407.\n",
      "NOTE: Epoch i=481, g_loss=  1.5694, d_loss=  0.0311.\n",
      "NOTE: Epoch i=482, g_loss=  1.7486, d_loss= -0.2217.\n",
      "NOTE: Epoch i=483, g_loss=  1.6924, d_loss= -0.0211.\n",
      "NOTE: Epoch i=484, g_loss=  1.4408, d_loss= -0.1033.\n",
      "NOTE: Epoch i=485, g_loss=  1.6118, d_loss= -0.0302.\n",
      "NOTE: Epoch i=486, g_loss=  1.7162, d_loss= -0.0718.\n",
      "NOTE: Epoch i=487, g_loss=  1.5631, d_loss=  0.0926.\n",
      "NOTE: Epoch i=488, g_loss=  1.4922, d_loss=  0.0485.\n",
      "NOTE: Epoch i=489, g_loss=  1.6811, d_loss= -0.0861.\n",
      "NOTE: Epoch i=490, g_loss=  1.6075, d_loss= -0.1047.\n",
      "NOTE: Epoch i=491, g_loss=  1.4678, d_loss=  0.0782.\n",
      "NOTE: Epoch i=492, g_loss=  1.6529, d_loss=  0.0172.\n",
      "NOTE: Epoch i=493, g_loss=  1.5473, d_loss= -0.1320.\n",
      "NOTE: Epoch i=494, g_loss=  1.6276, d_loss= -0.2504.\n",
      "NOTE: Epoch i=495, g_loss=  1.7627, d_loss=  0.0342.\n",
      "NOTE: Epoch i=496, g_loss=  1.5356, d_loss= -0.4966.\n",
      "NOTE: Epoch i=497, g_loss=  1.3490, d_loss= -0.1576.\n",
      "NOTE: Epoch i=498, g_loss=  1.4671, d_loss= -0.4723.\n",
      "NOTE: Epoch i=499, g_loss=  1.4563, d_loss= -0.1126.\n",
      "NOTE: Epoch i=500, g_loss=  1.7616, d_loss= -0.0826.\n",
      "NOTE: 13184873 bytes were written to the table \"cpctStore\" in the caslib \"CASUSER(alphel)\".\n",
      "NOTE: tabularGanTrain action completed successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; IterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch Number\">EpochNumber</th>\n",
       "      <th title=\"Autoencoder Loss\">AutoencoderLoss</th>\n",
       "      <th title=\"Generator Loss\">GeneratorLoss</th>\n",
       "      <th title=\"Discriminator Loss\">DiscriminatorLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.049489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.037792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.029271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.023146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.020312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.535579</td>\n",
       "      <td>-0.496610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.348975</td>\n",
       "      <td>-0.157579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.467066</td>\n",
       "      <td>-0.472342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.456292</td>\n",
       "      <td>-0.112555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.761593</td>\n",
       "      <td>-0.082624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; LevelFreq</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Variable Name\">VarName</th>\n",
       "      <th title=\"Level\">Level</th>\n",
       "      <th title=\"Frequency\">Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>education</td>\n",
       "      <td>10th</td>\n",
       "      <td>962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>education</td>\n",
       "      <td>11th</td>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>education</td>\n",
       "      <td>12th</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>education</td>\n",
       "      <td>1st-4th</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>education</td>\n",
       "      <td>5th-6th</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>workclass</td>\n",
       "      <td>Private</td>\n",
       "      <td>25632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>workclass</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>workclass</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>2667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>workclass</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>workclass</td>\n",
       "      <td>Without-pay</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 3 columns</p>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"RowId\">RowId</th>\n",
       "      <th title=\"Description\">Description</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "      <th title=\"Numeric Value\">nValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMBEDDINGDIM</td>\n",
       "      <td>Generator Embedding Dimension</td>\n",
       "      <td>128</td>\n",
       "      <td>1.280000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MINIBATCHSIZE</td>\n",
       "      <td>Number of Observations in One Minibatch</td>\n",
       "      <td>500</td>\n",
       "      <td>5.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PACKSIZE</td>\n",
       "      <td>Number of Observations Group Together in Apply...</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REGDWEIGHT</td>\n",
       "      <td>Weight for Regularizing the Discriminator</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OPTIMIZERAE_BETA1</td>\n",
       "      <td>Exponential Decay Rate for the First Moment Es...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>9.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OPTIMIZERAE_BETA2</td>\n",
       "      <td>Exponential Decay Rate for the Second Moment E...</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>9.990000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OPTIMIZERAE_LEARNINGRATE</td>\n",
       "      <td>Learning Rate for the Autoencoder's Optimizer</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OPTIMIZERAE_NUMEPOCHS</td>\n",
       "      <td>Number of Epochs for the Autoencoder's Training</td>\n",
       "      <td>500</td>\n",
       "      <td>5.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OPTIMIZERAE_WEIGHTDECAY</td>\n",
       "      <td>Weight Decay for the Autoencoder's Optimizer</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>1.000000e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OPTIMIZERGAN_BETA1</td>\n",
       "      <td>Exponential Decay Rate for the First Moment Es...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OPTIMIZERGAN_BETA2</td>\n",
       "      <td>Exponential Decay Rate for the Second Moment E...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>9.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OPTIMIZERGAN_LEARNINGRATE</td>\n",
       "      <td>Learning Rate for the GAN Optimizer</td>\n",
       "      <td>2e-05</td>\n",
       "      <td>2.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>OPTIMIZERGAN_NUMEPOCHS</td>\n",
       "      <td>Number of Epochs for the GAN Training</td>\n",
       "      <td>500</td>\n",
       "      <td>5.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>OPTIMIZERGAN_WEIGHTDECAYD</td>\n",
       "      <td>Weight Decay for the Generator's Optimizer</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>OPTIMIZERGAN_WEIGHTDECAYG</td>\n",
       "      <td>Weight Decay for the Discriminator's Optimizer</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SEED</td>\n",
       "      <td>Seed for Random Initialization</td>\n",
       "      <td>12345</td>\n",
       "      <td>1.234500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>USELOGLEVELFREQ</td>\n",
       "      <td>Whether to Use Log Frequency of Categorical Le...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; NObs</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"RowId\">RowId</th>\n",
       "      <th title=\"Description\">Description</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NREAD</td>\n",
       "      <td>Number of Observations Read</td>\n",
       "      <td>34189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NUSED</td>\n",
       "      <td>Number of Observations Used</td>\n",
       "      <td>34189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Label\">Label</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(alphel)</td>\n",
       "      <td>out</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>CASTable('out', caslib='CASUSER(alphel)')</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 2.42e+03s</span> &#183; <span class=\"cas-user\">user 1.06e+04s</span> &#183; <span class=\"cas-sys\">sys 414s</span> &#183; <span class=\"cas-memory\">mem 29MB</span></small></p>"
      ],
      "text/plain": [
       "[IterHistory]\n",
       "\n",
       "      EpochNumber  AutoencoderLoss  GeneratorLoss  DiscriminatorLoss\n",
       " 0              1         0.049489            NaN                NaN\n",
       " 1              2         0.037792            NaN                NaN\n",
       " 2              3         0.029271            NaN                NaN\n",
       " 3              4         0.023146            NaN                NaN\n",
       " 4              5         0.020312            NaN                NaN\n",
       " ..           ...              ...            ...                ...\n",
       " 995          496              NaN       1.535579          -0.496610\n",
       " 996          497              NaN       1.348975          -0.157579\n",
       " 997          498              NaN       1.467066          -0.472342\n",
       " 998          499              NaN       1.456292          -0.112555\n",
       " 999          500              NaN       1.761593          -0.082624\n",
       " \n",
       " [1000 rows x 4 columns]\n",
       "\n",
       "[LevelFreq]\n",
       "\n",
       "        VarName             Level  Frequency\n",
       " 0    education              10th        962\n",
       " 1    education              11th       1290\n",
       " 2    education              12th        462\n",
       " 3    education           1st-4th        168\n",
       " 4    education           5th-6th        363\n",
       " ..         ...               ...        ...\n",
       " 96   workclass           Private      25632\n",
       " 97   workclass      Self-emp-inc       1226\n",
       " 98   workclass  Self-emp-not-inc       2667\n",
       " 99   workclass         State-gov       1420\n",
       " 100  workclass       Without-pay         14\n",
       " \n",
       " [101 rows x 3 columns]\n",
       "\n",
       "[ModelInfo]\n",
       "\n",
       "                         RowId  \\\n",
       " 0                EMBEDDINGDIM   \n",
       " 1               MINIBATCHSIZE   \n",
       " 2                    PACKSIZE   \n",
       " 3                  REGDWEIGHT   \n",
       " 4           OPTIMIZERAE_BETA1   \n",
       " 5           OPTIMIZERAE_BETA2   \n",
       " 6    OPTIMIZERAE_LEARNINGRATE   \n",
       " 7       OPTIMIZERAE_NUMEPOCHS   \n",
       " 8     OPTIMIZERAE_WEIGHTDECAY   \n",
       " 9          OPTIMIZERGAN_BETA1   \n",
       " 10         OPTIMIZERGAN_BETA2   \n",
       " 11  OPTIMIZERGAN_LEARNINGRATE   \n",
       " 12     OPTIMIZERGAN_NUMEPOCHS   \n",
       " 13  OPTIMIZERGAN_WEIGHTDECAYD   \n",
       " 14  OPTIMIZERGAN_WEIGHTDECAYG   \n",
       " 15                       SEED   \n",
       " 16            USELOGLEVELFREQ   \n",
       " \n",
       "                                           Description     Value        nValue  \n",
       " 0                       Generator Embedding Dimension       128  1.280000e+02  \n",
       " 1             Number of Observations in One Minibatch       500  5.000000e+02  \n",
       " 2   Number of Observations Group Together in Apply...        10  1.000000e+01  \n",
       " 3           Weight for Regularizing the Discriminator        10  1.000000e+01  \n",
       " 4   Exponential Decay Rate for the First Moment Es...  0.900000  9.000000e-01  \n",
       " 5   Exponential Decay Rate for the Second Moment E...  0.999000  9.990000e-01  \n",
       " 6       Learning Rate for the Autoencoder's Optimizer     0.001  1.000000e-03  \n",
       " 7     Number of Epochs for the Autoencoder's Training       500  5.000000e+02  \n",
       " 8        Weight Decay for the Autoencoder's Optimizer     1e-08  1.000000e-08  \n",
       " 9   Exponential Decay Rate for the First Moment Es...  0.500000  5.000000e-01  \n",
       " 10  Exponential Decay Rate for the Second Moment E...  0.900000  9.000000e-01  \n",
       " 11                Learning Rate for the GAN Optimizer     2e-05  2.000000e-05  \n",
       " 12              Number of Epochs for the GAN Training       500  5.000000e+02  \n",
       " 13         Weight Decay for the Generator's Optimizer    0.0001  1.000000e-04  \n",
       " 14     Weight Decay for the Discriminator's Optimizer     1e-06  1.000000e-06  \n",
       " 15                     Seed for Random Initialization     12345  1.234500e+04  \n",
       " 16  Whether to Use Log Frequency of Categorical Le...      True  1.000000e+00  \n",
       "\n",
       "[NObs]\n",
       "\n",
       "    RowId                  Description  Value\n",
       " 0  NREAD  Number of Observations Read  34189\n",
       " 1  NUSED  Number of Observations Used  34189\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib Name Label  Rows  Columns  \\\n",
       " 0  CASUSER(alphel)  out           0       15   \n",
       " \n",
       "                                     casTable  \n",
       " 0  CASTable('out', caslib='CASUSER(alphel)')  \n",
       "\n",
       "+ Elapsed: 2.42e+03s, user: 1.06e+04s, sys: 414s, mem: 29mb"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = s.tabularGanTrain(\n",
    "table = {\"name\":\"GAN_data_train\"},\n",
    "    centroidsTable= \"cen\",\n",
    "    gpu = 1,\n",
    "    nominals = adult_discrete_columns,\n",
    "    optimizerAe ={\"method\":'ADAM',\"numEpochs\":500},\n",
    "    optimizerGan ={\"method\":'ADAM',\"numEpochs\":500},\n",
    "    seed = 12345,\n",
    "    scoreSeed = 1234,\n",
    "    numSamples =50000,\n",
    "    saveState ={\"name\":\"cpctStore\", \"replace\":True},\n",
    "    casOut = {\"name\":\"out\", \"replace\":True}\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = s.fetch('out', to=400000, maxrows=400000)['Fetch']\n",
    "gloss = results.IterHistory['GeneratorLoss'].dropna().reset_index(drop=True)\n",
    "dloss = results.IterHistory['DiscriminatorLoss'].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Losses for CTGAN on Adult data')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(dloss)\n",
    "plt.plot(gloss)\n",
    "plt.legend(['D', 'G'])\n",
    "plt.title('Losses for CTGAN on Adult data')\n",
    "#plt.savefig('Original-CTGAN-Adult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    41434\n",
       "1.0     8566\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25989\n",
       "1     8200\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAN_data_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>workclass_State-gov</th>\n",
       "      <th>workclass_Without-pay</th>\n",
       "      <th>education_10th</th>\n",
       "      <th>education_11th</th>\n",
       "      <th>education_12th</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   workclass_Federal-gov  workclass_Local-gov  workclass_Private  \\\n",
       "0                      0                    0                  1   \n",
       "1                      0                    0                  1   \n",
       "2                      0                    0                  0   \n",
       "3                      0                    0                  1   \n",
       "4                      0                    0                  1   \n",
       "\n",
       "   workclass_Self-emp-inc  workclass_Self-emp-not-inc  workclass_State-gov  \\\n",
       "0                       0                           0                    0   \n",
       "1                       0                           0                    0   \n",
       "2                       0                           0                    1   \n",
       "3                       0                           0                    0   \n",
       "4                       0                           0                    0   \n",
       "\n",
       "   workclass_Without-pay  education_10th  education_11th  education_12th  ...  \\\n",
       "0                      0               0               0               0  ...   \n",
       "1                      0               0               0               0  ...   \n",
       "2                      0               0               0               0  ...   \n",
       "3                      0               0               0               0  ...   \n",
       "4                      0               0               0               0  ...   \n",
       "\n",
       "   native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "0                        0                           0   \n",
       "1                        0                           0   \n",
       "2                        0                           0   \n",
       "3                        0                           0   \n",
       "4                        0                           0   \n",
       "\n",
       "   native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                        0                     0                      0   \n",
       "1                        0                     0                      0   \n",
       "2                        0                     0                      0   \n",
       "3                        0                     0                      0   \n",
       "4                        0                     0                      0   \n",
       "\n",
       "   native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                        0                               0   \n",
       "1                        0                               0   \n",
       "2                        0                               0   \n",
       "3                        0                               0   \n",
       "4                        0                               0   \n",
       "\n",
       "   native-country_United-States  native-country_Vietnam  \\\n",
       "0                             1                       0   \n",
       "1                             1                       0   \n",
       "2                             1                       0   \n",
       "3                             1                       0   \n",
       "4                             1                       0   \n",
       "\n",
       "   native-country_Yugoslavia  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the Ruiwen's Encoder for ML utility and CTGAN transformer for GAN\n",
    "str_cols= [ 'workclass', 'education', 'marital-status', 'relationship','race', 'sex','native-country']\n",
    "num_cols = ['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week', 'label']\n",
    "dataframe = pd.DataFrame(samples.loc[:,str_cols])\n",
    "\n",
    "one_hot_columns = pd.DataFrame()\n",
    "for col_name, item in dataframe.iteritems():\n",
    "    \n",
    "    #print(col_name)\n",
    "    #print(item)\n",
    "    col = pd.get_dummies(item, prefix=col_name)\n",
    "    one_hot_columns =pd.concat([one_hot_columns,col],axis=1)\n",
    "one_hot_columns.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire data is concat of discrete and contiuous cols\n",
    "fake_all = pd.concat([one_hot_columns,samples.loc[:,num_cols]],axis=1)\n",
    "#adult_data_all.head()\n",
    "fake_X = fake_all.drop([\"label\"],axis=1)\n",
    "fake_X, fake_y = fake_X,fake_all.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure cols of generated data has the same index sort \n",
    "fake_X = fake_X.reindex(adult_data_all.columns,axis=1, fill_value=0) # fill new cols with 0\n",
    "#len(fake_X.columns)\n",
    "fake_X = fake_X.drop([\"label\"],axis=1) # separate X and Y for training\n",
    "fake_X, fake_y = fake_X,fake_all.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the GAN generated train data:  (50000, 91)\n",
      "shape of the GAN generated labels:  (50000,)\n",
      "shape of the original test data:  (14653, 91)\n",
      "number of events in the fake data:  8566.0\n"
     ]
    }
   ],
   "source": [
    "# check the training data shape to agree with the original data (# of features/cols)\n",
    "print('shape of the GAN generated train data: ',fake_X.shape)\n",
    "print('shape of the GAN generated labels: ',fake_y.shape)\n",
    "print('shape of the original test data: ',orig_X_test.shape)\n",
    "print('number of events in the fake data: ', sum(fake_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML scores for the GAN generated data:\n",
      "Decision Tree Acc:  0.7981983211629018 f-1:  0.33833072275676884 AUC: 0.8329082108973012\n",
      "Linear SVM Acc:  0.6241042789872381 f-1:  0.2371191135734072 AUC: 0.49540179457272293\n",
      "Random Forest Acc:  0.8055688254964853 f-1:  0.41366536324346576 AUC: 0.8575254003753149\n",
      "Logistic Regression Acc:  0.7668736777451717 f-1:  0.0871191876002138 AUC: 0.4714826251863258\n",
      "MLP Acc:  0.7690575308810482 f-1:  0.06930693069306931 AUC: 0.6227984231084562\n"
     ]
    }
   ],
   "source": [
    "# train a classifier on the CPCTGAN generated data\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5,random_state=0),\n",
    "    SVC(kernel = 'linear', max_iter=1000, C=0.025, random_state=0, probability=True),\n",
    "    RandomForestClassifier(n_estimators=200, random_state=0),\n",
    "    LogisticRegression(max_iter=1000, random_state=0),\n",
    "    MLPClassifier(alpha=1, max_iter=1000, random_state=0)]\n",
    "\n",
    "print('ML scores for the GAN generated data:')\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(fake_X, fake_y)\n",
    "    score = clf.score(orig_X_test, orig_y_test)\n",
    "    y_pred = clf.predict(orig_X_test)\n",
    "    Acc = accuracy_score(orig_y_test, y_pred)\n",
    "    fscore = f1_score(orig_y_test, y_pred, average='binary')\n",
    "    AUC = roc_auc_score(orig_y_test, clf.predict_proba(orig_X_test)[:, 1])\n",
    "    print(name,'Acc: ', Acc, 'f-1: ', fscore, 'AUC:', AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    41434\n",
       "1.0     8566\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11166\n",
       "1     3487\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CTGAN on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctgan import CTGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G:  1.5204,Loss D: -0.0589\n",
      "Epoch 2, Loss G:  1.3082,Loss D: -0.0196\n",
      "Epoch 3, Loss G:  0.3795,Loss D: -0.0636\n",
      "Epoch 4, Loss G:  0.0595,Loss D: -0.0625\n",
      "Epoch 5, Loss G: -0.4881,Loss D:  0.0530\n",
      "Epoch 6, Loss G: -0.8528,Loss D:  0.1767\n",
      "Epoch 7, Loss G: -0.8646,Loss D: -0.0188\n",
      "Epoch 8, Loss G: -1.3634,Loss D:  0.1349\n",
      "Epoch 9, Loss G: -1.6166,Loss D: -0.1497\n",
      "Epoch 10, Loss G: -1.6396,Loss D: -0.0809\n",
      "Epoch 11, Loss G: -1.6600,Loss D:  0.0691\n",
      "Epoch 12, Loss G: -1.5443,Loss D: -0.0439\n",
      "Epoch 13, Loss G: -1.5796,Loss D:  0.0283\n",
      "Epoch 14, Loss G: -1.7363,Loss D: -0.0391\n",
      "Epoch 15, Loss G: -1.8126,Loss D: -0.0799\n",
      "Epoch 16, Loss G: -1.7884,Loss D: -0.1437\n",
      "Epoch 17, Loss G: -1.5641,Loss D: -0.2686\n",
      "Epoch 18, Loss G: -1.3873,Loss D: -0.1432\n",
      "Epoch 19, Loss G: -1.5918,Loss D: -0.2049\n",
      "Epoch 20, Loss G: -1.2530,Loss D: -0.1029\n",
      "Epoch 21, Loss G: -1.2472,Loss D: -0.0676\n",
      "Epoch 22, Loss G: -1.1067,Loss D: -0.0157\n",
      "Epoch 23, Loss G: -0.8825,Loss D:  0.1365\n",
      "Epoch 24, Loss G: -1.1675,Loss D: -0.2641\n",
      "Epoch 25, Loss G: -0.6752,Loss D: -0.1700\n",
      "Epoch 26, Loss G: -1.1881,Loss D:  0.0643\n",
      "Epoch 27, Loss G: -0.7482,Loss D: -0.1080\n",
      "Epoch 28, Loss G: -0.9440,Loss D: -0.0107\n",
      "Epoch 29, Loss G: -0.7580,Loss D: -0.1536\n",
      "Epoch 30, Loss G: -0.9671,Loss D: -0.0439\n",
      "Epoch 31, Loss G: -0.5064,Loss D: -0.4074\n",
      "Epoch 32, Loss G: -0.4858,Loss D: -0.1329\n",
      "Epoch 33, Loss G: -0.7305,Loss D: -0.4691\n",
      "Epoch 34, Loss G: -0.6143,Loss D: -0.2731\n",
      "Epoch 35, Loss G: -0.2699,Loss D: -0.2159\n",
      "Epoch 36, Loss G: -0.5824,Loss D: -0.2886\n",
      "Epoch 37, Loss G: -0.7441,Loss D: -0.3648\n",
      "Epoch 38, Loss G: -0.3543,Loss D: -0.0151\n",
      "Epoch 39, Loss G: -0.7844,Loss D: -0.0788\n",
      "Epoch 40, Loss G: -0.4944,Loss D: -0.2120\n",
      "Epoch 41, Loss G: -0.4974,Loss D: -0.0903\n",
      "Epoch 42, Loss G: -0.4967,Loss D: -0.1543\n",
      "Epoch 43, Loss G: -0.4646,Loss D:  0.0977\n",
      "Epoch 44, Loss G: -0.4798,Loss D: -0.1144\n",
      "Epoch 45, Loss G: -0.5222,Loss D: -0.1402\n",
      "Epoch 46, Loss G: -0.7394,Loss D:  0.0757\n",
      "Epoch 47, Loss G: -0.6333,Loss D: -0.1643\n",
      "Epoch 48, Loss G: -0.6109,Loss D:  0.0722\n",
      "Epoch 49, Loss G: -0.5874,Loss D: -0.1074\n",
      "Epoch 50, Loss G: -0.5085,Loss D: -0.1138\n",
      "Epoch 51, Loss G: -0.4210,Loss D: -0.3038\n",
      "Epoch 52, Loss G: -0.5141,Loss D: -0.0255\n",
      "Epoch 53, Loss G: -0.7830,Loss D: -0.4105\n",
      "Epoch 54, Loss G: -0.4411,Loss D: -0.0085\n",
      "Epoch 55, Loss G: -0.8316,Loss D: -0.2894\n",
      "Epoch 56, Loss G: -0.5945,Loss D: -0.3384\n",
      "Epoch 57, Loss G: -0.1575,Loss D: -0.0361\n",
      "Epoch 58, Loss G: -0.3761,Loss D: -0.0829\n",
      "Epoch 59, Loss G: -0.4139,Loss D: -0.3196\n",
      "Epoch 60, Loss G: -0.4073,Loss D: -0.3283\n",
      "Epoch 61, Loss G: -0.3903,Loss D:  0.0807\n",
      "Epoch 62, Loss G: -0.2961,Loss D:  0.0519\n",
      "Epoch 63, Loss G: -0.4963,Loss D: -0.4257\n",
      "Epoch 64, Loss G: -0.5287,Loss D: -0.1808\n",
      "Epoch 65, Loss G: -0.5435,Loss D: -0.2333\n",
      "Epoch 66, Loss G: -0.5222,Loss D: -0.1647\n",
      "Epoch 67, Loss G: -0.4860,Loss D: -0.4415\n",
      "Epoch 68, Loss G: -0.2013,Loss D: -0.1823\n",
      "Epoch 69, Loss G: -0.2437,Loss D: -0.0821\n",
      "Epoch 70, Loss G: -0.0931,Loss D: -0.1551\n",
      "Epoch 71, Loss G: -0.0673,Loss D:  0.2061\n",
      "Epoch 72, Loss G: -0.5369,Loss D: -0.4556\n",
      "Epoch 73, Loss G: -0.3625,Loss D:  0.0792\n",
      "Epoch 74, Loss G: -0.7008,Loss D: -0.0726\n",
      "Epoch 75, Loss G: -0.1825,Loss D:  0.1620\n",
      "Epoch 76, Loss G: -0.6459,Loss D: -0.2282\n",
      "Epoch 77, Loss G: -0.6718,Loss D: -0.4402\n",
      "Epoch 78, Loss G: -0.6153,Loss D: -0.3048\n",
      "Epoch 79, Loss G: -0.2837,Loss D:  0.0332\n",
      "Epoch 80, Loss G: -0.0415,Loss D: -0.0352\n",
      "Epoch 81, Loss G: -0.1253,Loss D:  0.1022\n",
      "Epoch 82, Loss G: -0.5164,Loss D: -0.0626\n",
      "Epoch 83, Loss G: -0.3784,Loss D: -0.3736\n",
      "Epoch 84, Loss G: -0.2002,Loss D: -0.2186\n",
      "Epoch 85, Loss G: -0.2860,Loss D:  0.0617\n",
      "Epoch 86, Loss G: -0.3441,Loss D: -0.2208\n",
      "Epoch 87, Loss G: -0.6744,Loss D: -0.4157\n",
      "Epoch 88, Loss G: -0.3651,Loss D: -0.0375\n",
      "Epoch 89, Loss G: -0.1126,Loss D:  0.1236\n",
      "Epoch 90, Loss G:  0.0756,Loss D: -0.1663\n",
      "Epoch 91, Loss G: -0.0876,Loss D:  0.0489\n",
      "Epoch 92, Loss G: -0.2868,Loss D:  0.3419\n",
      "Epoch 93, Loss G: -0.5514,Loss D: -0.2793\n",
      "Epoch 94, Loss G: -0.6070,Loss D: -0.3704\n",
      "Epoch 95, Loss G: -0.4164,Loss D: -0.4379\n",
      "Epoch 96, Loss G: -0.2256,Loss D: -0.3582\n",
      "Epoch 97, Loss G: -0.2871,Loss D: -0.1983\n",
      "Epoch 98, Loss G: -0.4738,Loss D: -0.2188\n",
      "Epoch 99, Loss G: -0.4107,Loss D: -0.2476\n",
      "Epoch 100, Loss G: -0.4345,Loss D: -0.6736\n",
      "Epoch 101, Loss G: -0.4384,Loss D: -0.0148\n",
      "Epoch 102, Loss G: -0.5066,Loss D: -0.1070\n",
      "Epoch 103, Loss G: -0.3243,Loss D: -0.3461\n",
      "Epoch 104, Loss G: -0.3192,Loss D: -0.1477\n",
      "Epoch 105, Loss G: -0.4528,Loss D: -0.6196\n",
      "Epoch 106, Loss G: -0.1920,Loss D: -0.1304\n",
      "Epoch 107, Loss G: -0.3382,Loss D:  0.0137\n",
      "Epoch 108, Loss G: -0.6447,Loss D: -0.1219\n",
      "Epoch 109, Loss G: -0.5780,Loss D:  0.1183\n",
      "Epoch 110, Loss G: -0.4620,Loss D: -0.0619\n",
      "Epoch 111, Loss G: -0.9813,Loss D: -0.1638\n",
      "Epoch 112, Loss G: -0.5989,Loss D: -0.1004\n",
      "Epoch 113, Loss G: -0.5231,Loss D: -0.4076\n",
      "Epoch 114, Loss G: -0.0011,Loss D: -0.0548\n",
      "Epoch 115, Loss G: -0.4226,Loss D: -0.0291\n",
      "Epoch 116, Loss G: -0.4883,Loss D: -0.3530\n",
      "Epoch 117, Loss G: -0.0875,Loss D: -0.3607\n",
      "Epoch 118, Loss G: -0.2122,Loss D: -0.3071\n",
      "Epoch 119, Loss G: -0.1953,Loss D: -0.1712\n",
      "Epoch 120, Loss G: -0.1481,Loss D: -0.2434\n",
      "Epoch 121, Loss G:  0.0350,Loss D: -0.4387\n",
      "Epoch 122, Loss G: -0.0533,Loss D: -0.1624\n",
      "Epoch 123, Loss G: -0.3879,Loss D: -0.3227\n",
      "Epoch 124, Loss G: -0.3267,Loss D:  0.0464\n",
      "Epoch 125, Loss G: -0.5266,Loss D: -0.2155\n",
      "Epoch 126, Loss G: -0.5800,Loss D: -0.0969\n",
      "Epoch 127, Loss G: -0.6546,Loss D: -0.2593\n",
      "Epoch 128, Loss G: -0.6395,Loss D: -0.2014\n",
      "Epoch 129, Loss G: -0.8032,Loss D: -0.2368\n",
      "Epoch 130, Loss G: -0.8324,Loss D:  0.0770\n",
      "Epoch 131, Loss G: -0.8096,Loss D: -0.2423\n",
      "Epoch 132, Loss G: -0.8819,Loss D:  0.0321\n",
      "Epoch 133, Loss G: -0.4402,Loss D: -0.2283\n",
      "Epoch 134, Loss G: -0.3029,Loss D: -0.2670\n",
      "Epoch 135, Loss G: -0.7010,Loss D: -0.1472\n",
      "Epoch 136, Loss G: -0.4620,Loss D: -0.0176\n",
      "Epoch 137, Loss G: -0.6707,Loss D:  0.2394\n",
      "Epoch 138, Loss G: -0.5521,Loss D: -0.1195\n",
      "Epoch 139, Loss G: -1.0602,Loss D: -0.2104\n",
      "Epoch 140, Loss G: -0.9683,Loss D: -0.0709\n",
      "Epoch 141, Loss G: -0.8965,Loss D: -0.4151\n",
      "Epoch 142, Loss G: -0.6977,Loss D: -0.1498\n",
      "Epoch 143, Loss G: -0.6823,Loss D: -0.2123\n",
      "Epoch 144, Loss G: -0.4761,Loss D:  0.1153\n",
      "Epoch 145, Loss G: -0.5204,Loss D: -0.0268\n",
      "Epoch 146, Loss G: -0.8108,Loss D: -0.2421\n",
      "Epoch 147, Loss G: -0.4167,Loss D: -0.1945\n",
      "Epoch 148, Loss G: -0.3549,Loss D: -0.1437\n",
      "Epoch 149, Loss G: -0.5896,Loss D:  0.0333\n",
      "Epoch 150, Loss G: -0.8065,Loss D:  0.0477\n",
      "Epoch 151, Loss G: -0.5507,Loss D: -0.0260\n",
      "Epoch 152, Loss G: -0.8264,Loss D: -0.2259\n",
      "Epoch 153, Loss G: -0.9091,Loss D: -0.0910\n",
      "Epoch 154, Loss G: -0.9418,Loss D:  0.1654\n",
      "Epoch 155, Loss G: -0.6373,Loss D: -0.0758\n",
      "Epoch 156, Loss G: -0.6285,Loss D: -0.1953\n",
      "Epoch 157, Loss G: -0.2866,Loss D:  0.0712\n",
      "Epoch 158, Loss G: -0.1081,Loss D: -0.3679\n",
      "Epoch 159, Loss G: -0.1008,Loss D:  0.0671\n",
      "Epoch 160, Loss G: -0.3946,Loss D: -0.1083\n",
      "Epoch 161, Loss G: -0.3111,Loss D: -0.1813\n",
      "Epoch 162, Loss G: -0.2932,Loss D: -0.0118\n",
      "Epoch 163, Loss G: -0.5795,Loss D: -0.1887\n",
      "Epoch 164, Loss G: -0.5808,Loss D:  0.0012\n",
      "Epoch 165, Loss G: -0.5173,Loss D: -0.3189\n",
      "Epoch 166, Loss G:  0.0968,Loss D: -0.1070\n",
      "Epoch 167, Loss G: -0.1787,Loss D: -0.4388\n",
      "Epoch 168, Loss G: -0.3372,Loss D: -0.1056\n",
      "Epoch 169, Loss G: -0.6230,Loss D:  0.1067\n",
      "Epoch 170, Loss G: -0.3993,Loss D: -0.1651\n",
      "Epoch 171, Loss G: -0.5094,Loss D: -0.4315\n",
      "Epoch 172, Loss G: -0.4636,Loss D:  0.2632\n",
      "Epoch 173, Loss G: -0.4907,Loss D: -0.2545\n",
      "Epoch 174, Loss G: -0.3549,Loss D: -0.0294\n",
      "Epoch 175, Loss G: -0.1653,Loss D: -0.0016\n",
      "Epoch 176, Loss G: -0.5348,Loss D:  0.4383\n",
      "Epoch 177, Loss G: -0.1358,Loss D:  0.1231\n",
      "Epoch 178, Loss G: -0.5487,Loss D: -0.3614\n",
      "Epoch 179, Loss G: -0.1175,Loss D: -0.2036\n",
      "Epoch 180, Loss G: -0.0131,Loss D: -0.2538\n",
      "Epoch 181, Loss G: -0.6898,Loss D:  0.1743\n",
      "Epoch 182, Loss G: -0.7075,Loss D:  0.0358\n",
      "Epoch 183, Loss G: -0.2275,Loss D: -0.1115\n",
      "Epoch 184, Loss G: -0.2534,Loss D: -0.2970\n",
      "Epoch 185, Loss G: -0.3423,Loss D: -0.2823\n",
      "Epoch 186, Loss G: -0.8519,Loss D: -0.0091\n",
      "Epoch 187, Loss G: -0.8118,Loss D: -0.5135\n",
      "Epoch 188, Loss G: -0.4834,Loss D: -0.3445\n",
      "Epoch 189, Loss G: -0.7609,Loss D: -0.3969\n",
      "Epoch 190, Loss G: -0.7910,Loss D:  0.0251\n",
      "Epoch 191, Loss G: -0.4421,Loss D:  0.0827\n",
      "Epoch 192, Loss G: -0.5781,Loss D: -0.5282\n",
      "Epoch 193, Loss G: -0.3351,Loss D: -0.2709\n",
      "Epoch 194, Loss G: -0.3656,Loss D: -0.2156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195, Loss G: -0.5261,Loss D: -0.0075\n",
      "Epoch 196, Loss G: -0.5167,Loss D: -0.0146\n",
      "Epoch 197, Loss G: -0.3848,Loss D:  0.0450\n",
      "Epoch 198, Loss G: -0.4742,Loss D: -0.0643\n",
      "Epoch 199, Loss G: -0.2714,Loss D: -0.3152\n",
      "Epoch 200, Loss G: -0.6815,Loss D:  0.0548\n",
      "Epoch 201, Loss G: -0.3424,Loss D: -0.7198\n",
      "Epoch 202, Loss G: -0.4733,Loss D: -0.2780\n",
      "Epoch 203, Loss G: -0.4560,Loss D: -0.1680\n",
      "Epoch 204, Loss G: -0.4523,Loss D: -0.2623\n",
      "Epoch 205, Loss G: -0.3346,Loss D: -0.2298\n",
      "Epoch 206, Loss G: -0.3219,Loss D: -0.0520\n",
      "Epoch 207, Loss G: -0.6410,Loss D:  0.0222\n",
      "Epoch 208, Loss G: -0.6945,Loss D: -0.3104\n",
      "Epoch 209, Loss G: -0.3281,Loss D: -0.1552\n",
      "Epoch 210, Loss G: -0.3758,Loss D:  0.0352\n",
      "Epoch 211, Loss G: -0.5997,Loss D: -0.2539\n",
      "Epoch 212, Loss G: -0.9433,Loss D: -0.2055\n",
      "Epoch 213, Loss G: -0.3669,Loss D: -0.2796\n",
      "Epoch 214, Loss G: -0.6740,Loss D: -0.5681\n",
      "Epoch 215, Loss G: -0.3460,Loss D: -0.1960\n",
      "Epoch 216, Loss G: -0.4993,Loss D:  0.0812\n",
      "Epoch 217, Loss G: -0.2390,Loss D:  0.0715\n",
      "Epoch 218, Loss G: -0.2190,Loss D:  0.0581\n",
      "Epoch 219, Loss G: -0.4999,Loss D: -0.1057\n",
      "Epoch 220, Loss G: -0.4327,Loss D: -0.2667\n",
      "Epoch 221, Loss G: -0.1117,Loss D: -0.4024\n",
      "Epoch 222, Loss G: -0.2729,Loss D: -0.2394\n",
      "Epoch 223, Loss G: -0.9382,Loss D: -0.2359\n",
      "Epoch 224, Loss G: -0.6576,Loss D: -0.1543\n",
      "Epoch 225, Loss G: -0.6933,Loss D:  0.1362\n",
      "Epoch 226, Loss G: -0.6789,Loss D:  0.0653\n",
      "Epoch 227, Loss G: -0.9076,Loss D: -0.1465\n",
      "Epoch 228, Loss G: -0.7067,Loss D: -0.0455\n",
      "Epoch 229, Loss G: -0.6092,Loss D: -0.2768\n",
      "Epoch 230, Loss G: -0.4421,Loss D: -0.3197\n",
      "Epoch 231, Loss G: -0.4451,Loss D: -0.0494\n",
      "Epoch 232, Loss G: -0.4267,Loss D: -0.3240\n",
      "Epoch 233, Loss G: -0.6708,Loss D: -0.2173\n",
      "Epoch 234, Loss G: -0.7320,Loss D:  0.1204\n",
      "Epoch 235, Loss G: -0.3844,Loss D: -0.2552\n",
      "Epoch 236, Loss G: -0.1323,Loss D:  0.0908\n",
      "Epoch 237, Loss G: -0.5989,Loss D: -0.2574\n",
      "Epoch 238, Loss G: -0.7756,Loss D: -0.1392\n",
      "Epoch 239, Loss G: -0.5036,Loss D:  0.0522\n",
      "Epoch 240, Loss G: -1.2277,Loss D: -0.0970\n",
      "Epoch 241, Loss G: -0.4960,Loss D: -0.0785\n",
      "Epoch 242, Loss G: -0.4453,Loss D: -0.3259\n",
      "Epoch 243, Loss G: -0.6760,Loss D:  0.0365\n",
      "Epoch 244, Loss G: -0.7019,Loss D: -0.0792\n",
      "Epoch 245, Loss G: -0.7156,Loss D: -0.0287\n",
      "Epoch 246, Loss G: -0.7414,Loss D:  0.1374\n",
      "Epoch 247, Loss G: -0.6613,Loss D: -0.2397\n",
      "Epoch 248, Loss G: -0.4174,Loss D: -0.0842\n",
      "Epoch 249, Loss G: -0.6416,Loss D:  0.2292\n",
      "Epoch 250, Loss G: -0.6455,Loss D: -0.0473\n",
      "Epoch 251, Loss G: -0.8171,Loss D: -0.3052\n",
      "Epoch 252, Loss G: -1.1034,Loss D: -0.1164\n",
      "Epoch 253, Loss G: -0.5303,Loss D: -0.0332\n",
      "Epoch 254, Loss G: -0.8259,Loss D: -0.0488\n",
      "Epoch 255, Loss G: -0.8273,Loss D: -0.1455\n",
      "Epoch 256, Loss G: -1.0085,Loss D: -0.1423\n",
      "Epoch 257, Loss G: -0.9939,Loss D:  0.1704\n",
      "Epoch 258, Loss G: -0.8204,Loss D: -0.1964\n",
      "Epoch 259, Loss G: -0.8056,Loss D:  0.1393\n",
      "Epoch 260, Loss G: -0.5755,Loss D: -0.2698\n",
      "Epoch 261, Loss G: -0.5882,Loss D: -0.0160\n",
      "Epoch 262, Loss G: -0.7021,Loss D: -0.0676\n",
      "Epoch 263, Loss G: -1.1396,Loss D: -0.4111\n",
      "Epoch 264, Loss G: -0.6160,Loss D:  0.0253\n",
      "Epoch 265, Loss G: -0.5163,Loss D: -0.3519\n",
      "Epoch 266, Loss G: -0.9575,Loss D: -0.1245\n",
      "Epoch 267, Loss G: -0.6242,Loss D: -0.1941\n",
      "Epoch 268, Loss G: -0.7371,Loss D: -0.2783\n",
      "Epoch 269, Loss G: -0.4675,Loss D: -0.0639\n",
      "Epoch 270, Loss G: -0.6273,Loss D:  0.0723\n",
      "Epoch 271, Loss G: -0.4682,Loss D: -0.1475\n",
      "Epoch 272, Loss G: -0.5386,Loss D: -0.1193\n",
      "Epoch 273, Loss G: -0.8612,Loss D: -0.0678\n",
      "Epoch 274, Loss G: -0.6992,Loss D: -0.2288\n",
      "Epoch 275, Loss G: -0.8774,Loss D: -0.3205\n",
      "Epoch 276, Loss G: -0.7236,Loss D:  0.2234\n",
      "Epoch 277, Loss G: -0.7263,Loss D: -0.0645\n",
      "Epoch 278, Loss G: -0.4903,Loss D: -0.1171\n",
      "Epoch 279, Loss G: -0.7958,Loss D: -0.0293\n",
      "Epoch 280, Loss G: -0.6850,Loss D: -0.2408\n",
      "Epoch 281, Loss G: -0.3454,Loss D: -0.3174\n",
      "Epoch 282, Loss G: -0.6004,Loss D: -0.0192\n",
      "Epoch 283, Loss G: -0.6993,Loss D: -0.2943\n",
      "Epoch 284, Loss G: -0.6387,Loss D: -0.0375\n",
      "Epoch 285, Loss G: -0.7774,Loss D:  0.0125\n",
      "Epoch 286, Loss G: -0.4734,Loss D: -0.0649\n",
      "Epoch 287, Loss G: -0.7862,Loss D: -0.0383\n",
      "Epoch 288, Loss G: -0.8359,Loss D: -0.0794\n",
      "Epoch 289, Loss G: -0.7114,Loss D: -0.1252\n",
      "Epoch 290, Loss G: -0.5785,Loss D: -0.2530\n",
      "Epoch 291, Loss G: -0.7966,Loss D: -0.2145\n",
      "Epoch 292, Loss G: -0.4502,Loss D: -0.2386\n",
      "Epoch 293, Loss G: -0.6392,Loss D: -0.0711\n",
      "Epoch 294, Loss G: -0.8231,Loss D: -0.0644\n",
      "Epoch 295, Loss G: -0.7606,Loss D: -0.5262\n",
      "Epoch 296, Loss G: -0.7451,Loss D: -0.0792\n",
      "Epoch 297, Loss G: -0.9466,Loss D:  0.1131\n",
      "Epoch 298, Loss G: -0.8512,Loss D:  0.2109\n",
      "Epoch 299, Loss G: -1.2204,Loss D: -0.1842\n",
      "Epoch 300, Loss G: -0.9649,Loss D:  0.1763\n",
      "Epoch 301, Loss G: -0.5643,Loss D: -0.2560\n",
      "Epoch 302, Loss G: -0.6294,Loss D:  0.1711\n",
      "Epoch 303, Loss G: -0.5973,Loss D: -0.1083\n",
      "Epoch 304, Loss G: -0.8768,Loss D: -0.1616\n",
      "Epoch 305, Loss G: -0.6200,Loss D: -0.2762\n",
      "Epoch 306, Loss G: -0.8998,Loss D:  0.0356\n",
      "Epoch 307, Loss G: -0.7164,Loss D: -0.0432\n",
      "Epoch 308, Loss G: -0.8320,Loss D: -0.1855\n",
      "Epoch 309, Loss G: -0.6331,Loss D: -0.1842\n",
      "Epoch 310, Loss G: -0.5868,Loss D: -0.0810\n",
      "Epoch 311, Loss G: -0.5636,Loss D: -0.1205\n",
      "Epoch 312, Loss G: -0.6761,Loss D: -0.2816\n",
      "Epoch 313, Loss G: -0.3690,Loss D: -0.2160\n",
      "Epoch 314, Loss G: -0.6106,Loss D: -0.1428\n",
      "Epoch 315, Loss G: -0.5485,Loss D: -0.0411\n",
      "Epoch 316, Loss G: -0.6027,Loss D: -0.0350\n",
      "Epoch 317, Loss G: -0.4996,Loss D: -0.1543\n",
      "Epoch 318, Loss G: -0.1570,Loss D: -0.1759\n",
      "Epoch 319, Loss G: -0.6600,Loss D:  0.1328\n",
      "Epoch 320, Loss G: -0.9173,Loss D:  0.1161\n",
      "Epoch 321, Loss G: -0.4722,Loss D: -0.0749\n",
      "Epoch 322, Loss G: -0.4981,Loss D: -0.2932\n",
      "Epoch 323, Loss G: -0.4490,Loss D:  0.0057\n",
      "Epoch 324, Loss G: -0.6763,Loss D: -0.1857\n",
      "Epoch 325, Loss G: -0.8839,Loss D: -0.2102\n",
      "Epoch 326, Loss G: -0.6263,Loss D:  0.0198\n",
      "Epoch 327, Loss G: -0.5369,Loss D:  0.0153\n",
      "Epoch 328, Loss G: -0.5256,Loss D:  0.0266\n",
      "Epoch 329, Loss G: -0.5156,Loss D: -0.1352\n",
      "Epoch 330, Loss G: -0.5898,Loss D: -0.1710\n",
      "Epoch 331, Loss G: -0.5245,Loss D: -0.4147\n",
      "Epoch 332, Loss G: -0.4557,Loss D: -0.0399\n",
      "Epoch 333, Loss G: -0.5902,Loss D: -0.0334\n",
      "Epoch 334, Loss G: -0.7583,Loss D: -0.3484\n",
      "Epoch 335, Loss G: -0.8729,Loss D: -0.0140\n",
      "Epoch 336, Loss G: -0.8809,Loss D: -0.1483\n",
      "Epoch 337, Loss G: -0.6062,Loss D: -0.0138\n",
      "Epoch 338, Loss G: -0.2344,Loss D: -0.0850\n",
      "Epoch 339, Loss G: -0.5057,Loss D:  0.0247\n",
      "Epoch 340, Loss G: -0.4425,Loss D: -0.0968\n",
      "Epoch 341, Loss G: -0.3537,Loss D: -0.2031\n",
      "Epoch 342, Loss G: -0.7123,Loss D: -0.2024\n",
      "Epoch 343, Loss G: -0.3680,Loss D: -0.1018\n",
      "Epoch 344, Loss G: -0.7860,Loss D: -0.0502\n",
      "Epoch 345, Loss G: -0.5115,Loss D: -0.1396\n",
      "Epoch 346, Loss G: -0.8126,Loss D:  0.0471\n",
      "Epoch 347, Loss G: -0.7758,Loss D:  0.0746\n",
      "Epoch 348, Loss G: -0.8493,Loss D: -0.2248\n",
      "Epoch 349, Loss G: -0.8669,Loss D:  0.0635\n",
      "Epoch 350, Loss G: -0.8235,Loss D: -0.3770\n",
      "Epoch 351, Loss G: -0.7427,Loss D: -0.2254\n",
      "Epoch 352, Loss G: -0.5808,Loss D: -0.1055\n",
      "Epoch 353, Loss G: -0.8540,Loss D:  0.1020\n",
      "Epoch 354, Loss G: -0.7492,Loss D: -0.0616\n",
      "Epoch 355, Loss G: -0.9062,Loss D: -0.2115\n",
      "Epoch 356, Loss G: -0.8342,Loss D: -0.1366\n",
      "Epoch 357, Loss G: -0.6910,Loss D:  0.0025\n",
      "Epoch 358, Loss G: -0.6918,Loss D: -0.1613\n",
      "Epoch 359, Loss G: -0.5704,Loss D:  0.0319\n",
      "Epoch 360, Loss G: -0.7480,Loss D: -0.2128\n",
      "Epoch 361, Loss G: -0.6113,Loss D: -0.1072\n",
      "Epoch 362, Loss G: -0.4710,Loss D:  0.1014\n",
      "Epoch 363, Loss G: -0.5148,Loss D: -0.2196\n",
      "Epoch 364, Loss G: -0.5663,Loss D:  0.0567\n",
      "Epoch 365, Loss G: -0.4160,Loss D: -0.2630\n",
      "Epoch 366, Loss G: -0.6312,Loss D: -0.4768\n",
      "Epoch 367, Loss G: -0.9290,Loss D: -0.2357\n",
      "Epoch 368, Loss G: -0.8862,Loss D: -0.2141\n",
      "Epoch 369, Loss G: -0.7190,Loss D: -0.0571\n",
      "Epoch 370, Loss G: -0.5764,Loss D: -0.0525\n",
      "Epoch 371, Loss G: -0.6775,Loss D: -0.2179\n",
      "Epoch 372, Loss G: -0.6564,Loss D: -0.1616\n",
      "Epoch 373, Loss G: -0.7669,Loss D: -0.0077\n",
      "Epoch 374, Loss G: -0.6540,Loss D: -0.2500\n",
      "Epoch 375, Loss G: -0.4725,Loss D: -0.5374\n",
      "Epoch 376, Loss G: -0.2874,Loss D:  0.0967\n",
      "Epoch 377, Loss G: -0.6336,Loss D: -0.0083\n",
      "Epoch 378, Loss G: -0.9344,Loss D: -0.1784\n",
      "Epoch 379, Loss G: -0.7272,Loss D: -0.2735\n",
      "Epoch 380, Loss G: -0.4143,Loss D: -0.2263\n",
      "Epoch 381, Loss G: -0.7541,Loss D: -0.0784\n",
      "Epoch 382, Loss G: -0.6474,Loss D:  0.0589\n",
      "Epoch 383, Loss G: -0.8396,Loss D:  0.0181\n",
      "Epoch 384, Loss G: -0.7923,Loss D: -0.2155\n",
      "Epoch 385, Loss G: -0.3806,Loss D:  0.0138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 386, Loss G: -0.7364,Loss D: -0.0137\n",
      "Epoch 387, Loss G: -0.8928,Loss D:  0.2607\n",
      "Epoch 388, Loss G: -0.7541,Loss D: -0.1009\n",
      "Epoch 389, Loss G: -0.5556,Loss D: -0.0925\n",
      "Epoch 390, Loss G: -0.5772,Loss D: -0.2002\n",
      "Epoch 391, Loss G: -0.6142,Loss D:  0.1093\n",
      "Epoch 392, Loss G: -0.8911,Loss D:  0.0897\n",
      "Epoch 393, Loss G: -0.6778,Loss D: -0.0927\n",
      "Epoch 394, Loss G: -0.7712,Loss D: -0.0347\n",
      "Epoch 395, Loss G: -0.8085,Loss D: -0.0481\n",
      "Epoch 396, Loss G: -1.0440,Loss D: -0.2046\n",
      "Epoch 397, Loss G: -0.6481,Loss D: -0.1431\n",
      "Epoch 398, Loss G: -0.8562,Loss D: -0.0813\n",
      "Epoch 399, Loss G: -0.7476,Loss D:  0.0745\n",
      "Epoch 400, Loss G: -0.6510,Loss D: -0.1932\n",
      "Epoch 401, Loss G: -1.1282,Loss D: -0.0421\n",
      "Epoch 402, Loss G: -0.9055,Loss D: -0.2322\n",
      "Epoch 403, Loss G: -0.7069,Loss D: -0.3115\n",
      "Epoch 404, Loss G: -0.9516,Loss D:  0.1022\n",
      "Epoch 405, Loss G: -0.7850,Loss D:  0.0154\n",
      "Epoch 406, Loss G: -1.0781,Loss D:  0.1001\n",
      "Epoch 407, Loss G: -1.0614,Loss D: -0.0170\n",
      "Epoch 408, Loss G: -0.8128,Loss D: -0.1404\n",
      "Epoch 409, Loss G: -1.0384,Loss D: -0.0797\n",
      "Epoch 410, Loss G: -0.6959,Loss D: -0.1580\n",
      "Epoch 411, Loss G: -0.7516,Loss D: -0.2882\n",
      "Epoch 412, Loss G: -0.6375,Loss D: -0.2501\n",
      "Epoch 413, Loss G: -0.7480,Loss D: -0.0278\n",
      "Epoch 414, Loss G: -0.6893,Loss D: -0.3788\n",
      "Epoch 415, Loss G: -1.0022,Loss D: -0.1790\n",
      "Epoch 416, Loss G: -0.8015,Loss D: -0.2292\n",
      "Epoch 417, Loss G: -0.6518,Loss D: -0.2482\n",
      "Epoch 418, Loss G: -0.7536,Loss D: -0.1560\n",
      "Epoch 419, Loss G: -0.9372,Loss D:  0.0273\n",
      "Epoch 420, Loss G: -0.9219,Loss D:  0.1316\n",
      "Epoch 421, Loss G: -0.8804,Loss D:  0.0381\n",
      "Epoch 422, Loss G: -0.6959,Loss D:  0.0212\n",
      "Epoch 423, Loss G: -0.8587,Loss D:  0.0528\n",
      "Epoch 424, Loss G: -0.9473,Loss D: -0.0717\n",
      "Epoch 425, Loss G: -0.8413,Loss D: -0.1393\n",
      "Epoch 426, Loss G: -0.9416,Loss D: -0.0018\n",
      "Epoch 427, Loss G: -0.7469,Loss D: -0.2611\n",
      "Epoch 428, Loss G: -0.7805,Loss D: -0.3774\n",
      "Epoch 429, Loss G: -0.8574,Loss D:  0.1168\n",
      "Epoch 430, Loss G: -1.0202,Loss D: -0.1102\n",
      "Epoch 431, Loss G: -0.8864,Loss D: -0.0997\n",
      "Epoch 432, Loss G: -0.9451,Loss D: -0.0897\n",
      "Epoch 433, Loss G: -0.3758,Loss D: -0.1606\n",
      "Epoch 434, Loss G: -0.8660,Loss D:  0.1298\n",
      "Epoch 435, Loss G: -0.8638,Loss D: -0.0851\n",
      "Epoch 436, Loss G: -1.1409,Loss D: -0.0190\n",
      "Epoch 437, Loss G: -1.1163,Loss D: -0.1180\n",
      "Epoch 438, Loss G: -0.9152,Loss D: -0.0084\n",
      "Epoch 439, Loss G: -0.9934,Loss D: -0.1863\n",
      "Epoch 440, Loss G: -0.8895,Loss D: -0.0091\n",
      "Epoch 441, Loss G: -1.0073,Loss D: -0.0707\n",
      "Epoch 442, Loss G: -1.0297,Loss D: -0.1256\n",
      "Epoch 443, Loss G: -0.9719,Loss D: -0.0195\n",
      "Epoch 444, Loss G: -0.9477,Loss D: -0.2761\n",
      "Epoch 445, Loss G: -1.1287,Loss D:  0.0632\n",
      "Epoch 446, Loss G: -0.7159,Loss D: -0.1454\n",
      "Epoch 447, Loss G: -0.9034,Loss D:  0.0002\n",
      "Epoch 448, Loss G: -0.7703,Loss D: -0.1824\n",
      "Epoch 449, Loss G: -0.9769,Loss D: -0.1771\n",
      "Epoch 450, Loss G: -0.6605,Loss D: -0.1582\n",
      "Epoch 451, Loss G: -0.6428,Loss D: -0.0729\n",
      "Epoch 452, Loss G: -0.8198,Loss D: -0.1791\n",
      "Epoch 453, Loss G: -0.9181,Loss D:  0.0289\n",
      "Epoch 454, Loss G: -0.9880,Loss D: -0.1756\n",
      "Epoch 455, Loss G: -0.6417,Loss D: -0.1599\n",
      "Epoch 456, Loss G: -0.7837,Loss D: -0.2468\n",
      "Epoch 457, Loss G: -0.7345,Loss D: -0.2645\n",
      "Epoch 458, Loss G: -0.8633,Loss D:  0.0126\n",
      "Epoch 459, Loss G: -1.0630,Loss D: -0.0634\n",
      "Epoch 460, Loss G: -0.8384,Loss D: -0.0728\n",
      "Epoch 461, Loss G: -0.8299,Loss D: -0.2991\n",
      "Epoch 462, Loss G: -0.7885,Loss D: -0.0955\n",
      "Epoch 463, Loss G: -0.9614,Loss D:  0.1101\n",
      "Epoch 464, Loss G: -0.8668,Loss D: -0.1121\n",
      "Epoch 465, Loss G: -0.6227,Loss D: -0.0367\n",
      "Epoch 466, Loss G: -0.6467,Loss D: -0.1003\n",
      "Epoch 467, Loss G: -0.7523,Loss D: -0.0288\n",
      "Epoch 468, Loss G: -0.7975,Loss D: -0.2323\n",
      "Epoch 469, Loss G: -0.8361,Loss D: -0.1475\n",
      "Epoch 470, Loss G: -0.8292,Loss D: -0.2002\n",
      "Epoch 471, Loss G: -0.7344,Loss D: -0.0477\n",
      "Epoch 472, Loss G: -0.7198,Loss D:  0.0321\n",
      "Epoch 473, Loss G: -0.8787,Loss D: -0.0043\n",
      "Epoch 474, Loss G: -0.5508,Loss D: -0.2923\n",
      "Epoch 475, Loss G: -0.6949,Loss D: -0.2942\n",
      "Epoch 476, Loss G: -0.5445,Loss D:  0.0189\n",
      "Epoch 477, Loss G: -0.4933,Loss D: -0.1506\n",
      "Epoch 478, Loss G: -0.5221,Loss D: -0.2732\n",
      "Epoch 479, Loss G: -0.8407,Loss D: -0.0296\n",
      "Epoch 480, Loss G: -0.3302,Loss D: -0.0234\n",
      "Epoch 481, Loss G: -0.6012,Loss D: -0.0608\n",
      "Epoch 482, Loss G: -0.6607,Loss D: -0.0055\n",
      "Epoch 483, Loss G: -0.9576,Loss D:  0.1378\n",
      "Epoch 484, Loss G: -0.3988,Loss D: -0.0381\n",
      "Epoch 485, Loss G: -0.5267,Loss D:  0.1134\n",
      "Epoch 486, Loss G: -0.5585,Loss D:  0.1148\n",
      "Epoch 487, Loss G: -0.5147,Loss D: -0.0771\n",
      "Epoch 488, Loss G: -0.7761,Loss D: -0.2823\n",
      "Epoch 489, Loss G: -0.6560,Loss D:  0.0371\n",
      "Epoch 490, Loss G: -0.7073,Loss D: -0.1298\n",
      "Epoch 491, Loss G: -0.5977,Loss D:  0.0604\n",
      "Epoch 492, Loss G: -0.6338,Loss D: -0.1378\n",
      "Epoch 493, Loss G: -0.6375,Loss D:  0.1188\n",
      "Epoch 494, Loss G: -0.9230,Loss D:  0.0789\n",
      "Epoch 495, Loss G: -0.3480,Loss D: -0.1147\n",
      "Epoch 496, Loss G: -0.3569,Loss D: -0.2521\n",
      "Epoch 497, Loss G: -0.3042,Loss D: -0.1597\n",
      "Epoch 498, Loss G: -0.5156,Loss D: -0.0595\n",
      "Epoch 499, Loss G: -0.5293,Loss D:  0.2267\n",
      "Epoch 500, Loss G: -0.8768,Loss D:  0.1429\n"
     ]
    }
   ],
   "source": [
    "# train CTGAN and generate fake data\n",
    "ctgan = CTGANSynthesizer(verbose=True)\n",
    "ctgan.fit(GAN_data_train, adult_discrete_columns, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    39123\n",
       "1    10877\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create fake samples\n",
    "samples = ctgan.sample(50000)\n",
    "samples['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>workclass_State-gov</th>\n",
       "      <th>workclass_Without-pay</th>\n",
       "      <th>education_10th</th>\n",
       "      <th>education_11th</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   workclass_Federal-gov  workclass_Local-gov  workclass_Never-worked  \\\n",
       "0                      0                    1                       0   \n",
       "1                      0                    0                       0   \n",
       "2                      0                    0                       0   \n",
       "3                      0                    0                       0   \n",
       "4                      0                    0                       0   \n",
       "\n",
       "   workclass_Private  workclass_Self-emp-inc  workclass_Self-emp-not-inc  \\\n",
       "0                  0                       0                           0   \n",
       "1                  1                       0                           0   \n",
       "2                  1                       0                           0   \n",
       "3                  0                       0                           1   \n",
       "4                  1                       0                           0   \n",
       "\n",
       "   workclass_State-gov  workclass_Without-pay  education_10th  education_11th  \\\n",
       "0                    0                      0               0               0   \n",
       "1                    0                      0               0               0   \n",
       "2                    0                      0               0               0   \n",
       "3                    0                      0               0               0   \n",
       "4                    0                      0               0               0   \n",
       "\n",
       "   ...  native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "0  ...                        0                           0   \n",
       "1  ...                        0                           0   \n",
       "2  ...                        0                           0   \n",
       "3  ...                        0                           0   \n",
       "4  ...                        0                           0   \n",
       "\n",
       "   native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                        0                     0                      0   \n",
       "1                        0                     0                      0   \n",
       "2                        0                     0                      0   \n",
       "3                        0                     0                      0   \n",
       "4                        0                     0                      0   \n",
       "\n",
       "   native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                        0                               0   \n",
       "1                        0                               0   \n",
       "2                        0                               0   \n",
       "3                        0                               0   \n",
       "4                        0                               0   \n",
       "\n",
       "   native-country_United-States  native-country_Vietnam  \\\n",
       "0                             1                       0   \n",
       "1                             1                       0   \n",
       "2                             1                       0   \n",
       "3                             1                       0   \n",
       "4                             1                       0   \n",
       "\n",
       "   native-country_Yugoslavia  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the Ruiwen's Encoder for ML utility and CTGAN transformer for GAN\n",
    "str_cols= [ 'workclass', 'education', 'marital-status', 'relationship','race', 'sex','native-country']\n",
    "num_cols = ['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week', 'label']\n",
    "dataframe = pd.DataFrame(samples.loc[:,str_cols])\n",
    "\n",
    "one_hot_columns = pd.DataFrame()\n",
    "for col_name, item in dataframe.iteritems():\n",
    "    \n",
    "    #print(col_name)\n",
    "    #print(item)\n",
    "    col = pd.get_dummies(item, prefix=col_name)\n",
    "    one_hot_columns =pd.concat([one_hot_columns,col],axis=1)\n",
    "one_hot_columns.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire data is concat of discrete and contiuous cols\n",
    "fake_all = pd.concat([one_hot_columns,samples.loc[:,num_cols]],axis=1)\n",
    "#adult_data_all.head()\n",
    "fake_X = fake_all.drop([\"label\"],axis=1)\n",
    "fake_X, fake_y = fake_X,fake_all.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure cols of generated data has the same index sort \n",
    "fake_X = fake_X.reindex(adult_data_all.columns,axis=1, fill_value=0) # fill new cols with 0\n",
    "#len(fake_X.columns)\n",
    "fake_X = fake_X.drop([\"label\"],axis=1) # separate X and Y for training\n",
    "fake_X, fake_y = fake_X,fake_all.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the GAN generated train data:  (50000, 91)\n",
      "shape of the GAN generated labels:  (50000,)\n",
      "shape of the original test data:  (14653, 91)\n",
      "number of events in the fake data:  10877\n"
     ]
    }
   ],
   "source": [
    "# check the training data shape to agree with the original data (# of features/cols)\n",
    "print('shape of the GAN generated train data: ',fake_X.shape)\n",
    "print('shape of the GAN generated labels: ',fake_y.shape)\n",
    "print('shape of the original test data: ',orig_X_test.shape)\n",
    "print('number of events in the fake data: ', sum(fake_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML scores for the GAN generated data:\n",
      "Decision Tree Acc:  0.7976523578789326 f-1:  0.33295838020247476 AUC: 0.841645417607766\n",
      "Linear SVM Acc:  0.3564457790213608 f-1:  0.37133333333333335 AUC: 0.4867450664095051\n",
      "Random Forest Acc:  0.8259742032348325 f-1:  0.5513722730471499 AUC: 0.8808836855255372\n",
      "Logistic Regression Acc:  0.7958097317955367 f-1:  0.3415492957746479 AUC: 0.6166703419435491\n",
      "MLP Acc:  0.5426875042653382 f-1:  0.47073690861701284 AUC: 0.7231856447331998\n"
     ]
    }
   ],
   "source": [
    "# train a classifier on the CTGAN generated data\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5,random_state=0),\n",
    "    SVC(kernel = 'linear', max_iter=1000, C=0.025, random_state=0, probability=True),\n",
    "    RandomForestClassifier(n_estimators=200, random_state=0),\n",
    "    LogisticRegression(max_iter=1000, random_state=0),\n",
    "    MLPClassifier(alpha=1, max_iter=1000, random_state=0)]\n",
    "\n",
    "print('ML scores for the GAN generated data:')\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(fake_X, fake_y)\n",
    "    score = clf.score(orig_X_test, orig_y_test)\n",
    "    y_pred = clf.predict(orig_X_test)\n",
    "    Acc = accuracy_score(orig_y_test, y_pred)\n",
    "    fscore = f1_score(orig_y_test, y_pred, average='binary')\n",
    "    AUC = roc_auc_score(orig_y_test, clf.predict_proba(orig_X_test)[:, 1])\n",
    "    print(name,'Acc: ', Acc, 'f-1: ', fscore, 'AUC:', AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
