{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swat as sw\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_data = pd.read_csv('../gan-testing/data/covertype.csv')\n",
    "cov_discrete_columns = \"v11,v12,v13,v14,v15,v16,v17,v18,v19,v20,v21,v22,v23,v24,v25,v26,v27,v28,v29,v30,v31,v32,v33,v34,v35,v36,v37,v38,v39,v40,v41,v42,v43,v44,v45,v46,v47,v48,v49,v50,v51,v52,v53,v54,label\".split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v46</th>\n",
       "      <th>v47</th>\n",
       "      <th>v48</th>\n",
       "      <th>v49</th>\n",
       "      <th>v50</th>\n",
       "      <th>v51</th>\n",
       "      <th>v52</th>\n",
       "      <th>v53</th>\n",
       "      <th>v54</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581007</th>\n",
       "      <td>2396</td>\n",
       "      <td>153</td>\n",
       "      <td>20</td>\n",
       "      <td>85</td>\n",
       "      <td>17</td>\n",
       "      <td>108</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>118</td>\n",
       "      <td>837</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581008</th>\n",
       "      <td>2391</td>\n",
       "      <td>152</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>119</td>\n",
       "      <td>845</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581009</th>\n",
       "      <td>2386</td>\n",
       "      <td>159</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>236</td>\n",
       "      <td>241</td>\n",
       "      <td>130</td>\n",
       "      <td>854</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581010</th>\n",
       "      <td>2384</td>\n",
       "      <td>170</td>\n",
       "      <td>15</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>230</td>\n",
       "      <td>245</td>\n",
       "      <td>143</td>\n",
       "      <td>864</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581011</th>\n",
       "      <td>2383</td>\n",
       "      <td>165</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>231</td>\n",
       "      <td>244</td>\n",
       "      <td>141</td>\n",
       "      <td>875</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581012 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          v1   v2  v3   v4   v5    v6   v7   v8   v9   v10  ...  v46  v47  \\\n",
       "0       2596   51   3  258    0   510  221  232  148  6279  ...    0    0   \n",
       "1       2590   56   2  212   -6   390  220  235  151  6225  ...    0    0   \n",
       "2       2804  139   9  268   65  3180  234  238  135  6121  ...    0    0   \n",
       "3       2785  155  18  242  118  3090  238  238  122  6211  ...    0    0   \n",
       "4       2595   45   2  153   -1   391  220  234  150  6172  ...    0    0   \n",
       "...      ...  ...  ..  ...  ...   ...  ...  ...  ...   ...  ...  ...  ...   \n",
       "581007  2396  153  20   85   17   108  240  237  118   837  ...    0    0   \n",
       "581008  2391  152  19   67   12    95  240  237  119   845  ...    0    0   \n",
       "581009  2386  159  17   60    7    90  236  241  130   854  ...    0    0   \n",
       "581010  2384  170  15   60    5    90  230  245  143   864  ...    0    0   \n",
       "581011  2383  165  13   60    4    67  231  244  141   875  ...    0    0   \n",
       "\n",
       "        v48  v49  v50  v51  v52  v53  v54  label  \n",
       "0         0    0    0    0    0    0    0      5  \n",
       "1         0    0    0    0    0    0    0      5  \n",
       "2         0    0    0    0    0    0    0      2  \n",
       "3         0    0    0    0    0    0    0      2  \n",
       "4         0    0    0    0    0    0    0      5  \n",
       "...     ...  ...  ...  ...  ...  ...  ...    ...  \n",
       "581007    0    0    0    0    0    0    0      3  \n",
       "581008    0    0    0    0    0    0    0      3  \n",
       "581009    0    0    0    0    0    0    0      3  \n",
       "581010    0    0    0    0    0    0    0      3  \n",
       "581011    0    0    0    0    0    0    0      3  \n",
       "\n",
       "[581012 rows x 55 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_numeric_columns = [x for x in cov_data.columns if x not in cov_discrete_columns]\n",
    "cov_numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    283301\n",
       "1    211840\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick only two labels for binary classification\n",
    "cov_data = cov_data[cov_data.label <=2]##binary classification\n",
    "df=cov_data.loc[:,'label']\n",
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample_df(df, col, n_samples):\n",
    "    n = min(n_samples, df[col].value_counts().min())\n",
    "    df_ = df.groupby(col).apply(lambda x: x.sample(n))\n",
    "    df_.index = df_.index.droplevel(0)\n",
    "    return df_\n",
    "\n",
    "cov_data_sampled = stratified_sample_df(cov_data, 'label', 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    25000\n",
       "1    25000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=cov_data_sampled.loc[:,'label']\n",
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    25000\n",
      "1    25000\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cov_X = cov_data_sampled.drop([\"label\"],axis=1)\n",
    "orig_X, orig_y = cov_X,cov_data_sampled.loc[:,\"label\"]\n",
    "print(orig_y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "orig_X_train, orig_X_test, orig_y_train, orig_y_test = train_test_split(orig_X, orig_y, test_size=0.3, random_state=123)\n",
    "my_data1_train = pd.concat([orig_X_train,orig_y_train],axis=1)\n",
    "my_data1_test = pd.concat([orig_X_test,orig_y_test],axis=1)\n",
    "#my_data1_test.describe()\n",
    "orig_y_train = orig_y_train.astype('int')\n",
    "orig_y_test = orig_y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train ML models on the train set of the original data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "names = [\"Decision Tree\",\"Linear SVM\", \"Random Forest\", \"Logistic Regression\",\"MLP\"]\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5,random_state=0),\n",
    "    SVC(kernel = 'linear', max_iter=1000, C=0.025, random_state=0, probability=True),\n",
    "    RandomForestClassifier(n_estimators=200, random_state=0),\n",
    "    LogisticRegression(max_iter=1000, random_state=0),\n",
    "    MLPClassifier(alpha=1, max_iter=1000, random_state=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML scores for the original data:\n",
      "Decision Tree Acc:  0.7616 f-1:  0.7622656561627443 AUC: 0.8335347270362052\n",
      "Linear SVM Acc:  0.4634 f-1:  0.5254407169388597 AUC: 0.4473331177870317\n",
      "Random Forest Acc:  0.8948 f-1:  0.8925799863852961 AUC: 0.9631996006708671\n",
      "Logistic Regression Acc:  0.7638666666666667 f-1:  0.7625368731563422 AUC: 0.8405767459921754\n",
      "MLP Acc:  0.567 f-1:  0.23145189918352857 AUC: 0.757500932159804\n"
     ]
    }
   ],
   "source": [
    "print('ML scores for the original data:')\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(orig_X_train, orig_y_train)\n",
    "    score = clf.score(orig_X_test, orig_y_test)\n",
    "    y_pred = clf.predict(orig_X_test)\n",
    "    Acc = accuracy_score(orig_y_test, y_pred)\n",
    "    fscore = f1_score(orig_y_test, y_pred, average='binary')\n",
    "    AUC = roc_auc_score(orig_y_test, clf.predict_proba(orig_X_test)[:, 1])\n",
    "    print(name,'Acc: ', Acc, 'f-1: ', fscore, 'AUC:', AUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v11',\n",
       " 'v12',\n",
       " 'v13',\n",
       " 'v14',\n",
       " 'v15',\n",
       " 'v16',\n",
       " 'v17',\n",
       " 'v18',\n",
       " 'v19',\n",
       " 'v20',\n",
       " 'v21',\n",
       " 'v22',\n",
       " 'v23',\n",
       " 'v24',\n",
       " 'v25',\n",
       " 'v26',\n",
       " 'v27',\n",
       " 'v28',\n",
       " 'v29',\n",
       " 'v30',\n",
       " 'v31',\n",
       " 'v32',\n",
       " 'v33',\n",
       " 'v34',\n",
       " 'v35',\n",
       " 'v36',\n",
       " 'v37',\n",
       " 'v38',\n",
       " 'v39',\n",
       " 'v40',\n",
       " 'v41',\n",
       " 'v42',\n",
       " 'v43',\n",
       " 'v44',\n",
       " 'v45',\n",
       " 'v46',\n",
       " 'v47',\n",
       " 'v48',\n",
       " 'v49',\n",
       " 'v50',\n",
       " 'v51',\n",
       " 'v52',\n",
       " 'v53',\n",
       " 'v54',\n",
       " 'label']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # now input the data to CPCTGAN and train\n",
    "# cov_discrete_columns = cov_discrete_columns[:-1]\n",
    "cov_discrete_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the train data for gan with same seed as the ML utility\n",
    "GAN_X = cov_data_sampled.drop([\"label\"],axis=1)\n",
    "GAN_orig_X, GAN_orig_y = GAN_X,cov_data_sampled.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "GAN_X_train, GAN_X_test, GAN_y_train, GAN_y_test = train_test_split(GAN_orig_X, GAN_orig_y, test_size=0.3, random_state=123)\n",
    "GAN_data_train = pd.concat([GAN_X_train,GAN_y_train],axis=1)\n",
    "GAN_data_test = pd.concat([GAN_X_test,GAN_y_test],axis=1)\n",
    "#my_data1_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sw.CAS('dl2073.clstr.rnd.sas.com',33789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Added action set 'generativeAdversarialNet'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; actionset</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>generativeAdversarialNet</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.352s</span> &#183; <span class=\"cas-user\">user 3.36s</span> &#183; <span class=\"cas-sys\">sys 2.86s</span> &#183; <span class=\"cas-memory\">mem 0.222MB</span></small></p>"
      ],
      "text/plain": [
       "[actionset]\n",
       "\n",
       " 'generativeAdversarialNet'\n",
       "\n",
       "+ Elapsed: 0.352s, user: 3.36s, sys: 2.86s, mem: 0.222mb"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.loadactionset('generativeAdversarialNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table GAN_DATA_TRAIN in caslib CASUSER(alphel).\n",
      "NOTE: The table GAN_DATA_TRAIN has been created in caslib CASUSER(alphel) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; caslib</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASUSER(alphel)</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; tableName</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>GAN_DATA_TRAIN</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; casTable</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASTable('GAN_DATA_TRAIN', caslib='CASUSER(alphel)')</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.161s</span> &#183; <span class=\"cas-user\">user 0.281s</span> &#183; <span class=\"cas-sys\">sys 0.0886s</span> &#183; <span class=\"cas-memory\">mem 80.3MB</span></small></p>"
      ],
      "text/plain": [
       "[caslib]\n",
       "\n",
       " 'CASUSER(alphel)'\n",
       "\n",
       "[tableName]\n",
       "\n",
       " 'GAN_DATA_TRAIN'\n",
       "\n",
       "[casTable]\n",
       "\n",
       " CASTable('GAN_DATA_TRAIN', caslib='CASUSER(alphel)')\n",
       "\n",
       "+ Elapsed: 0.161s, user: 0.281s, sys: 0.0886s, mem: 80.3mb"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.upload(GAN_data_train, casout=dict(name='GAN_data_train', replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table CEN in caslib CASUSER(alphel).\n",
      "NOTE: The table CEN has been created in caslib CASUSER(alphel) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VarName</th>\n",
       "      <th>Centroid_i</th>\n",
       "      <th>weight</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.16094</td>\n",
       "      <td>1876.13</td>\n",
       "      <td>765.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.15416</td>\n",
       "      <td>1625.05</td>\n",
       "      <td>718.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.22085</td>\n",
       "      <td>910.25</td>\n",
       "      <td>414.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.15237</td>\n",
       "      <td>2396.76</td>\n",
       "      <td>673.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.15592</td>\n",
       "      <td>1468.69</td>\n",
       "      <td>672.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>v8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.13265</td>\n",
       "      <td>216.85</td>\n",
       "      <td>15.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>v8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.17448</td>\n",
       "      <td>238.29</td>\n",
       "      <td>9.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>v9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.36185</td>\n",
       "      <td>146.95</td>\n",
       "      <td>33.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>v9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.30082</td>\n",
       "      <td>118.90</td>\n",
       "      <td>47.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>v9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.33733</td>\n",
       "      <td>143.99</td>\n",
       "      <td>34.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   VarName  Centroid_i   weight     Mean     Std\n",
       "0      v10           1  0.16094  1876.13  765.03\n",
       "1      v10           2  0.15416  1625.05  718.52\n",
       "2      v10           3  0.22085   910.25  414.09\n",
       "3      v10           4  0.15237  2396.76  673.61\n",
       "4      v10           5  0.15592  1468.69  672.13\n",
       "..     ...         ...      ...      ...     ...\n",
       "59      v8           6  0.13265   216.85   15.12\n",
       "60      v8           7  0.17448   238.29    9.43\n",
       "61      v9           1  0.36185   146.95   33.01\n",
       "62      v9           2  0.30082   118.90   47.51\n",
       "63      v9           3  0.33733   143.99   34.99\n",
       "\n",
       "[64 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cen = pd.read_csv(\"../gan-testing/data/covertype_centroids.csv\")\n",
    "s.upload(cen, casout=dict(name='cen', replace=True))\n",
    "cen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using device: GPU 0.\n",
      "NOTE: Epoch i=1, ae_loss=  0.1302.\n",
      "NOTE: Epoch i=2, ae_loss=  0.1174.\n",
      "NOTE: Epoch i=3, ae_loss=  0.1093.\n",
      "NOTE: Epoch i=4, ae_loss=  0.1061.\n",
      "NOTE: Epoch i=5, ae_loss=  0.0944.\n",
      "NOTE: Epoch i=6, ae_loss=  0.0555.\n",
      "NOTE: Epoch i=7, ae_loss=  0.0505.\n",
      "NOTE: Epoch i=8, ae_loss=  0.0452.\n",
      "NOTE: Epoch i=9, ae_loss=  0.0437.\n",
      "NOTE: Epoch i=10, ae_loss=  0.0419.\n",
      "NOTE: Epoch i=11, ae_loss=  0.0408.\n",
      "NOTE: Epoch i=12, ae_loss=  0.0408.\n",
      "NOTE: Epoch i=13, ae_loss=  0.0374.\n",
      "NOTE: Epoch i=14, ae_loss=  0.0361.\n",
      "NOTE: Epoch i=15, ae_loss=  0.0368.\n",
      "NOTE: Epoch i=16, ae_loss=  0.0343.\n",
      "NOTE: Epoch i=17, ae_loss=  0.0325.\n",
      "NOTE: Epoch i=18, ae_loss=  0.0319.\n",
      "NOTE: Epoch i=19, ae_loss=  0.0312.\n",
      "NOTE: Epoch i=20, ae_loss=  0.0310.\n",
      "NOTE: Epoch i=21, ae_loss=  0.0306.\n",
      "NOTE: Epoch i=22, ae_loss=  0.0304.\n",
      "NOTE: Epoch i=23, ae_loss=  0.0289.\n",
      "NOTE: Epoch i=24, ae_loss=  0.0274.\n",
      "NOTE: Epoch i=25, ae_loss=  0.0277.\n",
      "NOTE: Epoch i=26, ae_loss=  0.0271.\n",
      "NOTE: Epoch i=27, ae_loss=  0.0264.\n",
      "NOTE: Epoch i=28, ae_loss=  0.0266.\n",
      "NOTE: Epoch i=29, ae_loss=  0.0252.\n",
      "NOTE: Epoch i=30, ae_loss=  0.0249.\n",
      "NOTE: Epoch i=31, ae_loss=  0.0245.\n",
      "NOTE: Epoch i=32, ae_loss=  0.0250.\n",
      "NOTE: Epoch i=33, ae_loss=  0.0245.\n",
      "NOTE: Epoch i=34, ae_loss=  0.0231.\n",
      "NOTE: Epoch i=35, ae_loss=  0.0234.\n",
      "NOTE: Epoch i=36, ae_loss=  0.0248.\n",
      "NOTE: Epoch i=37, ae_loss=  0.0225.\n",
      "NOTE: Epoch i=38, ae_loss=  0.0212.\n",
      "NOTE: Epoch i=39, ae_loss=  0.0221.\n",
      "NOTE: Epoch i=40, ae_loss=  0.0219.\n",
      "NOTE: Epoch i=41, ae_loss=  0.0215.\n",
      "NOTE: Epoch i=42, ae_loss=  0.0203.\n",
      "NOTE: Epoch i=43, ae_loss=  0.0221.\n",
      "NOTE: Epoch i=44, ae_loss=  0.0224.\n",
      "NOTE: Epoch i=45, ae_loss=  0.0203.\n",
      "NOTE: Epoch i=46, ae_loss=  0.0209.\n",
      "NOTE: Epoch i=47, ae_loss=  0.0203.\n",
      "NOTE: Epoch i=48, ae_loss=  0.0199.\n",
      "NOTE: Epoch i=49, ae_loss=  0.0189.\n",
      "NOTE: Epoch i=50, ae_loss=  0.0200.\n",
      "NOTE: Epoch i=51, ae_loss=  0.0194.\n",
      "NOTE: Epoch i=52, ae_loss=  0.0205.\n",
      "NOTE: Epoch i=53, ae_loss=  0.0199.\n",
      "NOTE: Epoch i=54, ae_loss=  0.0195.\n",
      "NOTE: Epoch i=55, ae_loss=  0.0184.\n",
      "NOTE: Epoch i=56, ae_loss=  0.0187.\n",
      "NOTE: Epoch i=57, ae_loss=  0.0194.\n",
      "NOTE: Epoch i=58, ae_loss=  0.0184.\n",
      "NOTE: Epoch i=59, ae_loss=  0.0188.\n",
      "NOTE: Epoch i=60, ae_loss=  0.0180.\n",
      "NOTE: Epoch i=61, ae_loss=  0.0185.\n",
      "NOTE: Epoch i=62, ae_loss=  0.0180.\n",
      "NOTE: Epoch i=63, ae_loss=  0.0186.\n",
      "NOTE: Epoch i=64, ae_loss=  0.0186.\n",
      "NOTE: Epoch i=65, ae_loss=  0.0186.\n",
      "NOTE: Epoch i=66, ae_loss=  0.0185.\n",
      "NOTE: Epoch i=67, ae_loss=  0.0178.\n",
      "NOTE: Epoch i=68, ae_loss=  0.0185.\n",
      "NOTE: Epoch i=69, ae_loss=  0.0186.\n",
      "NOTE: Epoch i=70, ae_loss=  0.0179.\n",
      "NOTE: Epoch i=71, ae_loss=  0.0177.\n",
      "NOTE: Epoch i=72, ae_loss=  0.0187.\n",
      "NOTE: Epoch i=73, ae_loss=  0.0188.\n",
      "NOTE: Epoch i=74, ae_loss=  0.0176.\n",
      "NOTE: Epoch i=75, ae_loss=  0.0182.\n",
      "NOTE: Epoch i=76, ae_loss=  0.0176.\n",
      "NOTE: Epoch i=77, ae_loss=  0.0178.\n",
      "NOTE: Epoch i=78, ae_loss=  0.0180.\n",
      "NOTE: Epoch i=79, ae_loss=  0.0173.\n",
      "NOTE: Epoch i=80, ae_loss=  0.0178.\n",
      "NOTE: Epoch i=81, ae_loss=  0.0171.\n",
      "NOTE: Epoch i=82, ae_loss=  0.0185.\n",
      "NOTE: Epoch i=83, ae_loss=  0.0173.\n",
      "NOTE: Epoch i=84, ae_loss=  0.0174.\n",
      "NOTE: Epoch i=85, ae_loss=  0.0173.\n",
      "NOTE: Epoch i=86, ae_loss=  0.0174.\n",
      "NOTE: Epoch i=87, ae_loss=  0.0166.\n",
      "NOTE: Epoch i=88, ae_loss=  0.0174.\n",
      "NOTE: Epoch i=89, ae_loss=  0.0164.\n",
      "NOTE: Epoch i=90, ae_loss=  0.0166.\n",
      "NOTE: Epoch i=91, ae_loss=  0.0175.\n",
      "NOTE: Epoch i=92, ae_loss=  0.0172.\n",
      "NOTE: Epoch i=93, ae_loss=  0.0167.\n",
      "NOTE: Epoch i=94, ae_loss=  0.0164.\n",
      "NOTE: Epoch i=95, ae_loss=  0.0162.\n",
      "NOTE: Epoch i=96, ae_loss=  0.0162.\n",
      "NOTE: Epoch i=97, ae_loss=  0.0158.\n",
      "NOTE: Epoch i=98, ae_loss=  0.0168.\n",
      "NOTE: Epoch i=99, ae_loss=  0.0159.\n",
      "NOTE: Epoch i=100, ae_loss=  0.0163.\n",
      "NOTE: Epoch i=101, ae_loss=  0.0160.\n",
      "NOTE: Epoch i=102, ae_loss=  0.0163.\n",
      "NOTE: Epoch i=103, ae_loss=  0.0158.\n",
      "NOTE: Epoch i=104, ae_loss=  0.0157.\n",
      "NOTE: Epoch i=105, ae_loss=  0.0154.\n",
      "NOTE: Epoch i=106, ae_loss=  0.0151.\n",
      "NOTE: Epoch i=107, ae_loss=  0.0159.\n",
      "NOTE: Epoch i=108, ae_loss=  0.0166.\n",
      "NOTE: Epoch i=109, ae_loss=  0.0164.\n",
      "NOTE: Epoch i=110, ae_loss=  0.0156.\n",
      "NOTE: Epoch i=111, ae_loss=  0.0146.\n",
      "NOTE: Epoch i=112, ae_loss=  0.0162.\n",
      "NOTE: Epoch i=113, ae_loss=  0.0159.\n",
      "NOTE: Epoch i=114, ae_loss=  0.0164.\n",
      "NOTE: Epoch i=115, ae_loss=  0.0158.\n",
      "NOTE: Epoch i=116, ae_loss=  0.0151.\n",
      "NOTE: Epoch i=117, ae_loss=  0.0155.\n",
      "NOTE: Epoch i=118, ae_loss=  0.0158.\n",
      "NOTE: Epoch i=119, ae_loss=  0.0153.\n",
      "NOTE: Epoch i=120, ae_loss=  0.0159.\n",
      "NOTE: Epoch i=121, ae_loss=  0.0151.\n",
      "NOTE: Epoch i=122, ae_loss=  0.0160.\n",
      "NOTE: Epoch i=123, ae_loss=  0.0154.\n",
      "NOTE: Epoch i=124, ae_loss=  0.0162.\n",
      "NOTE: Epoch i=125, ae_loss=  0.0152.\n",
      "NOTE: Epoch i=126, ae_loss=  0.0143.\n",
      "NOTE: Epoch i=127, ae_loss=  0.0154.\n",
      "NOTE: Epoch i=128, ae_loss=  0.0158.\n",
      "NOTE: Epoch i=129, ae_loss=  0.0155.\n",
      "NOTE: Epoch i=130, ae_loss=  0.0157.\n",
      "NOTE: Epoch i=131, ae_loss=  0.0146.\n",
      "NOTE: Epoch i=132, ae_loss=  0.0145.\n",
      "NOTE: Epoch i=133, ae_loss=  0.0157.\n",
      "NOTE: Epoch i=134, ae_loss=  0.0157.\n",
      "NOTE: Epoch i=135, ae_loss=  0.0158.\n",
      "NOTE: Epoch i=136, ae_loss=  0.0146.\n",
      "NOTE: Epoch i=137, ae_loss=  0.0156.\n",
      "NOTE: Epoch i=138, ae_loss=  0.0158.\n",
      "NOTE: Epoch i=139, ae_loss=  0.0146.\n",
      "NOTE: Epoch i=140, ae_loss=  0.0146.\n",
      "NOTE: Epoch i=141, ae_loss=  0.0153.\n",
      "NOTE: Epoch i=142, ae_loss=  0.0146.\n",
      "NOTE: Epoch i=143, ae_loss=  0.0143.\n",
      "NOTE: Epoch i=144, ae_loss=  0.0141.\n",
      "NOTE: Epoch i=145, ae_loss=  0.0154.\n",
      "NOTE: Epoch i=146, ae_loss=  0.0155.\n",
      "NOTE: Epoch i=147, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=148, ae_loss=  0.0155.\n",
      "NOTE: Epoch i=149, ae_loss=  0.0146.\n",
      "NOTE: Epoch i=150, ae_loss=  0.0148.\n",
      "NOTE: Epoch i=151, ae_loss=  0.0140.\n",
      "NOTE: Epoch i=152, ae_loss=  0.0140.\n",
      "NOTE: Epoch i=153, ae_loss=  0.0149.\n",
      "NOTE: Epoch i=154, ae_loss=  0.0147.\n",
      "NOTE: Epoch i=155, ae_loss=  0.0148.\n",
      "NOTE: Epoch i=156, ae_loss=  0.0150.\n",
      "NOTE: Epoch i=157, ae_loss=  0.0139.\n",
      "NOTE: Epoch i=158, ae_loss=  0.0148.\n",
      "NOTE: Epoch i=159, ae_loss=  0.0141.\n",
      "NOTE: Epoch i=160, ae_loss=  0.0142.\n",
      "NOTE: Epoch i=161, ae_loss=  0.0138.\n",
      "NOTE: Epoch i=162, ae_loss=  0.0143.\n",
      "NOTE: Epoch i=163, ae_loss=  0.0147.\n",
      "NOTE: Epoch i=164, ae_loss=  0.0134.\n",
      "NOTE: Epoch i=165, ae_loss=  0.0152.\n",
      "NOTE: Epoch i=166, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=167, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=168, ae_loss=  0.0134.\n",
      "NOTE: Epoch i=169, ae_loss=  0.0151.\n",
      "NOTE: Epoch i=170, ae_loss=  0.0147.\n",
      "NOTE: Epoch i=171, ae_loss=  0.0140.\n",
      "NOTE: Epoch i=172, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=173, ae_loss=  0.0138.\n",
      "NOTE: Epoch i=174, ae_loss=  0.0139.\n",
      "NOTE: Epoch i=175, ae_loss=  0.0143.\n",
      "NOTE: Epoch i=176, ae_loss=  0.0145.\n",
      "NOTE: Epoch i=177, ae_loss=  0.0144.\n",
      "NOTE: Epoch i=178, ae_loss=  0.0145.\n",
      "NOTE: Epoch i=179, ae_loss=  0.0149.\n",
      "NOTE: Epoch i=180, ae_loss=  0.0144.\n",
      "NOTE: Epoch i=181, ae_loss=  0.0141.\n",
      "NOTE: Epoch i=182, ae_loss=  0.0145.\n",
      "NOTE: Epoch i=183, ae_loss=  0.0141.\n",
      "NOTE: Epoch i=184, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=185, ae_loss=  0.0139.\n",
      "NOTE: Epoch i=186, ae_loss=  0.0140.\n",
      "NOTE: Epoch i=187, ae_loss=  0.0142.\n",
      "NOTE: Epoch i=188, ae_loss=  0.0138.\n",
      "NOTE: Epoch i=189, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=190, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=191, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=192, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=193, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=194, ae_loss=  0.0139.\n",
      "NOTE: Epoch i=195, ae_loss=  0.0137.\n",
      "NOTE: Epoch i=196, ae_loss=  0.0142.\n",
      "NOTE: Epoch i=197, ae_loss=  0.0140.\n",
      "NOTE: Epoch i=198, ae_loss=  0.0146.\n",
      "NOTE: Epoch i=199, ae_loss=  0.0144.\n",
      "NOTE: Epoch i=200, ae_loss=  0.0141.\n",
      "NOTE: Epoch i=201, ae_loss=  0.0141.\n",
      "NOTE: Epoch i=202, ae_loss=  0.0143.\n",
      "NOTE: Epoch i=203, ae_loss=  0.0139.\n",
      "NOTE: Epoch i=204, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=205, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=206, ae_loss=  0.0148.\n",
      "NOTE: Epoch i=207, ae_loss=  0.0143.\n",
      "NOTE: Epoch i=208, ae_loss=  0.0139.\n",
      "NOTE: Epoch i=209, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=210, ae_loss=  0.0137.\n",
      "NOTE: Epoch i=211, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=212, ae_loss=  0.0140.\n",
      "NOTE: Epoch i=213, ae_loss=  0.0141.\n",
      "NOTE: Epoch i=214, ae_loss=  0.0137.\n",
      "NOTE: Epoch i=215, ae_loss=  0.0144.\n",
      "NOTE: Epoch i=216, ae_loss=  0.0134.\n",
      "NOTE: Epoch i=217, ae_loss=  0.0138.\n",
      "NOTE: Epoch i=218, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=219, ae_loss=  0.0141.\n",
      "NOTE: Epoch i=220, ae_loss=  0.0137.\n",
      "NOTE: Epoch i=221, ae_loss=  0.0139.\n",
      "NOTE: Epoch i=222, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=223, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=224, ae_loss=  0.0141.\n",
      "NOTE: Epoch i=225, ae_loss=  0.0136.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=226, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=227, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=228, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=229, ae_loss=  0.0141.\n",
      "NOTE: Epoch i=230, ae_loss=  0.0139.\n",
      "NOTE: Epoch i=231, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=232, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=233, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=234, ae_loss=  0.0142.\n",
      "NOTE: Epoch i=235, ae_loss=  0.0140.\n",
      "NOTE: Epoch i=236, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=237, ae_loss=  0.0142.\n",
      "NOTE: Epoch i=238, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=239, ae_loss=  0.0139.\n",
      "NOTE: Epoch i=240, ae_loss=  0.0142.\n",
      "NOTE: Epoch i=241, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=242, ae_loss=  0.0134.\n",
      "NOTE: Epoch i=243, ae_loss=  0.0137.\n",
      "NOTE: Epoch i=244, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=245, ae_loss=  0.0139.\n",
      "NOTE: Epoch i=246, ae_loss=  0.0121.\n",
      "NOTE: Epoch i=247, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=248, ae_loss=  0.0123.\n",
      "NOTE: Epoch i=249, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=250, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=251, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=252, ae_loss=  0.0138.\n",
      "NOTE: Epoch i=253, ae_loss=  0.0141.\n",
      "NOTE: Epoch i=254, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=255, ae_loss=  0.0138.\n",
      "NOTE: Epoch i=256, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=257, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=258, ae_loss=  0.0137.\n",
      "NOTE: Epoch i=259, ae_loss=  0.0137.\n",
      "NOTE: Epoch i=260, ae_loss=  0.0137.\n",
      "NOTE: Epoch i=261, ae_loss=  0.0143.\n",
      "NOTE: Epoch i=262, ae_loss=  0.0141.\n",
      "NOTE: Epoch i=263, ae_loss=  0.0140.\n",
      "NOTE: Epoch i=264, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=265, ae_loss=  0.0138.\n",
      "NOTE: Epoch i=266, ae_loss=  0.0126.\n",
      "NOTE: Epoch i=267, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=268, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=269, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=270, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=271, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=272, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=273, ae_loss=  0.0123.\n",
      "NOTE: Epoch i=274, ae_loss=  0.0141.\n",
      "NOTE: Epoch i=275, ae_loss=  0.0137.\n",
      "NOTE: Epoch i=276, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=277, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=278, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=279, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=280, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=281, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=282, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=283, ae_loss=  0.0122.\n",
      "NOTE: Epoch i=284, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=285, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=286, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=287, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=288, ae_loss=  0.0122.\n",
      "NOTE: Epoch i=289, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=290, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=291, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=292, ae_loss=  0.0125.\n",
      "NOTE: Epoch i=293, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=294, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=295, ae_loss=  0.0137.\n",
      "NOTE: Epoch i=296, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=297, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=298, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=299, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=300, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=301, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=302, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=303, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=304, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=305, ae_loss=  0.0140.\n",
      "NOTE: Epoch i=306, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=307, ae_loss=  0.0134.\n",
      "NOTE: Epoch i=308, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=309, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=310, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=311, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=312, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=313, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=314, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=315, ae_loss=  0.0121.\n",
      "NOTE: Epoch i=316, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=317, ae_loss=  0.0134.\n",
      "NOTE: Epoch i=318, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=319, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=320, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=321, ae_loss=  0.0139.\n",
      "NOTE: Epoch i=322, ae_loss=  0.0124.\n",
      "NOTE: Epoch i=323, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=324, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=325, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=326, ae_loss=  0.0123.\n",
      "NOTE: Epoch i=327, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=328, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=329, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=330, ae_loss=  0.0124.\n",
      "NOTE: Epoch i=331, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=332, ae_loss=  0.0126.\n",
      "NOTE: Epoch i=333, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=334, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=335, ae_loss=  0.0140.\n",
      "NOTE: Epoch i=336, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=337, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=338, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=339, ae_loss=  0.0124.\n",
      "NOTE: Epoch i=340, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=341, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=342, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=343, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=344, ae_loss=  0.0122.\n",
      "NOTE: Epoch i=345, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=346, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=347, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=348, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=349, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=350, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=351, ae_loss=  0.0122.\n",
      "NOTE: Epoch i=352, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=353, ae_loss=  0.0126.\n",
      "NOTE: Epoch i=354, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=355, ae_loss=  0.0134.\n",
      "NOTE: Epoch i=356, ae_loss=  0.0134.\n",
      "NOTE: Epoch i=357, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=358, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=359, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=360, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=361, ae_loss=  0.0124.\n",
      "NOTE: Epoch i=362, ae_loss=  0.0137.\n",
      "NOTE: Epoch i=363, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=364, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=365, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=366, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=367, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=368, ae_loss=  0.0138.\n",
      "NOTE: Epoch i=369, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=370, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=371, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=372, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=373, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=374, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=375, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=376, ae_loss=  0.0119.\n",
      "NOTE: Epoch i=377, ae_loss=  0.0134.\n",
      "NOTE: Epoch i=378, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=379, ae_loss=  0.0134.\n",
      "NOTE: Epoch i=380, ae_loss=  0.0126.\n",
      "NOTE: Epoch i=381, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=382, ae_loss=  0.0119.\n",
      "NOTE: Epoch i=383, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=384, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=385, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=386, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=387, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=388, ae_loss=  0.0140.\n",
      "NOTE: Epoch i=389, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=390, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=391, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=392, ae_loss=  0.0126.\n",
      "NOTE: Epoch i=393, ae_loss=  0.0125.\n",
      "NOTE: Epoch i=394, ae_loss=  0.0124.\n",
      "NOTE: Epoch i=395, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=396, ae_loss=  0.0124.\n",
      "NOTE: Epoch i=397, ae_loss=  0.0124.\n",
      "NOTE: Epoch i=398, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=399, ae_loss=  0.0125.\n",
      "NOTE: Epoch i=400, ae_loss=  0.0125.\n",
      "NOTE: Epoch i=401, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=402, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=403, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=404, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=405, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=406, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=407, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=408, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=409, ae_loss=  0.0124.\n",
      "NOTE: Epoch i=410, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=411, ae_loss=  0.0125.\n",
      "NOTE: Epoch i=412, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=413, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=414, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=415, ae_loss=  0.0122.\n",
      "NOTE: Epoch i=416, ae_loss=  0.0122.\n",
      "NOTE: Epoch i=417, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=418, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=419, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=420, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=421, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=422, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=423, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=424, ae_loss=  0.0123.\n",
      "NOTE: Epoch i=425, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=426, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=427, ae_loss=  0.0124.\n",
      "NOTE: Epoch i=428, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=429, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=430, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=431, ae_loss=  0.0126.\n",
      "NOTE: Epoch i=432, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=433, ae_loss=  0.0122.\n",
      "NOTE: Epoch i=434, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=435, ae_loss=  0.0134.\n",
      "NOTE: Epoch i=436, ae_loss=  0.0124.\n",
      "NOTE: Epoch i=437, ae_loss=  0.0123.\n",
      "NOTE: Epoch i=438, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=439, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=440, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=441, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=442, ae_loss=  0.0124.\n",
      "NOTE: Epoch i=443, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=444, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=445, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=446, ae_loss=  0.0125.\n",
      "NOTE: Epoch i=447, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=448, ae_loss=  0.0120.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=449, ae_loss=  0.0121.\n",
      "NOTE: Epoch i=450, ae_loss=  0.0118.\n",
      "NOTE: Epoch i=451, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=452, ae_loss=  0.0119.\n",
      "NOTE: Epoch i=453, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=454, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=455, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=456, ae_loss=  0.0124.\n",
      "NOTE: Epoch i=457, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=458, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=459, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=460, ae_loss=  0.0122.\n",
      "NOTE: Epoch i=461, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=462, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=463, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=464, ae_loss=  0.0124.\n",
      "NOTE: Epoch i=465, ae_loss=  0.0126.\n",
      "NOTE: Epoch i=466, ae_loss=  0.0126.\n",
      "NOTE: Epoch i=467, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=468, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=469, ae_loss=  0.0119.\n",
      "NOTE: Epoch i=470, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=471, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=472, ae_loss=  0.0120.\n",
      "NOTE: Epoch i=473, ae_loss=  0.0135.\n",
      "NOTE: Epoch i=474, ae_loss=  0.0134.\n",
      "NOTE: Epoch i=475, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=476, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=477, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=478, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=479, ae_loss=  0.0126.\n",
      "NOTE: Epoch i=480, ae_loss=  0.0132.\n",
      "NOTE: Epoch i=481, ae_loss=  0.0122.\n",
      "NOTE: Epoch i=482, ae_loss=  0.0126.\n",
      "NOTE: Epoch i=483, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=484, ae_loss=  0.0119.\n",
      "NOTE: Epoch i=485, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=486, ae_loss=  0.0120.\n",
      "NOTE: Epoch i=487, ae_loss=  0.0130.\n",
      "NOTE: Epoch i=488, ae_loss=  0.0131.\n",
      "NOTE: Epoch i=489, ae_loss=  0.0136.\n",
      "NOTE: Epoch i=490, ae_loss=  0.0126.\n",
      "NOTE: Epoch i=491, ae_loss=  0.0124.\n",
      "NOTE: Epoch i=492, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=493, ae_loss=  0.0119.\n",
      "NOTE: Epoch i=494, ae_loss=  0.0125.\n",
      "NOTE: Epoch i=495, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=496, ae_loss=  0.0129.\n",
      "NOTE: Epoch i=497, ae_loss=  0.0128.\n",
      "NOTE: Epoch i=498, ae_loss=  0.0133.\n",
      "NOTE: Epoch i=499, ae_loss=  0.0125.\n",
      "NOTE: Epoch i=500, ae_loss=  0.0127.\n",
      "NOTE: Epoch i=1, g_loss=  0.5478, d_loss= -0.0066.\n",
      "NOTE: Epoch i=2, g_loss=  0.9375, d_loss= -0.1021.\n",
      "NOTE: Epoch i=3, g_loss=  1.1767, d_loss= -0.1543.\n",
      "NOTE: Epoch i=4, g_loss=  1.2148, d_loss= -0.2041.\n",
      "NOTE: Epoch i=5, g_loss=  1.2356, d_loss= -0.3011.\n",
      "NOTE: Epoch i=6, g_loss=  1.2214, d_loss= -0.3535.\n",
      "NOTE: Epoch i=7, g_loss=  1.3591, d_loss= -0.4668.\n",
      "NOTE: Epoch i=8, g_loss=  1.4514, d_loss= -0.4344.\n",
      "NOTE: Epoch i=9, g_loss=  1.5795, d_loss= -0.4105.\n",
      "NOTE: Epoch i=10, g_loss=  1.6752, d_loss= -0.3001.\n",
      "NOTE: Epoch i=11, g_loss=  1.4276, d_loss= -0.6392.\n",
      "NOTE: Epoch i=12, g_loss=  1.6710, d_loss= -0.4579.\n",
      "NOTE: Epoch i=13, g_loss=  1.5627, d_loss= -0.5311.\n",
      "NOTE: Epoch i=14, g_loss=  1.8487, d_loss= -0.3494.\n",
      "NOTE: Epoch i=15, g_loss=  1.8473, d_loss= -0.3482.\n",
      "NOTE: Epoch i=16, g_loss=  1.5620, d_loss= -0.3749.\n",
      "NOTE: Epoch i=17, g_loss=  1.6233, d_loss= -0.3192.\n",
      "NOTE: Epoch i=18, g_loss=  1.8053, d_loss= -0.3597.\n",
      "NOTE: Epoch i=19, g_loss=  1.7866, d_loss= -0.3357.\n",
      "NOTE: Epoch i=20, g_loss=  1.8607, d_loss= -0.4647.\n",
      "NOTE: Epoch i=21, g_loss=  1.8421, d_loss= -0.4430.\n",
      "NOTE: Epoch i=22, g_loss=  1.8884, d_loss= -0.0925.\n",
      "NOTE: Epoch i=23, g_loss=  2.0339, d_loss= -0.2964.\n",
      "NOTE: Epoch i=24, g_loss=  1.6920, d_loss= -0.1130.\n",
      "NOTE: Epoch i=25, g_loss=  1.7546, d_loss= -0.0441.\n",
      "NOTE: Epoch i=26, g_loss=  0.9350, d_loss=  0.3040.\n",
      "NOTE: Epoch i=27, g_loss=  0.0199, d_loss=  0.1752.\n",
      "NOTE: Epoch i=28, g_loss= -1.1207, d_loss=  0.2956.\n",
      "NOTE: Epoch i=29, g_loss= -1.9772, d_loss=  0.0263.\n",
      "NOTE: Epoch i=30, g_loss= -2.7963, d_loss= -0.1471.\n",
      "NOTE: Epoch i=31, g_loss= -3.3883, d_loss=  0.2542.\n",
      "NOTE: Epoch i=32, g_loss= -3.9145, d_loss= -0.0474.\n",
      "NOTE: Epoch i=33, g_loss= -4.2211, d_loss=  0.0331.\n",
      "NOTE: Epoch i=34, g_loss= -4.6293, d_loss=  0.2155.\n",
      "NOTE: Epoch i=35, g_loss= -4.8332, d_loss=  0.3547.\n",
      "NOTE: Epoch i=36, g_loss= -5.1444, d_loss= -0.0553.\n",
      "NOTE: Epoch i=37, g_loss= -5.2804, d_loss= -0.1185.\n",
      "NOTE: Epoch i=38, g_loss= -5.4113, d_loss=  0.2762.\n",
      "NOTE: Epoch i=39, g_loss= -5.2879, d_loss= -0.0078.\n",
      "NOTE: Epoch i=40, g_loss= -5.2546, d_loss= -0.0020.\n",
      "NOTE: Epoch i=41, g_loss= -5.4454, d_loss= -0.0423.\n",
      "NOTE: Epoch i=42, g_loss= -5.1287, d_loss=  0.0751.\n",
      "NOTE: Epoch i=43, g_loss= -5.1982, d_loss= -0.2088.\n",
      "NOTE: Epoch i=44, g_loss= -5.4565, d_loss=  0.2934.\n",
      "NOTE: Epoch i=45, g_loss= -5.3592, d_loss=  0.1348.\n",
      "NOTE: Epoch i=46, g_loss= -5.3176, d_loss=  0.2148.\n",
      "NOTE: Epoch i=47, g_loss= -5.1332, d_loss= -0.2534.\n",
      "NOTE: Epoch i=48, g_loss= -5.0519, d_loss=  0.0149.\n",
      "NOTE: Epoch i=49, g_loss= -4.8332, d_loss=  0.1636.\n",
      "NOTE: Epoch i=50, g_loss= -4.6106, d_loss= -0.0088.\n",
      "NOTE: Epoch i=51, g_loss= -4.5544, d_loss=  0.0374.\n",
      "NOTE: Epoch i=52, g_loss= -4.3214, d_loss=  0.0294.\n",
      "NOTE: Epoch i=53, g_loss= -4.1784, d_loss=  0.1692.\n",
      "NOTE: Epoch i=54, g_loss= -4.1563, d_loss= -0.1333.\n",
      "NOTE: Epoch i=55, g_loss= -4.3323, d_loss= -0.1390.\n",
      "NOTE: Epoch i=56, g_loss= -4.2582, d_loss=  0.0822.\n",
      "NOTE: Epoch i=57, g_loss= -4.2232, d_loss=  0.0681.\n",
      "NOTE: Epoch i=58, g_loss= -4.4844, d_loss=  0.1501.\n",
      "NOTE: Epoch i=59, g_loss= -4.4532, d_loss=  0.1477.\n",
      "NOTE: Epoch i=60, g_loss= -4.7531, d_loss=  0.1079.\n",
      "NOTE: Epoch i=61, g_loss= -5.0215, d_loss=  0.2607.\n",
      "NOTE: Epoch i=62, g_loss= -5.1611, d_loss=  0.1489.\n",
      "NOTE: Epoch i=63, g_loss= -5.6132, d_loss=  0.4270.\n",
      "NOTE: Epoch i=64, g_loss= -6.0574, d_loss= -0.0690.\n",
      "NOTE: Epoch i=65, g_loss= -6.3372, d_loss=  0.2697.\n",
      "NOTE: Epoch i=66, g_loss= -6.4282, d_loss=  0.2012.\n",
      "NOTE: Epoch i=67, g_loss= -6.2029, d_loss= -0.0279.\n",
      "NOTE: Epoch i=68, g_loss= -6.0924, d_loss=  0.2271.\n",
      "NOTE: Epoch i=69, g_loss= -6.2418, d_loss=  0.1964.\n",
      "NOTE: Epoch i=70, g_loss= -6.5605, d_loss=  0.3050.\n",
      "NOTE: Epoch i=71, g_loss= -6.2512, d_loss= -0.1057.\n",
      "NOTE: Epoch i=72, g_loss= -6.0484, d_loss=  0.0469.\n",
      "NOTE: Epoch i=73, g_loss= -6.1413, d_loss= -0.1425.\n",
      "NOTE: Epoch i=74, g_loss= -5.7510, d_loss=  0.1152.\n",
      "NOTE: Epoch i=75, g_loss= -5.8711, d_loss=  0.1056.\n",
      "NOTE: Epoch i=76, g_loss= -5.7449, d_loss=  0.0260.\n",
      "NOTE: Epoch i=77, g_loss= -5.5414, d_loss= -0.0348.\n",
      "NOTE: Epoch i=78, g_loss= -5.5216, d_loss=  0.0926.\n",
      "NOTE: Epoch i=79, g_loss= -5.4564, d_loss=  0.2188.\n",
      "NOTE: Epoch i=80, g_loss= -5.1369, d_loss= -0.0988.\n",
      "NOTE: Epoch i=81, g_loss= -5.5126, d_loss= -0.1763.\n",
      "NOTE: Epoch i=82, g_loss= -5.1884, d_loss= -0.2135.\n",
      "NOTE: Epoch i=83, g_loss= -5.3693, d_loss= -0.0821.\n",
      "NOTE: Epoch i=84, g_loss= -5.8547, d_loss=  0.0308.\n",
      "NOTE: Epoch i=85, g_loss= -5.6998, d_loss=  0.2472.\n",
      "NOTE: Epoch i=86, g_loss= -5.9770, d_loss=  0.1658.\n",
      "NOTE: Epoch i=87, g_loss= -6.3024, d_loss= -0.1415.\n",
      "NOTE: Epoch i=88, g_loss= -6.2650, d_loss=  0.0219.\n",
      "NOTE: Epoch i=89, g_loss= -6.3416, d_loss=  0.2402.\n",
      "NOTE: Epoch i=90, g_loss= -6.2797, d_loss= -0.1260.\n",
      "NOTE: Epoch i=91, g_loss= -6.5714, d_loss= -0.2206.\n",
      "NOTE: Epoch i=92, g_loss= -6.3958, d_loss=  0.0624.\n",
      "NOTE: Epoch i=93, g_loss= -6.2543, d_loss=  0.0237.\n",
      "NOTE: Epoch i=94, g_loss= -6.2853, d_loss=  0.1443.\n",
      "NOTE: Epoch i=95, g_loss= -6.2469, d_loss= -0.0682.\n",
      "NOTE: Epoch i=96, g_loss= -6.0877, d_loss=  0.3174.\n",
      "NOTE: Epoch i=97, g_loss= -5.9377, d_loss=  0.3038.\n",
      "NOTE: Epoch i=98, g_loss= -6.1778, d_loss=  0.0059.\n",
      "NOTE: Epoch i=99, g_loss= -5.9043, d_loss=  0.0375.\n",
      "NOTE: Epoch i=100, g_loss= -5.9575, d_loss= -0.0411.\n",
      "NOTE: Epoch i=101, g_loss= -5.8666, d_loss=  0.2455.\n",
      "NOTE: Epoch i=102, g_loss= -5.7883, d_loss=  0.0817.\n",
      "NOTE: Epoch i=103, g_loss= -6.0267, d_loss=  0.0553.\n",
      "NOTE: Epoch i=104, g_loss= -6.0406, d_loss= -0.3715.\n",
      "NOTE: Epoch i=105, g_loss= -6.2582, d_loss= -0.0306.\n",
      "NOTE: Epoch i=106, g_loss= -6.5736, d_loss= -0.1813.\n",
      "NOTE: Epoch i=107, g_loss= -6.6054, d_loss= -0.1031.\n",
      "NOTE: Epoch i=108, g_loss= -6.4282, d_loss= -0.0937.\n",
      "NOTE: Epoch i=109, g_loss= -6.7300, d_loss= -0.0723.\n",
      "NOTE: Epoch i=110, g_loss= -6.5987, d_loss=  0.1743.\n",
      "NOTE: Epoch i=111, g_loss= -6.5948, d_loss=  0.0742.\n",
      "NOTE: Epoch i=112, g_loss= -6.7210, d_loss= -0.3425.\n",
      "NOTE: Epoch i=113, g_loss= -6.6409, d_loss= -0.1124.\n",
      "NOTE: Epoch i=114, g_loss= -6.5625, d_loss=  0.1655.\n",
      "NOTE: Epoch i=115, g_loss= -6.7600, d_loss= -0.2888.\n",
      "NOTE: Epoch i=116, g_loss= -6.4950, d_loss= -0.0097.\n",
      "NOTE: Epoch i=117, g_loss= -6.5176, d_loss=  0.0705.\n",
      "NOTE: Epoch i=118, g_loss= -6.2259, d_loss= -0.0494.\n",
      "NOTE: Epoch i=119, g_loss= -6.5981, d_loss=  0.0105.\n",
      "NOTE: Epoch i=120, g_loss= -6.8692, d_loss= -0.1285.\n",
      "NOTE: Epoch i=121, g_loss= -6.9103, d_loss= -0.0010.\n",
      "NOTE: Epoch i=122, g_loss= -6.9779, d_loss= -0.3459.\n",
      "NOTE: Epoch i=123, g_loss= -7.1978, d_loss=  0.0591.\n",
      "NOTE: Epoch i=124, g_loss= -7.1905, d_loss=  0.1281.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=125, g_loss= -7.2427, d_loss=  0.1543.\n",
      "NOTE: Epoch i=126, g_loss= -7.1909, d_loss= -0.1987.\n",
      "NOTE: Epoch i=127, g_loss= -7.1065, d_loss=  0.3087.\n",
      "NOTE: Epoch i=128, g_loss= -7.1645, d_loss= -0.0488.\n",
      "NOTE: Epoch i=129, g_loss= -6.8826, d_loss= -0.1139.\n",
      "NOTE: Epoch i=130, g_loss= -6.9007, d_loss= -0.2998.\n",
      "NOTE: Epoch i=131, g_loss= -7.2684, d_loss=  0.2414.\n",
      "NOTE: Epoch i=132, g_loss= -6.7873, d_loss=  0.0993.\n",
      "NOTE: Epoch i=133, g_loss= -6.7639, d_loss= -0.6805.\n",
      "NOTE: Epoch i=134, g_loss= -6.5789, d_loss= -0.0896.\n",
      "NOTE: Epoch i=135, g_loss= -6.6832, d_loss=  0.2948.\n",
      "NOTE: Epoch i=136, g_loss= -6.5546, d_loss=  0.1349.\n",
      "NOTE: Epoch i=137, g_loss= -6.4751, d_loss= -0.0637.\n",
      "NOTE: Epoch i=138, g_loss= -6.6585, d_loss= -0.0310.\n",
      "NOTE: Epoch i=139, g_loss= -6.4088, d_loss=  0.1335.\n",
      "NOTE: Epoch i=140, g_loss= -6.0700, d_loss= -0.0367.\n",
      "NOTE: Epoch i=141, g_loss= -6.4670, d_loss= -0.0906.\n",
      "NOTE: Epoch i=142, g_loss= -6.4519, d_loss=  0.2411.\n",
      "NOTE: Epoch i=143, g_loss= -6.3458, d_loss=  0.2268.\n",
      "NOTE: Epoch i=144, g_loss= -6.4566, d_loss= -0.0701.\n",
      "NOTE: Epoch i=145, g_loss= -6.7389, d_loss= -0.0266.\n",
      "NOTE: Epoch i=146, g_loss= -6.8626, d_loss= -0.3805.\n",
      "NOTE: Epoch i=147, g_loss= -6.9432, d_loss=  0.0357.\n",
      "NOTE: Epoch i=148, g_loss= -6.6708, d_loss= -0.2665.\n",
      "NOTE: Epoch i=149, g_loss= -6.8302, d_loss= -0.0710.\n",
      "NOTE: Epoch i=150, g_loss= -6.8800, d_loss= -0.2045.\n",
      "NOTE: Epoch i=151, g_loss= -6.9985, d_loss=  0.2696.\n",
      "NOTE: Epoch i=152, g_loss= -6.6983, d_loss=  0.0346.\n",
      "NOTE: Epoch i=153, g_loss= -6.7065, d_loss= -0.5120.\n",
      "NOTE: Epoch i=154, g_loss= -6.7940, d_loss=  0.1432.\n",
      "NOTE: Epoch i=155, g_loss= -6.5575, d_loss= -0.0741.\n",
      "NOTE: Epoch i=156, g_loss= -6.5560, d_loss=  0.1708.\n",
      "NOTE: Epoch i=157, g_loss= -6.8099, d_loss= -0.2949.\n",
      "NOTE: Epoch i=158, g_loss= -6.4097, d_loss=  0.0541.\n",
      "NOTE: Epoch i=159, g_loss= -6.7161, d_loss= -0.2586.\n",
      "NOTE: Epoch i=160, g_loss= -7.1693, d_loss= -0.2775.\n",
      "NOTE: Epoch i=161, g_loss= -6.9372, d_loss= -0.0439.\n",
      "NOTE: Epoch i=162, g_loss= -6.9979, d_loss=  0.1361.\n",
      "NOTE: Epoch i=163, g_loss= -6.8070, d_loss= -0.1453.\n",
      "NOTE: Epoch i=164, g_loss= -7.1416, d_loss= -0.4478.\n",
      "NOTE: Epoch i=165, g_loss= -6.7061, d_loss= -0.0458.\n",
      "NOTE: Epoch i=166, g_loss= -6.7065, d_loss= -0.2667.\n",
      "NOTE: Epoch i=167, g_loss= -6.9610, d_loss= -0.2169.\n",
      "NOTE: Epoch i=168, g_loss= -6.7511, d_loss= -0.0369.\n",
      "NOTE: Epoch i=169, g_loss= -6.8988, d_loss=  0.1915.\n",
      "NOTE: Epoch i=170, g_loss= -7.1674, d_loss=  0.0623.\n",
      "NOTE: Epoch i=171, g_loss= -6.7867, d_loss= -0.2482.\n",
      "NOTE: Epoch i=172, g_loss= -6.7693, d_loss=  0.3356.\n",
      "NOTE: Epoch i=173, g_loss= -6.7931, d_loss=  0.2426.\n",
      "NOTE: Epoch i=174, g_loss= -6.8925, d_loss= -0.4211.\n",
      "NOTE: Epoch i=175, g_loss= -6.7197, d_loss= -0.0395.\n",
      "NOTE: Epoch i=176, g_loss= -6.6410, d_loss= -0.0757.\n",
      "NOTE: Epoch i=177, g_loss= -6.8348, d_loss= -0.2856.\n",
      "NOTE: Epoch i=178, g_loss= -6.7376, d_loss= -0.5379.\n",
      "NOTE: Epoch i=179, g_loss= -6.9984, d_loss=  0.0334.\n",
      "NOTE: Epoch i=180, g_loss= -6.9888, d_loss=  0.0769.\n",
      "NOTE: Epoch i=181, g_loss= -7.0741, d_loss= -0.1603.\n",
      "NOTE: Epoch i=182, g_loss= -7.2548, d_loss=  0.0262.\n",
      "NOTE: Epoch i=183, g_loss= -7.7227, d_loss=  0.0314.\n",
      "NOTE: Epoch i=184, g_loss= -7.2963, d_loss=  0.1546.\n",
      "NOTE: Epoch i=185, g_loss= -7.5978, d_loss= -0.2768.\n",
      "NOTE: Epoch i=186, g_loss= -7.4495, d_loss=  0.1322.\n",
      "NOTE: Epoch i=187, g_loss= -7.3951, d_loss=  0.1999.\n",
      "NOTE: Epoch i=188, g_loss= -7.3246, d_loss= -0.0668.\n",
      "NOTE: Epoch i=189, g_loss= -7.4843, d_loss= -0.1697.\n",
      "NOTE: Epoch i=190, g_loss= -7.1239, d_loss=  0.2523.\n",
      "NOTE: Epoch i=191, g_loss= -7.2908, d_loss= -0.0418.\n",
      "NOTE: Epoch i=192, g_loss= -7.1663, d_loss=  0.0356.\n",
      "NOTE: Epoch i=193, g_loss= -7.3036, d_loss=  0.0623.\n",
      "NOTE: Epoch i=194, g_loss= -7.5225, d_loss= -0.0810.\n",
      "NOTE: Epoch i=195, g_loss= -7.2192, d_loss= -0.1703.\n",
      "NOTE: Epoch i=196, g_loss= -7.5749, d_loss= -0.4334.\n",
      "NOTE: Epoch i=197, g_loss= -7.6577, d_loss=  0.1527.\n",
      "NOTE: Epoch i=198, g_loss= -7.2578, d_loss= -0.3564.\n",
      "NOTE: Epoch i=199, g_loss= -7.5671, d_loss=  0.1070.\n",
      "NOTE: Epoch i=200, g_loss= -7.4326, d_loss= -0.1524.\n",
      "NOTE: Epoch i=201, g_loss= -6.9686, d_loss= -0.2306.\n",
      "NOTE: Epoch i=202, g_loss= -7.4736, d_loss=  0.1152.\n",
      "NOTE: Epoch i=203, g_loss= -7.2624, d_loss=  0.3752.\n",
      "NOTE: Epoch i=204, g_loss= -7.2183, d_loss= -0.0764.\n",
      "NOTE: Epoch i=205, g_loss= -7.3957, d_loss= -0.1206.\n",
      "NOTE: Epoch i=206, g_loss= -7.5016, d_loss= -0.2217.\n",
      "NOTE: Epoch i=207, g_loss= -7.0015, d_loss= -0.4874.\n",
      "NOTE: Epoch i=208, g_loss= -7.0590, d_loss= -0.3386.\n",
      "NOTE: Epoch i=209, g_loss= -7.1042, d_loss= -0.3468.\n",
      "NOTE: Epoch i=210, g_loss= -6.6195, d_loss=  0.3100.\n",
      "NOTE: Epoch i=211, g_loss= -7.2188, d_loss= -0.5643.\n",
      "NOTE: Epoch i=212, g_loss= -6.8057, d_loss=  0.0490.\n",
      "NOTE: Epoch i=213, g_loss= -6.9444, d_loss= -0.3303.\n",
      "NOTE: Epoch i=214, g_loss= -7.1746, d_loss= -0.4193.\n",
      "NOTE: Epoch i=215, g_loss= -6.4811, d_loss=  0.0656.\n",
      "NOTE: Epoch i=216, g_loss= -6.9786, d_loss=  0.1674.\n",
      "NOTE: Epoch i=217, g_loss= -6.5387, d_loss= -0.3758.\n",
      "NOTE: Epoch i=218, g_loss= -6.6055, d_loss= -0.2056.\n",
      "NOTE: Epoch i=219, g_loss= -6.4904, d_loss= -0.1775.\n",
      "NOTE: Epoch i=220, g_loss= -6.5653, d_loss=  0.0337.\n",
      "NOTE: Epoch i=221, g_loss= -6.2760, d_loss=  0.0561.\n",
      "NOTE: Epoch i=222, g_loss= -6.6463, d_loss=  0.1928.\n",
      "NOTE: Epoch i=223, g_loss= -6.7877, d_loss=  0.1354.\n",
      "NOTE: Epoch i=224, g_loss= -6.9623, d_loss= -0.2307.\n",
      "NOTE: Epoch i=225, g_loss= -6.9752, d_loss=  0.1069.\n",
      "NOTE: Epoch i=226, g_loss= -6.4636, d_loss=  0.3266.\n",
      "NOTE: Epoch i=227, g_loss= -6.5331, d_loss= -0.4405.\n",
      "NOTE: Epoch i=228, g_loss= -6.6526, d_loss= -0.3103.\n",
      "NOTE: Epoch i=229, g_loss= -6.7040, d_loss= -0.1677.\n",
      "NOTE: Epoch i=230, g_loss= -6.2504, d_loss=  0.0573.\n",
      "NOTE: Epoch i=231, g_loss= -6.3790, d_loss=  0.0720.\n",
      "NOTE: Epoch i=232, g_loss= -5.9716, d_loss= -0.3329.\n",
      "NOTE: Epoch i=233, g_loss= -5.8927, d_loss=  0.1613.\n",
      "NOTE: Epoch i=234, g_loss= -5.6064, d_loss=  0.4896.\n",
      "NOTE: Epoch i=235, g_loss= -5.7193, d_loss= -0.1936.\n",
      "NOTE: Epoch i=236, g_loss= -5.5577, d_loss=  0.5843.\n",
      "NOTE: Epoch i=237, g_loss= -5.5399, d_loss=  0.0713.\n",
      "NOTE: Epoch i=238, g_loss= -5.2826, d_loss=  0.2081.\n",
      "NOTE: Epoch i=239, g_loss= -5.4977, d_loss=  0.0335.\n",
      "NOTE: Epoch i=240, g_loss= -5.2935, d_loss=  0.1333.\n",
      "NOTE: Epoch i=241, g_loss= -5.4228, d_loss= -0.1817.\n",
      "NOTE: Epoch i=242, g_loss= -4.3993, d_loss= -0.0183.\n",
      "NOTE: Epoch i=243, g_loss= -4.4877, d_loss=  0.4497.\n",
      "NOTE: Epoch i=244, g_loss= -4.4463, d_loss= -0.4334.\n",
      "NOTE: Epoch i=245, g_loss= -4.1562, d_loss=  0.1252.\n",
      "NOTE: Epoch i=246, g_loss= -3.8182, d_loss= -0.1421.\n",
      "NOTE: Epoch i=247, g_loss= -4.4671, d_loss= -0.0243.\n",
      "NOTE: Epoch i=248, g_loss= -4.3201, d_loss=  0.0933.\n",
      "NOTE: Epoch i=249, g_loss= -3.7421, d_loss=  0.2206.\n",
      "NOTE: Epoch i=250, g_loss= -3.5928, d_loss= -0.6482.\n",
      "NOTE: Epoch i=251, g_loss= -3.4300, d_loss=  0.0503.\n",
      "NOTE: Epoch i=252, g_loss= -3.6345, d_loss= -0.2225.\n",
      "NOTE: Epoch i=253, g_loss= -3.7832, d_loss=  0.3384.\n",
      "NOTE: Epoch i=254, g_loss= -3.3792, d_loss=  0.1839.\n",
      "NOTE: Epoch i=255, g_loss= -3.4698, d_loss= -0.0647.\n",
      "NOTE: Epoch i=256, g_loss= -3.5339, d_loss=  0.3019.\n",
      "NOTE: Epoch i=257, g_loss= -3.4812, d_loss=  0.4500.\n",
      "NOTE: Epoch i=258, g_loss= -3.2246, d_loss=  0.2361.\n",
      "NOTE: Epoch i=259, g_loss= -3.5253, d_loss= -0.6200.\n",
      "NOTE: Epoch i=260, g_loss= -3.3273, d_loss= -0.0494.\n",
      "NOTE: Epoch i=261, g_loss= -3.7220, d_loss=  0.2801.\n",
      "NOTE: Epoch i=262, g_loss= -3.3645, d_loss= -0.6065.\n",
      "NOTE: Epoch i=263, g_loss= -3.2553, d_loss= -0.3665.\n",
      "NOTE: Epoch i=264, g_loss= -3.0808, d_loss= -0.2923.\n",
      "NOTE: Epoch i=265, g_loss= -3.1108, d_loss=  0.3729.\n",
      "NOTE: Epoch i=266, g_loss= -3.3278, d_loss=  0.3883.\n",
      "NOTE: Epoch i=267, g_loss= -2.8830, d_loss=  0.2739.\n",
      "NOTE: Epoch i=268, g_loss= -2.7883, d_loss=  0.1836.\n",
      "NOTE: Epoch i=269, g_loss= -3.2275, d_loss= -0.6383.\n",
      "NOTE: Epoch i=270, g_loss= -2.7815, d_loss= -0.6009.\n",
      "NOTE: Epoch i=271, g_loss= -2.3051, d_loss= -0.1834.\n",
      "NOTE: Epoch i=272, g_loss= -2.2501, d_loss=  0.2135.\n",
      "NOTE: Epoch i=273, g_loss= -1.8378, d_loss= -0.1165.\n",
      "NOTE: Epoch i=274, g_loss= -2.1170, d_loss=  0.1251.\n",
      "NOTE: Epoch i=275, g_loss= -2.0687, d_loss=  0.9019.\n",
      "NOTE: Epoch i=276, g_loss= -1.4910, d_loss= -0.5030.\n",
      "NOTE: Epoch i=277, g_loss= -1.8774, d_loss=  0.4216.\n",
      "NOTE: Epoch i=278, g_loss= -2.2372, d_loss= -0.4312.\n",
      "NOTE: Epoch i=279, g_loss= -1.2846, d_loss= -0.4372.\n",
      "NOTE: Epoch i=280, g_loss= -1.7633, d_loss= -0.0590.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=281, g_loss= -1.8550, d_loss= -0.8134.\n",
      "NOTE: Epoch i=282, g_loss= -1.9044, d_loss= -0.5647.\n",
      "NOTE: Epoch i=283, g_loss= -1.8367, d_loss=  0.3780.\n",
      "NOTE: Epoch i=284, g_loss= -1.9489, d_loss=  0.0779.\n",
      "NOTE: Epoch i=285, g_loss= -1.5419, d_loss=  0.3410.\n",
      "NOTE: Epoch i=286, g_loss= -1.7877, d_loss= -0.5972.\n",
      "NOTE: Epoch i=287, g_loss= -1.2894, d_loss=  0.6042.\n",
      "NOTE: Epoch i=288, g_loss= -1.8879, d_loss= -0.2084.\n",
      "NOTE: Epoch i=289, g_loss= -1.4370, d_loss= -0.0347.\n",
      "NOTE: Epoch i=290, g_loss= -1.3217, d_loss=  0.2681.\n",
      "NOTE: Epoch i=291, g_loss= -1.0621, d_loss= -0.5695.\n",
      "NOTE: Epoch i=292, g_loss= -1.0930, d_loss= -0.6217.\n",
      "NOTE: Epoch i=293, g_loss= -0.7692, d_loss= -0.1205.\n",
      "NOTE: Epoch i=294, g_loss= -0.7933, d_loss= -0.6144.\n",
      "NOTE: Epoch i=295, g_loss= -0.8296, d_loss=  0.0489.\n",
      "NOTE: Epoch i=296, g_loss= -0.6912, d_loss=  0.4603.\n",
      "NOTE: Epoch i=297, g_loss= -0.4174, d_loss=  0.6039.\n",
      "NOTE: Epoch i=298, g_loss= -0.8419, d_loss= -0.4798.\n",
      "NOTE: Epoch i=299, g_loss= -1.0092, d_loss= -0.2923.\n",
      "NOTE: Epoch i=300, g_loss= -0.4631, d_loss=  0.5998.\n",
      "NOTE: Epoch i=301, g_loss= -0.5872, d_loss=  0.3013.\n",
      "NOTE: Epoch i=302, g_loss= -0.1690, d_loss=  0.1099.\n",
      "NOTE: Epoch i=303, g_loss= -0.2982, d_loss=  0.0489.\n",
      "NOTE: Epoch i=304, g_loss= -0.1364, d_loss= -0.2146.\n",
      "NOTE: Epoch i=305, g_loss=  0.1002, d_loss=  0.3867.\n",
      "NOTE: Epoch i=306, g_loss= -0.0835, d_loss= -0.6842.\n",
      "NOTE: Epoch i=307, g_loss=  0.0724, d_loss=  0.0311.\n",
      "NOTE: Epoch i=308, g_loss= -0.5069, d_loss= -0.1793.\n",
      "NOTE: Epoch i=309, g_loss=  0.4453, d_loss=  0.4687.\n",
      "NOTE: Epoch i=310, g_loss=  0.1044, d_loss= -0.0108.\n",
      "NOTE: Epoch i=311, g_loss=  0.1626, d_loss= -0.3061.\n",
      "NOTE: Epoch i=312, g_loss=  0.5079, d_loss= -1.0062.\n",
      "NOTE: Epoch i=313, g_loss=  0.4734, d_loss= -0.6701.\n",
      "NOTE: Epoch i=314, g_loss=  0.6077, d_loss= -0.3496.\n",
      "NOTE: Epoch i=315, g_loss=  0.6415, d_loss=  0.7739.\n",
      "NOTE: Epoch i=316, g_loss=  0.0638, d_loss= -0.2167.\n",
      "NOTE: Epoch i=317, g_loss=  0.2575, d_loss= -0.1485.\n",
      "NOTE: Epoch i=318, g_loss=  0.9552, d_loss= -0.5874.\n",
      "NOTE: Epoch i=319, g_loss=  1.0467, d_loss= -0.4137.\n",
      "NOTE: Epoch i=320, g_loss=  0.3386, d_loss=  0.1427.\n",
      "NOTE: Epoch i=321, g_loss=  0.9363, d_loss= -0.5242.\n",
      "NOTE: Epoch i=322, g_loss=  0.6904, d_loss=  0.2190.\n",
      "NOTE: Epoch i=323, g_loss= -0.1432, d_loss=  0.0790.\n",
      "NOTE: Epoch i=324, g_loss=  1.3628, d_loss=  0.2707.\n",
      "NOTE: Epoch i=325, g_loss=  1.1413, d_loss=  0.8888.\n",
      "NOTE: Epoch i=326, g_loss=  1.2528, d_loss=  0.0964.\n",
      "NOTE: Epoch i=327, g_loss=  0.7868, d_loss= -0.2086.\n",
      "NOTE: Epoch i=328, g_loss=  0.8269, d_loss= -0.4083.\n",
      "NOTE: Epoch i=329, g_loss=  1.3378, d_loss= -0.4261.\n",
      "NOTE: Epoch i=330, g_loss=  0.6252, d_loss= -1.0000.\n",
      "NOTE: Epoch i=331, g_loss=  1.0384, d_loss=  0.5766.\n",
      "NOTE: Epoch i=332, g_loss=  0.4990, d_loss=  0.3741.\n",
      "NOTE: Epoch i=333, g_loss=  0.7144, d_loss=  0.3266.\n",
      "NOTE: Epoch i=334, g_loss=  0.6375, d_loss= -0.1684.\n",
      "NOTE: Epoch i=335, g_loss=  0.6935, d_loss= -0.5334.\n",
      "NOTE: Epoch i=336, g_loss=  0.3766, d_loss= -0.3869.\n",
      "NOTE: Epoch i=337, g_loss=  1.1118, d_loss= -0.1458.\n",
      "NOTE: Epoch i=338, g_loss=  0.8203, d_loss= -0.4521.\n",
      "NOTE: Epoch i=339, g_loss=  0.5864, d_loss= -0.6991.\n",
      "NOTE: Epoch i=340, g_loss=  0.8536, d_loss=  0.5240.\n",
      "NOTE: Epoch i=341, g_loss=  1.1819, d_loss= -0.2062.\n",
      "NOTE: Epoch i=342, g_loss=  1.2693, d_loss=  0.2093.\n",
      "NOTE: Epoch i=343, g_loss=  1.1239, d_loss=  0.3531.\n",
      "NOTE: Epoch i=344, g_loss=  0.8718, d_loss= -0.2997.\n",
      "NOTE: Epoch i=345, g_loss=  1.3970, d_loss=  0.1855.\n",
      "NOTE: Epoch i=346, g_loss=  1.1248, d_loss= -0.1315.\n",
      "NOTE: Epoch i=347, g_loss=  0.6265, d_loss= -0.0988.\n",
      "NOTE: Epoch i=348, g_loss=  0.6207, d_loss= -0.3017.\n",
      "NOTE: Epoch i=349, g_loss=  1.0932, d_loss=  0.2479.\n",
      "NOTE: Epoch i=350, g_loss=  1.5194, d_loss= -0.3424.\n",
      "NOTE: Epoch i=351, g_loss=  0.8401, d_loss= -0.8720.\n",
      "NOTE: Epoch i=352, g_loss=  1.4068, d_loss=  0.3824.\n",
      "NOTE: Epoch i=353, g_loss=  0.8685, d_loss=  0.2865.\n",
      "NOTE: Epoch i=354, g_loss=  1.2282, d_loss= -1.0084.\n",
      "NOTE: Epoch i=355, g_loss=  0.9352, d_loss= -0.5145.\n",
      "NOTE: Epoch i=356, g_loss=  0.9266, d_loss= -0.4527.\n",
      "NOTE: Epoch i=357, g_loss=  0.9697, d_loss=  0.1914.\n",
      "NOTE: Epoch i=358, g_loss=  1.7722, d_loss= -0.2555.\n",
      "NOTE: Epoch i=359, g_loss=  1.3312, d_loss= -0.3079.\n",
      "NOTE: Epoch i=360, g_loss=  1.9555, d_loss=  0.2466.\n",
      "NOTE: Epoch i=361, g_loss=  1.9855, d_loss=  0.3271.\n",
      "NOTE: Epoch i=362, g_loss=  1.1214, d_loss=  0.3937.\n",
      "NOTE: Epoch i=363, g_loss=  1.3282, d_loss= -0.5033.\n",
      "NOTE: Epoch i=364, g_loss=  1.8259, d_loss= -0.7365.\n",
      "NOTE: Epoch i=365, g_loss=  1.4943, d_loss=  0.1374.\n",
      "NOTE: Epoch i=366, g_loss=  1.2960, d_loss= -0.5699.\n",
      "NOTE: Epoch i=367, g_loss=  1.3773, d_loss= -0.2669.\n",
      "NOTE: Epoch i=368, g_loss=  1.8800, d_loss=  0.5691.\n",
      "NOTE: Epoch i=369, g_loss=  1.7241, d_loss= -0.3068.\n",
      "NOTE: Epoch i=370, g_loss=  0.9562, d_loss= -0.1694.\n",
      "NOTE: Epoch i=371, g_loss=  1.5495, d_loss= -0.3088.\n",
      "NOTE: Epoch i=372, g_loss=  1.3767, d_loss=  0.1931.\n",
      "NOTE: Epoch i=373, g_loss=  1.4246, d_loss=  0.3274.\n",
      "NOTE: Epoch i=374, g_loss=  1.5288, d_loss= -0.2573.\n",
      "NOTE: Epoch i=375, g_loss=  0.9607, d_loss= -0.1098.\n",
      "NOTE: Epoch i=376, g_loss=  1.8452, d_loss= -0.4684.\n",
      "NOTE: Epoch i=377, g_loss=  0.8501, d_loss=  0.1778.\n",
      "NOTE: Epoch i=378, g_loss=  1.2842, d_loss= -0.5150.\n",
      "NOTE: Epoch i=379, g_loss=  1.0693, d_loss=  0.2381.\n",
      "NOTE: Epoch i=380, g_loss=  0.8647, d_loss=  0.2563.\n",
      "NOTE: Epoch i=381, g_loss=  1.9079, d_loss= -0.0694.\n",
      "NOTE: Epoch i=382, g_loss=  1.5635, d_loss= -0.1561.\n",
      "NOTE: Epoch i=383, g_loss=  1.2172, d_loss=  0.0780.\n",
      "NOTE: Epoch i=384, g_loss=  1.3835, d_loss=  0.4913.\n",
      "NOTE: Epoch i=385, g_loss=  1.5688, d_loss=  0.3005.\n",
      "NOTE: Epoch i=386, g_loss=  0.8559, d_loss=  0.5428.\n",
      "NOTE: Epoch i=387, g_loss=  1.2515, d_loss= -0.0139.\n",
      "NOTE: Epoch i=388, g_loss=  1.6238, d_loss= -0.1836.\n",
      "NOTE: Epoch i=389, g_loss=  1.4011, d_loss= -0.8795.\n",
      "NOTE: Epoch i=390, g_loss=  1.4516, d_loss=  0.2279.\n",
      "NOTE: Epoch i=391, g_loss=  1.3913, d_loss= -0.3906.\n",
      "NOTE: Epoch i=392, g_loss=  1.0902, d_loss= -0.2963.\n",
      "NOTE: Epoch i=393, g_loss=  1.1823, d_loss= -0.4013.\n",
      "NOTE: Epoch i=394, g_loss=  1.1070, d_loss= -0.2842.\n",
      "NOTE: Epoch i=395, g_loss=  0.9902, d_loss=  0.3876.\n",
      "NOTE: Epoch i=396, g_loss=  0.7740, d_loss= -0.1699.\n",
      "NOTE: Epoch i=397, g_loss=  0.4353, d_loss=  0.6256.\n",
      "NOTE: Epoch i=398, g_loss=  1.1439, d_loss=  0.3716.\n",
      "NOTE: Epoch i=399, g_loss=  0.1580, d_loss= -0.3386.\n",
      "NOTE: Epoch i=400, g_loss=  0.9103, d_loss= -0.2312.\n",
      "NOTE: Epoch i=401, g_loss=  0.7001, d_loss=  0.5064.\n",
      "NOTE: Epoch i=402, g_loss=  1.5263, d_loss= -0.5203.\n",
      "NOTE: Epoch i=403, g_loss=  1.0480, d_loss= -0.8871.\n",
      "NOTE: Epoch i=404, g_loss=  1.3720, d_loss= -0.1994.\n",
      "NOTE: Epoch i=405, g_loss=  1.2575, d_loss= -0.0299.\n",
      "NOTE: Epoch i=406, g_loss=  1.3171, d_loss=  0.0293.\n",
      "NOTE: Epoch i=407, g_loss=  0.4500, d_loss= -0.1237.\n",
      "NOTE: Epoch i=408, g_loss=  1.3686, d_loss=  0.0849.\n",
      "NOTE: Epoch i=409, g_loss=  0.8498, d_loss=  0.4352.\n",
      "NOTE: Epoch i=410, g_loss=  1.5454, d_loss=  0.0194.\n",
      "NOTE: Epoch i=411, g_loss=  1.5594, d_loss= -0.1624.\n",
      "NOTE: Epoch i=412, g_loss=  0.9241, d_loss= -0.6713.\n",
      "NOTE: Epoch i=413, g_loss=  1.3712, d_loss= -0.3247.\n",
      "NOTE: Epoch i=414, g_loss=  0.3008, d_loss=  0.1775.\n",
      "NOTE: Epoch i=415, g_loss=  1.0265, d_loss= -0.2415.\n",
      "NOTE: Epoch i=416, g_loss=  0.3056, d_loss=  0.8207.\n",
      "NOTE: Epoch i=417, g_loss=  0.7194, d_loss= -0.2016.\n",
      "NOTE: Epoch i=418, g_loss=  0.4739, d_loss=  0.3145.\n",
      "NOTE: Epoch i=419, g_loss=  0.8555, d_loss= -0.1696.\n",
      "NOTE: Epoch i=420, g_loss=  0.9149, d_loss=  0.3026.\n",
      "NOTE: Epoch i=421, g_loss=  0.5180, d_loss=  0.5899.\n",
      "NOTE: Epoch i=422, g_loss=  1.1317, d_loss=  0.5156.\n",
      "NOTE: Epoch i=423, g_loss=  0.3402, d_loss=  0.0063.\n",
      "NOTE: Epoch i=424, g_loss=  0.2379, d_loss=  0.2718.\n",
      "NOTE: Epoch i=425, g_loss=  0.5556, d_loss= -0.5321.\n",
      "NOTE: Epoch i=426, g_loss=  0.2399, d_loss=  0.9082.\n",
      "NOTE: Epoch i=427, g_loss=  0.6325, d_loss= -0.3890.\n",
      "NOTE: Epoch i=428, g_loss=  0.8459, d_loss=  0.1935.\n",
      "NOTE: Epoch i=429, g_loss=  0.8501, d_loss= -0.1518.\n",
      "NOTE: Epoch i=430, g_loss=  0.0654, d_loss=  0.3286.\n",
      "NOTE: Epoch i=431, g_loss=  0.9314, d_loss= -0.6071.\n",
      "NOTE: Epoch i=432, g_loss=  0.5989, d_loss= -0.0618.\n",
      "NOTE: Epoch i=433, g_loss=  0.6793, d_loss= -0.7678.\n",
      "NOTE: Epoch i=434, g_loss=  0.1661, d_loss= -0.5065.\n",
      "NOTE: Epoch i=435, g_loss=  0.8377, d_loss= -0.4955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Epoch i=436, g_loss=  1.0911, d_loss=  0.2849.\n",
      "NOTE: Epoch i=437, g_loss=  1.0472, d_loss= -0.3053.\n",
      "NOTE: Epoch i=438, g_loss=  0.4914, d_loss= -0.0174.\n",
      "NOTE: Epoch i=439, g_loss=  1.0914, d_loss=  0.1315.\n",
      "NOTE: Epoch i=440, g_loss=  0.5566, d_loss= -0.1650.\n",
      "NOTE: Epoch i=441, g_loss=  0.3810, d_loss= -0.0622.\n",
      "NOTE: Epoch i=442, g_loss= -0.2789, d_loss= -0.1495.\n",
      "NOTE: Epoch i=443, g_loss=  0.7169, d_loss=  0.0248.\n",
      "NOTE: Epoch i=444, g_loss=  0.5074, d_loss=  0.6921.\n",
      "NOTE: Epoch i=445, g_loss=  0.4092, d_loss= -0.3402.\n",
      "NOTE: Epoch i=446, g_loss=  1.1709, d_loss= -0.0731.\n",
      "NOTE: Epoch i=447, g_loss=  0.7311, d_loss= -0.1302.\n",
      "NOTE: Epoch i=448, g_loss=  0.4982, d_loss=  0.8197.\n",
      "NOTE: Epoch i=449, g_loss=  1.1752, d_loss= -0.9691.\n",
      "NOTE: Epoch i=450, g_loss=  1.0008, d_loss=  0.3933.\n",
      "NOTE: Epoch i=451, g_loss=  1.2021, d_loss= -0.1222.\n",
      "NOTE: Epoch i=452, g_loss=  1.1509, d_loss=  0.3824.\n",
      "NOTE: Epoch i=453, g_loss=  0.7552, d_loss= -0.0188.\n",
      "NOTE: Epoch i=454, g_loss=  0.8294, d_loss= -0.4575.\n",
      "NOTE: Epoch i=455, g_loss=  1.0474, d_loss= -0.8620.\n",
      "NOTE: Epoch i=456, g_loss=  1.4227, d_loss= -0.1746.\n",
      "NOTE: Epoch i=457, g_loss=  1.2759, d_loss=  0.3746.\n",
      "NOTE: Epoch i=458, g_loss=  0.9350, d_loss=  0.2352.\n",
      "NOTE: Epoch i=459, g_loss=  1.2848, d_loss=  0.0746.\n",
      "NOTE: Epoch i=460, g_loss=  1.2529, d_loss= -0.2717.\n",
      "NOTE: Epoch i=461, g_loss=  0.7682, d_loss= -0.1983.\n",
      "NOTE: Epoch i=462, g_loss=  1.4934, d_loss= -0.0682.\n",
      "NOTE: Epoch i=463, g_loss=  1.9174, d_loss=  0.9544.\n",
      "NOTE: Epoch i=464, g_loss=  1.4136, d_loss= -0.6578.\n",
      "NOTE: Epoch i=465, g_loss=  1.7892, d_loss= -0.1126.\n",
      "NOTE: Epoch i=466, g_loss=  2.1166, d_loss=  0.1771.\n",
      "NOTE: Epoch i=467, g_loss=  1.7872, d_loss= -0.0725.\n",
      "NOTE: Epoch i=468, g_loss=  1.0444, d_loss= -0.3926.\n",
      "NOTE: Epoch i=469, g_loss=  1.1426, d_loss= -0.9001.\n",
      "NOTE: Epoch i=470, g_loss=  1.7416, d_loss=  0.0050.\n",
      "NOTE: Epoch i=471, g_loss=  1.4580, d_loss= -0.0123.\n",
      "NOTE: Epoch i=472, g_loss=  1.7170, d_loss= -0.1239.\n",
      "NOTE: Epoch i=473, g_loss=  1.3870, d_loss= -0.1073.\n",
      "NOTE: Epoch i=474, g_loss=  1.5899, d_loss= -0.7743.\n",
      "NOTE: Epoch i=475, g_loss=  1.3633, d_loss= -0.6100.\n",
      "NOTE: Epoch i=476, g_loss=  0.9062, d_loss=  0.6825.\n",
      "NOTE: Epoch i=477, g_loss=  1.0524, d_loss=  0.3568.\n",
      "NOTE: Epoch i=478, g_loss=  1.0010, d_loss=  0.1690.\n",
      "NOTE: Epoch i=479, g_loss=  0.9876, d_loss= -0.3779.\n",
      "NOTE: Epoch i=480, g_loss=  1.3365, d_loss=  1.1111.\n",
      "NOTE: Epoch i=481, g_loss=  0.6195, d_loss= -0.9511.\n",
      "NOTE: Epoch i=482, g_loss=  1.2217, d_loss= -0.1577.\n",
      "NOTE: Epoch i=483, g_loss=  0.8479, d_loss=  0.0497.\n",
      "NOTE: Epoch i=484, g_loss=  1.1905, d_loss= -0.2108.\n",
      "NOTE: Epoch i=485, g_loss=  1.2670, d_loss=  0.1744.\n",
      "NOTE: Epoch i=486, g_loss=  1.3434, d_loss=  0.9254.\n",
      "NOTE: Epoch i=487, g_loss=  1.2833, d_loss=  0.5459.\n",
      "NOTE: Epoch i=488, g_loss=  1.4638, d_loss= -0.0430.\n",
      "NOTE: Epoch i=489, g_loss=  1.2777, d_loss=  0.8839.\n",
      "NOTE: Epoch i=490, g_loss=  1.0140, d_loss= -0.2226.\n",
      "NOTE: Epoch i=491, g_loss=  1.7054, d_loss=  0.5505.\n",
      "NOTE: Epoch i=492, g_loss=  0.4066, d_loss= -0.0320.\n",
      "NOTE: Epoch i=493, g_loss=  1.2364, d_loss= -0.2847.\n",
      "NOTE: Epoch i=494, g_loss=  1.2932, d_loss= -0.3354.\n",
      "NOTE: Epoch i=495, g_loss=  0.7345, d_loss= -0.1276.\n",
      "NOTE: Epoch i=496, g_loss=  1.2072, d_loss=  0.1612.\n",
      "NOTE: Epoch i=497, g_loss=  1.0998, d_loss= -0.3957.\n",
      "NOTE: Epoch i=498, g_loss=  0.7733, d_loss= -0.3980.\n",
      "NOTE: Epoch i=499, g_loss=  0.9709, d_loss= -0.0635.\n",
      "NOTE: Epoch i=500, g_loss=  0.7416, d_loss= -0.1691.\n",
      "NOTE: 13335891 bytes were written to the table \"cpctStore\" in the caslib \"CASUSER(alphel)\".\n",
      "NOTE: tabularGanTrain action completed successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; IterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch Number\">EpochNumber</th>\n",
       "      <th title=\"Autoencoder Loss\">AutoencoderLoss</th>\n",
       "      <th title=\"Generator Loss\">GeneratorLoss</th>\n",
       "      <th title=\"Discriminator Loss\">DiscriminatorLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.130240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.117382</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.109262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.106114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.094419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.207158</td>\n",
       "      <td>0.161225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.099795</td>\n",
       "      <td>-0.395746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.773278</td>\n",
       "      <td>-0.398004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.970907</td>\n",
       "      <td>-0.063536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.741552</td>\n",
       "      <td>-0.169136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; LevelFreq</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Variable Name\">VarName</th>\n",
       "      <th title=\"Level\">Level</th>\n",
       "      <th title=\"Frequency\">Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>1</td>\n",
       "      <td>17633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>label</td>\n",
       "      <td>2</td>\n",
       "      <td>17367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v11</td>\n",
       "      <td>0</td>\n",
       "      <td>17341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v11</td>\n",
       "      <td>1</td>\n",
       "      <td>17659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v12</td>\n",
       "      <td>0</td>\n",
       "      <td>32901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>v52</td>\n",
       "      <td>1</td>\n",
       "      <td>771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>v53</td>\n",
       "      <td>0</td>\n",
       "      <td>34324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>v53</td>\n",
       "      <td>1</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>v54</td>\n",
       "      <td>0</td>\n",
       "      <td>34569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>v54</td>\n",
       "      <td>1</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 3 columns</p>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"RowId\">RowId</th>\n",
       "      <th title=\"Description\">Description</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "      <th title=\"Numeric Value\">nValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMBEDDINGDIM</td>\n",
       "      <td>Generator Embedding Dimension</td>\n",
       "      <td>128</td>\n",
       "      <td>1.280000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MINIBATCHSIZE</td>\n",
       "      <td>Number of Observations in One Minibatch</td>\n",
       "      <td>500</td>\n",
       "      <td>5.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PACKSIZE</td>\n",
       "      <td>Number of Observations Group Together in Apply...</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REGDWEIGHT</td>\n",
       "      <td>Weight for Regularizing the Discriminator</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OPTIMIZERAE_BETA1</td>\n",
       "      <td>Exponential Decay Rate for the First Moment Es...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>9.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OPTIMIZERAE_BETA2</td>\n",
       "      <td>Exponential Decay Rate for the Second Moment E...</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>9.990000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OPTIMIZERAE_LEARNINGRATE</td>\n",
       "      <td>Learning Rate for the Autoencoder's Optimizer</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OPTIMIZERAE_NUMEPOCHS</td>\n",
       "      <td>Number of Epochs for the Autoencoder's Training</td>\n",
       "      <td>500</td>\n",
       "      <td>5.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OPTIMIZERAE_WEIGHTDECAY</td>\n",
       "      <td>Weight Decay for the Autoencoder's Optimizer</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>1.000000e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OPTIMIZERGAN_BETA1</td>\n",
       "      <td>Exponential Decay Rate for the First Moment Es...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OPTIMIZERGAN_BETA2</td>\n",
       "      <td>Exponential Decay Rate for the Second Moment E...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>9.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OPTIMIZERGAN_LEARNINGRATE</td>\n",
       "      <td>Learning Rate for the GAN Optimizer</td>\n",
       "      <td>2e-05</td>\n",
       "      <td>2.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>OPTIMIZERGAN_NUMEPOCHS</td>\n",
       "      <td>Number of Epochs for the GAN Training</td>\n",
       "      <td>500</td>\n",
       "      <td>5.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>OPTIMIZERGAN_WEIGHTDECAYD</td>\n",
       "      <td>Weight Decay for the Generator's Optimizer</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>OPTIMIZERGAN_WEIGHTDECAYG</td>\n",
       "      <td>Weight Decay for the Discriminator's Optimizer</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SEED</td>\n",
       "      <td>Seed for Random Initialization</td>\n",
       "      <td>12345</td>\n",
       "      <td>1.234500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>USELOGLEVELFREQ</td>\n",
       "      <td>Whether to Use Log Frequency of Categorical Le...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; NObs</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"RowId\">RowId</th>\n",
       "      <th title=\"Description\">Description</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NREAD</td>\n",
       "      <td>Number of Observations Read</td>\n",
       "      <td>35000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NUSED</td>\n",
       "      <td>Number of Observations Used</td>\n",
       "      <td>35000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Label\">Label</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(alphel)</td>\n",
       "      <td>out</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>CASTable('out', caslib='CASUSER(alphel)')</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 4.49e+03s</span> &#183; <span class=\"cas-user\">user 1.32e+04s</span> &#183; <span class=\"cas-sys\">sys 459s</span> &#183; <span class=\"cas-memory\">mem 29.1MB</span></small></p>"
      ],
      "text/plain": [
       "[IterHistory]\n",
       "\n",
       "      EpochNumber  AutoencoderLoss  GeneratorLoss  DiscriminatorLoss\n",
       " 0              1         0.130240            NaN                NaN\n",
       " 1              2         0.117382            NaN                NaN\n",
       " 2              3         0.109262            NaN                NaN\n",
       " 3              4         0.106114            NaN                NaN\n",
       " 4              5         0.094419            NaN                NaN\n",
       " ..           ...              ...            ...                ...\n",
       " 995          496              NaN       1.207158           0.161225\n",
       " 996          497              NaN       1.099795          -0.395746\n",
       " 997          498              NaN       0.773278          -0.398004\n",
       " 998          499              NaN       0.970907          -0.063536\n",
       " 999          500              NaN       0.741552          -0.169136\n",
       " \n",
       " [1000 rows x 4 columns]\n",
       "\n",
       "[LevelFreq]\n",
       "\n",
       "    VarName Level  Frequency\n",
       " 0    label     1      17633\n",
       " 1    label     2      17367\n",
       " 2      v11     0      17341\n",
       " 3      v11     1      17659\n",
       " 4      v12     0      32901\n",
       " ..     ...   ...        ...\n",
       " 80     v52     1        771\n",
       " 81     v53     0      34324\n",
       " 82     v53     1        676\n",
       " 83     v54     0      34569\n",
       " 84     v54     1        431\n",
       " \n",
       " [85 rows x 3 columns]\n",
       "\n",
       "[ModelInfo]\n",
       "\n",
       "                         RowId  \\\n",
       " 0                EMBEDDINGDIM   \n",
       " 1               MINIBATCHSIZE   \n",
       " 2                    PACKSIZE   \n",
       " 3                  REGDWEIGHT   \n",
       " 4           OPTIMIZERAE_BETA1   \n",
       " 5           OPTIMIZERAE_BETA2   \n",
       " 6    OPTIMIZERAE_LEARNINGRATE   \n",
       " 7       OPTIMIZERAE_NUMEPOCHS   \n",
       " 8     OPTIMIZERAE_WEIGHTDECAY   \n",
       " 9          OPTIMIZERGAN_BETA1   \n",
       " 10         OPTIMIZERGAN_BETA2   \n",
       " 11  OPTIMIZERGAN_LEARNINGRATE   \n",
       " 12     OPTIMIZERGAN_NUMEPOCHS   \n",
       " 13  OPTIMIZERGAN_WEIGHTDECAYD   \n",
       " 14  OPTIMIZERGAN_WEIGHTDECAYG   \n",
       " 15                       SEED   \n",
       " 16            USELOGLEVELFREQ   \n",
       " \n",
       "                                           Description     Value        nValue  \n",
       " 0                       Generator Embedding Dimension       128  1.280000e+02  \n",
       " 1             Number of Observations in One Minibatch       500  5.000000e+02  \n",
       " 2   Number of Observations Group Together in Apply...        10  1.000000e+01  \n",
       " 3           Weight for Regularizing the Discriminator        10  1.000000e+01  \n",
       " 4   Exponential Decay Rate for the First Moment Es...  0.900000  9.000000e-01  \n",
       " 5   Exponential Decay Rate for the Second Moment E...  0.999000  9.990000e-01  \n",
       " 6       Learning Rate for the Autoencoder's Optimizer     0.001  1.000000e-03  \n",
       " 7     Number of Epochs for the Autoencoder's Training       500  5.000000e+02  \n",
       " 8        Weight Decay for the Autoencoder's Optimizer     1e-08  1.000000e-08  \n",
       " 9   Exponential Decay Rate for the First Moment Es...  0.500000  5.000000e-01  \n",
       " 10  Exponential Decay Rate for the Second Moment E...  0.900000  9.000000e-01  \n",
       " 11                Learning Rate for the GAN Optimizer     2e-05  2.000000e-05  \n",
       " 12              Number of Epochs for the GAN Training       500  5.000000e+02  \n",
       " 13         Weight Decay for the Generator's Optimizer    0.0001  1.000000e-04  \n",
       " 14     Weight Decay for the Discriminator's Optimizer     1e-06  1.000000e-06  \n",
       " 15                     Seed for Random Initialization     12345  1.234500e+04  \n",
       " 16  Whether to Use Log Frequency of Categorical Le...      True  1.000000e+00  \n",
       "\n",
       "[NObs]\n",
       "\n",
       "    RowId                  Description  Value\n",
       " 0  NREAD  Number of Observations Read  35000\n",
       " 1  NUSED  Number of Observations Used  35000\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib Name Label  Rows  Columns  \\\n",
       " 0  CASUSER(alphel)  out           0       55   \n",
       " \n",
       "                                     casTable  \n",
       " 0  CASTable('out', caslib='CASUSER(alphel)')  \n",
       "\n",
       "+ Elapsed: 4.49e+03s, user: 1.32e+04s, sys: 459s, mem: 29.1mb"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = s.tabularGanTrain(\n",
    "table = {\"name\":\"GAN_data_train\"},\n",
    "    centroidsTable= \"cen\",\n",
    "    gpu = 1,\n",
    "    nominals = cov_discrete_columns,\n",
    "    optimizerAe ={\"method\":'ADAM',\"numEpochs\":500},\n",
    "    optimizerGan ={\"method\":'ADAM',\"numEpochs\":500},\n",
    "    seed = 12345,\n",
    "    scoreSeed = 1234,\n",
    "    numSamples =50000,\n",
    "    saveState ={\"name\":\"cpctStore\", \"replace\":True},\n",
    "    casOut = {\"name\":\"out\", \"replace\":True}\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = s.fetch('out', to=400000, maxrows=400000)['Fetch']\n",
    "gloss = results.IterHistory['GeneratorLoss'].dropna().reset_index(drop=True)\n",
    "dloss = results.IterHistory['DiscriminatorLoss'].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Losses for CPCTGAN on CoverType data')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(dloss)\n",
    "plt.plot(gloss)\n",
    "plt.legend(['D', 'G'])\n",
    "plt.title('Losses for CPCTGAN on CoverType data')\n",
    "#plt.savefig('Original-CTGAN-Adult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    25941\n",
       "2.0    24059\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adult_data_all.head()\n",
    "fake_X = samples.drop([\"label\"],axis=1)\n",
    "fake_X, fake_y = fake_X,samples.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 54)\n",
      "(50000,)\n",
      "(15000, 54)\n"
     ]
    }
   ],
   "source": [
    "# check the training data shape to agree with the original data (# of features/cols)\n",
    "print(fake_X.shape)\n",
    "print(fake_y.shape)\n",
    "print(orig_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML scores for the GAN generated data:\n",
      "Decision Tree Acc:  0.7464666666666666 f-1:  0.7590750712701932 AUC: 0.8128868116410866\n",
      "Linear SVM Acc:  0.4693333333333333 f-1:  0.5012531328320802 AUC: 0.4663782358153482\n",
      "Random Forest Acc:  0.7560666666666667 f-1:  0.7613955004890773 AUC: 0.8280339660946889\n",
      "Logistic Regression Acc:  0.7218666666666667 f-1:  0.7225694906237532 AUC: 0.7924872232265183\n",
      "MLP Acc:  0.6744666666666667 f-1:  0.6264820622657387 AUC: 0.726017893875996\n"
     ]
    }
   ],
   "source": [
    "# train a classifier on the CPCTGAN generated data\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5,random_state=0),\n",
    "    SVC(kernel = 'linear', max_iter=1000, C=0.025, random_state=0, probability=True),\n",
    "    RandomForestClassifier(n_estimators=200, random_state=0),\n",
    "    LogisticRegression(max_iter=1000, random_state=0),\n",
    "    MLPClassifier(alpha=1, max_iter=1000, random_state=0)]\n",
    "\n",
    "print('ML scores for the GAN generated data:')\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(fake_X, fake_y)\n",
    "    score = clf.score(orig_X_test, orig_y_test)\n",
    "    y_pred = clf.predict(orig_X_test)\n",
    "    Acc = accuracy_score(orig_y_test, y_pred)\n",
    "    fscore = f1_score(orig_y_test, y_pred, average='binary')\n",
    "    AUC = roc_auc_score(orig_y_test, clf.predict_proba(orig_X_test)[:, 1])\n",
    "    print(name,'Acc: ', Acc, 'f-1: ', fscore, 'AUC:', AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    25941\n",
       "2.0    24059\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    7633\n",
       "1    7367\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6196"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred)/len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train CTGAN on same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctgan import CTGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -2.6864,Loss D: -0.6043\n",
      "Epoch 2, Loss G: -2.2125,Loss D:  0.3068\n",
      "Epoch 3, Loss G: -1.9280,Loss D: -0.1577\n",
      "Epoch 4, Loss G: -2.5762,Loss D: -0.2678\n",
      "Epoch 5, Loss G: -1.6353,Loss D: -0.6591\n",
      "Epoch 6, Loss G: -2.3421,Loss D:  0.1088\n",
      "Epoch 7, Loss G: -2.8534,Loss D:  0.3613\n",
      "Epoch 8, Loss G: -2.6113,Loss D: -0.4859\n",
      "Epoch 9, Loss G: -2.0300,Loss D:  0.6054\n",
      "Epoch 10, Loss G: -2.6107,Loss D: -0.1271\n",
      "Epoch 11, Loss G: -2.4115,Loss D: -0.5965\n",
      "Epoch 12, Loss G: -2.3249,Loss D:  0.3018\n",
      "Epoch 13, Loss G: -2.9022,Loss D: -0.2775\n",
      "Epoch 14, Loss G: -3.7291,Loss D: -0.0367\n",
      "Epoch 15, Loss G: -3.8095,Loss D: -0.5686\n",
      "Epoch 16, Loss G: -3.8778,Loss D:  0.7235\n",
      "Epoch 17, Loss G: -3.4199,Loss D: -0.0667\n",
      "Epoch 18, Loss G: -3.4569,Loss D:  0.5409\n",
      "Epoch 19, Loss G: -4.0054,Loss D: -1.0383\n",
      "Epoch 20, Loss G: -4.0684,Loss D:  0.0706\n",
      "Epoch 21, Loss G: -4.7258,Loss D: -0.4553\n",
      "Epoch 22, Loss G: -4.4477,Loss D:  0.4417\n",
      "Epoch 23, Loss G: -4.1633,Loss D: -1.0142\n",
      "Epoch 24, Loss G: -4.2719,Loss D:  0.3893\n",
      "Epoch 25, Loss G: -4.1194,Loss D:  0.3545\n",
      "Epoch 26, Loss G: -3.4320,Loss D: -0.0345\n",
      "Epoch 27, Loss G: -3.8138,Loss D:  0.4997\n",
      "Epoch 28, Loss G: -4.7603,Loss D: -0.3084\n",
      "Epoch 29, Loss G: -3.7331,Loss D:  0.2850\n",
      "Epoch 30, Loss G: -3.0636,Loss D: -0.7668\n",
      "Epoch 31, Loss G: -2.3652,Loss D:  0.1602\n",
      "Epoch 32, Loss G: -3.3579,Loss D: -0.1569\n",
      "Epoch 33, Loss G: -2.3172,Loss D:  0.2571\n",
      "Epoch 34, Loss G: -2.5262,Loss D: -0.6430\n",
      "Epoch 35, Loss G: -1.8015,Loss D: -0.5138\n",
      "Epoch 36, Loss G: -3.3537,Loss D: -1.5623\n",
      "Epoch 37, Loss G: -2.8920,Loss D:  1.3023\n",
      "Epoch 38, Loss G: -2.5831,Loss D:  0.8821\n",
      "Epoch 39, Loss G: -2.0318,Loss D:  0.3384\n",
      "Epoch 40, Loss G: -2.7864,Loss D:  0.4665\n",
      "Epoch 41, Loss G: -2.7564,Loss D: -0.2204\n",
      "Epoch 42, Loss G: -2.3929,Loss D: -0.4920\n",
      "Epoch 43, Loss G: -3.1713,Loss D: -0.6446\n",
      "Epoch 44, Loss G: -3.9644,Loss D:  0.2802\n",
      "Epoch 45, Loss G: -2.7785,Loss D: -0.1646\n",
      "Epoch 46, Loss G: -2.2389,Loss D: -0.0876\n",
      "Epoch 47, Loss G: -3.0401,Loss D: -0.4089\n",
      "Epoch 48, Loss G: -3.8257,Loss D: -0.4745\n",
      "Epoch 49, Loss G: -3.5153,Loss D:  0.2891\n",
      "Epoch 50, Loss G: -3.5871,Loss D: -0.1624\n",
      "Epoch 51, Loss G: -4.0879,Loss D: -0.2439\n",
      "Epoch 52, Loss G: -3.0253,Loss D: -1.1856\n",
      "Epoch 53, Loss G: -4.0611,Loss D:  0.2259\n",
      "Epoch 54, Loss G: -4.4561,Loss D: -0.4497\n",
      "Epoch 55, Loss G: -4.2306,Loss D: -0.8295\n",
      "Epoch 56, Loss G: -3.4635,Loss D:  0.3875\n",
      "Epoch 57, Loss G: -4.3947,Loss D:  0.2993\n",
      "Epoch 58, Loss G: -3.1805,Loss D:  0.1396\n",
      "Epoch 59, Loss G: -3.1173,Loss D:  0.5940\n",
      "Epoch 60, Loss G: -4.3838,Loss D: -0.0790\n",
      "Epoch 61, Loss G: -4.6624,Loss D:  0.4641\n",
      "Epoch 62, Loss G: -2.7300,Loss D:  0.0037\n",
      "Epoch 63, Loss G: -4.2870,Loss D:  0.3777\n",
      "Epoch 64, Loss G: -3.5918,Loss D:  0.5319\n",
      "Epoch 65, Loss G: -3.6838,Loss D:  0.8299\n",
      "Epoch 66, Loss G: -3.3904,Loss D: -0.4181\n",
      "Epoch 67, Loss G: -3.0979,Loss D:  0.5144\n",
      "Epoch 68, Loss G: -4.6060,Loss D:  0.6353\n",
      "Epoch 69, Loss G: -3.4613,Loss D: -0.0632\n",
      "Epoch 70, Loss G: -4.0181,Loss D:  0.1027\n",
      "Epoch 71, Loss G: -3.8487,Loss D: -0.5566\n",
      "Epoch 72, Loss G: -3.5160,Loss D:  0.6665\n",
      "Epoch 73, Loss G: -3.5466,Loss D:  0.1457\n",
      "Epoch 74, Loss G: -2.6723,Loss D:  0.2731\n",
      "Epoch 75, Loss G: -3.5162,Loss D: -0.1120\n",
      "Epoch 76, Loss G: -4.4062,Loss D: -0.3001\n",
      "Epoch 77, Loss G: -3.9118,Loss D:  0.4358\n",
      "Epoch 78, Loss G: -3.5968,Loss D: -0.2050\n",
      "Epoch 79, Loss G: -3.3512,Loss D:  0.1325\n",
      "Epoch 80, Loss G: -3.6084,Loss D: -1.0425\n",
      "Epoch 81, Loss G: -3.5468,Loss D: -0.0163\n",
      "Epoch 82, Loss G: -4.2064,Loss D:  0.0019\n",
      "Epoch 83, Loss G: -4.4429,Loss D: -0.2426\n",
      "Epoch 84, Loss G: -5.0147,Loss D:  0.8259\n",
      "Epoch 85, Loss G: -4.4324,Loss D:  0.0358\n",
      "Epoch 86, Loss G: -4.5752,Loss D:  0.5181\n",
      "Epoch 87, Loss G: -4.4825,Loss D: -0.2445\n",
      "Epoch 88, Loss G: -4.7729,Loss D:  0.3808\n",
      "Epoch 89, Loss G: -4.5178,Loss D:  0.1055\n",
      "Epoch 90, Loss G: -5.1636,Loss D:  0.4981\n",
      "Epoch 91, Loss G: -4.3162,Loss D:  0.1405\n",
      "Epoch 92, Loss G: -5.3199,Loss D:  0.7646\n",
      "Epoch 93, Loss G: -4.1587,Loss D: -0.4894\n",
      "Epoch 94, Loss G: -5.2072,Loss D: -0.1579\n",
      "Epoch 95, Loss G: -4.3557,Loss D:  1.2499\n",
      "Epoch 96, Loss G: -4.4888,Loss D: -0.2924\n",
      "Epoch 97, Loss G: -5.2847,Loss D:  0.2384\n",
      "Epoch 98, Loss G: -4.7995,Loss D: -0.1166\n",
      "Epoch 99, Loss G: -4.4534,Loss D: -0.4435\n",
      "Epoch 100, Loss G: -5.1694,Loss D: -0.9264\n",
      "Epoch 101, Loss G: -4.3094,Loss D: -0.2484\n",
      "Epoch 102, Loss G: -6.2278,Loss D: -0.9855\n",
      "Epoch 103, Loss G: -5.3927,Loss D:  1.0706\n",
      "Epoch 104, Loss G: -4.5063,Loss D:  0.8904\n",
      "Epoch 105, Loss G: -5.0154,Loss D: -1.0177\n",
      "Epoch 106, Loss G: -4.4105,Loss D: -0.5853\n",
      "Epoch 107, Loss G: -3.9502,Loss D:  0.9106\n",
      "Epoch 108, Loss G: -3.9392,Loss D: -0.1637\n",
      "Epoch 109, Loss G: -3.6317,Loss D:  0.5581\n",
      "Epoch 110, Loss G: -2.6949,Loss D: -1.0226\n",
      "Epoch 111, Loss G: -3.9778,Loss D: -0.2565\n",
      "Epoch 112, Loss G: -3.2572,Loss D: -0.3405\n",
      "Epoch 113, Loss G: -4.6984,Loss D: -1.0109\n",
      "Epoch 114, Loss G: -4.3478,Loss D: -0.7000\n",
      "Epoch 115, Loss G: -4.0200,Loss D:  0.5419\n",
      "Epoch 116, Loss G: -4.3584,Loss D:  0.0847\n",
      "Epoch 117, Loss G: -4.6053,Loss D:  0.3567\n",
      "Epoch 118, Loss G: -4.3985,Loss D:  0.0305\n",
      "Epoch 119, Loss G: -4.1854,Loss D: -0.2905\n",
      "Epoch 120, Loss G: -5.1213,Loss D: -0.2281\n",
      "Epoch 121, Loss G: -4.3654,Loss D: -0.1464\n",
      "Epoch 122, Loss G: -4.6202,Loss D: -0.2127\n",
      "Epoch 123, Loss G: -5.1646,Loss D:  0.0761\n",
      "Epoch 124, Loss G: -4.4298,Loss D:  0.2854\n",
      "Epoch 125, Loss G: -4.7169,Loss D:  0.2002\n",
      "Epoch 126, Loss G: -4.9228,Loss D:  0.1173\n",
      "Epoch 127, Loss G: -4.8497,Loss D: -0.4141\n",
      "Epoch 128, Loss G: -4.6758,Loss D: -0.0842\n",
      "Epoch 129, Loss G: -5.3561,Loss D:  0.1171\n",
      "Epoch 130, Loss G: -4.5887,Loss D:  0.5365\n",
      "Epoch 131, Loss G: -4.5163,Loss D: -0.0290\n",
      "Epoch 132, Loss G: -4.5710,Loss D: -0.0415\n",
      "Epoch 133, Loss G: -4.5043,Loss D:  0.2770\n",
      "Epoch 134, Loss G: -3.8436,Loss D:  0.2560\n",
      "Epoch 135, Loss G: -4.6012,Loss D: -0.5504\n",
      "Epoch 136, Loss G: -5.1084,Loss D:  0.2546\n",
      "Epoch 137, Loss G: -4.5116,Loss D: -0.5157\n",
      "Epoch 138, Loss G: -3.9908,Loss D: -1.1900\n",
      "Epoch 139, Loss G: -4.1713,Loss D: -0.6209\n",
      "Epoch 140, Loss G: -3.8110,Loss D:  0.3080\n",
      "Epoch 141, Loss G: -4.3436,Loss D:  0.1673\n",
      "Epoch 142, Loss G: -4.4493,Loss D: -0.4225\n",
      "Epoch 143, Loss G: -3.6741,Loss D:  0.2235\n",
      "Epoch 144, Loss G: -3.8387,Loss D: -1.0906\n",
      "Epoch 145, Loss G: -3.9437,Loss D:  0.1465\n",
      "Epoch 146, Loss G: -4.3115,Loss D:  0.2405\n",
      "Epoch 147, Loss G: -4.0120,Loss D:  0.0050\n",
      "Epoch 148, Loss G: -3.1720,Loss D: -1.1037\n",
      "Epoch 149, Loss G: -4.0171,Loss D: -0.5237\n",
      "Epoch 150, Loss G: -4.3661,Loss D:  0.1794\n",
      "Epoch 151, Loss G: -5.1030,Loss D: -0.9586\n",
      "Epoch 152, Loss G: -5.6373,Loss D: -0.4522\n",
      "Epoch 153, Loss G: -4.4438,Loss D: -0.2070\n",
      "Epoch 154, Loss G: -4.4617,Loss D: -0.4415\n",
      "Epoch 155, Loss G: -4.6224,Loss D: -0.0190\n",
      "Epoch 156, Loss G: -5.4870,Loss D:  0.0675\n",
      "Epoch 157, Loss G: -4.5388,Loss D:  0.3033\n",
      "Epoch 158, Loss G: -5.0429,Loss D: -0.5025\n",
      "Epoch 159, Loss G: -5.2062,Loss D:  0.0031\n",
      "Epoch 160, Loss G: -4.6843,Loss D:  0.0593\n",
      "Epoch 161, Loss G: -5.1486,Loss D:  0.2646\n",
      "Epoch 162, Loss G: -5.1826,Loss D:  0.3862\n",
      "Epoch 163, Loss G: -5.7157,Loss D: -0.4563\n",
      "Epoch 164, Loss G: -4.1711,Loss D:  0.5754\n",
      "Epoch 165, Loss G: -5.0622,Loss D: -0.9137\n",
      "Epoch 166, Loss G: -4.6479,Loss D:  0.1120\n",
      "Epoch 167, Loss G: -4.6081,Loss D:  0.4269\n",
      "Epoch 168, Loss G: -5.2640,Loss D: -0.3232\n",
      "Epoch 169, Loss G: -4.8405,Loss D: -0.2075\n",
      "Epoch 170, Loss G: -5.3106,Loss D: -0.1284\n",
      "Epoch 171, Loss G: -4.7974,Loss D:  0.7363\n",
      "Epoch 172, Loss G: -4.9609,Loss D:  0.1490\n",
      "Epoch 173, Loss G: -5.3479,Loss D:  0.2712\n",
      "Epoch 174, Loss G: -4.9270,Loss D:  0.5789\n",
      "Epoch 175, Loss G: -4.7744,Loss D: -0.1293\n",
      "Epoch 176, Loss G: -5.4318,Loss D: -0.2305\n",
      "Epoch 177, Loss G: -5.5315,Loss D: -0.3109\n",
      "Epoch 178, Loss G: -5.1207,Loss D: -0.4331\n",
      "Epoch 179, Loss G: -5.0403,Loss D:  0.1785\n",
      "Epoch 180, Loss G: -4.9936,Loss D: -0.1266\n",
      "Epoch 181, Loss G: -4.9171,Loss D:  0.0787\n",
      "Epoch 182, Loss G: -4.6185,Loss D:  0.1392\n",
      "Epoch 183, Loss G: -4.8205,Loss D: -0.1752\n",
      "Epoch 184, Loss G: -4.7416,Loss D:  0.4277\n",
      "Epoch 185, Loss G: -5.0518,Loss D:  0.6648\n",
      "Epoch 186, Loss G: -4.2711,Loss D: -0.0649\n",
      "Epoch 187, Loss G: -4.5558,Loss D:  0.0239\n",
      "Epoch 188, Loss G: -4.5087,Loss D: -0.3365\n",
      "Epoch 189, Loss G: -4.3045,Loss D:  0.2603\n",
      "Epoch 190, Loss G: -3.9623,Loss D: -0.3014\n",
      "Epoch 191, Loss G: -4.2829,Loss D: -0.5302\n",
      "Epoch 192, Loss G: -3.8803,Loss D:  0.2541\n",
      "Epoch 193, Loss G: -5.0129,Loss D:  0.7649\n",
      "Epoch 194, Loss G: -4.3024,Loss D: -0.2741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195, Loss G: -4.7568,Loss D: -0.0216\n",
      "Epoch 196, Loss G: -4.1681,Loss D:  0.7411\n",
      "Epoch 197, Loss G: -5.0201,Loss D:  0.0128\n",
      "Epoch 198, Loss G: -4.6964,Loss D:  0.1426\n",
      "Epoch 199, Loss G: -4.8464,Loss D:  0.3853\n",
      "Epoch 200, Loss G: -4.3099,Loss D:  0.2007\n",
      "Epoch 201, Loss G: -4.8220,Loss D:  0.0864\n",
      "Epoch 202, Loss G: -4.8249,Loss D:  0.1962\n",
      "Epoch 203, Loss G: -4.4208,Loss D:  0.0648\n",
      "Epoch 204, Loss G: -4.7018,Loss D: -0.1891\n",
      "Epoch 205, Loss G: -4.1471,Loss D: -0.1514\n",
      "Epoch 206, Loss G: -4.9994,Loss D:  0.2838\n",
      "Epoch 207, Loss G: -4.5416,Loss D: -0.1442\n",
      "Epoch 208, Loss G: -4.9797,Loss D: -0.5117\n",
      "Epoch 209, Loss G: -4.2278,Loss D:  0.2811\n",
      "Epoch 210, Loss G: -4.5277,Loss D:  0.0193\n",
      "Epoch 211, Loss G: -4.5482,Loss D:  0.3245\n",
      "Epoch 212, Loss G: -4.7757,Loss D: -0.4299\n",
      "Epoch 213, Loss G: -4.3209,Loss D: -0.2114\n",
      "Epoch 214, Loss G: -4.4885,Loss D: -0.2511\n",
      "Epoch 215, Loss G: -4.4451,Loss D:  0.4003\n",
      "Epoch 216, Loss G: -4.6960,Loss D: -0.0841\n",
      "Epoch 217, Loss G: -4.8026,Loss D: -0.6823\n",
      "Epoch 218, Loss G: -5.2880,Loss D:  0.0431\n",
      "Epoch 219, Loss G: -4.5847,Loss D: -0.1695\n",
      "Epoch 220, Loss G: -4.3507,Loss D: -0.2136\n",
      "Epoch 221, Loss G: -4.4581,Loss D:  0.1825\n",
      "Epoch 222, Loss G: -4.5035,Loss D: -0.4864\n",
      "Epoch 223, Loss G: -3.8446,Loss D:  0.3074\n",
      "Epoch 224, Loss G: -4.3966,Loss D:  0.5939\n",
      "Epoch 225, Loss G: -4.2911,Loss D: -0.2556\n",
      "Epoch 226, Loss G: -4.8491,Loss D:  0.3070\n",
      "Epoch 227, Loss G: -4.3558,Loss D:  0.3472\n",
      "Epoch 228, Loss G: -3.8479,Loss D:  0.4325\n",
      "Epoch 229, Loss G: -3.5847,Loss D: -0.3753\n",
      "Epoch 230, Loss G: -3.5252,Loss D: -0.0879\n",
      "Epoch 231, Loss G: -4.3412,Loss D:  0.1758\n",
      "Epoch 232, Loss G: -4.2037,Loss D:  0.2544\n",
      "Epoch 233, Loss G: -4.4050,Loss D:  0.0222\n",
      "Epoch 234, Loss G: -4.5161,Loss D:  0.0944\n",
      "Epoch 235, Loss G: -4.3157,Loss D: -0.3055\n",
      "Epoch 236, Loss G: -3.7607,Loss D:  0.0327\n",
      "Epoch 237, Loss G: -4.0829,Loss D: -0.1555\n",
      "Epoch 238, Loss G: -4.4780,Loss D: -0.0352\n",
      "Epoch 239, Loss G: -4.1331,Loss D: -0.0122\n",
      "Epoch 240, Loss G: -3.9290,Loss D:  0.4688\n",
      "Epoch 241, Loss G: -3.6329,Loss D:  0.1291\n",
      "Epoch 242, Loss G: -4.1107,Loss D:  0.1893\n",
      "Epoch 243, Loss G: -3.8499,Loss D:  0.0911\n",
      "Epoch 244, Loss G: -4.2421,Loss D:  0.0153\n",
      "Epoch 245, Loss G: -3.8935,Loss D: -0.3200\n",
      "Epoch 246, Loss G: -3.6052,Loss D:  0.0692\n",
      "Epoch 247, Loss G: -3.9746,Loss D: -0.2537\n",
      "Epoch 248, Loss G: -4.3279,Loss D:  0.0753\n",
      "Epoch 249, Loss G: -3.8973,Loss D:  0.1118\n",
      "Epoch 250, Loss G: -4.2966,Loss D: -0.0083\n",
      "Epoch 251, Loss G: -4.7144,Loss D:  0.1151\n",
      "Epoch 252, Loss G: -4.4746,Loss D:  0.0779\n",
      "Epoch 253, Loss G: -4.6786,Loss D: -0.0770\n",
      "Epoch 254, Loss G: -4.7470,Loss D: -0.4148\n",
      "Epoch 255, Loss G: -4.5877,Loss D: -0.3763\n",
      "Epoch 256, Loss G: -3.9139,Loss D:  0.4714\n",
      "Epoch 257, Loss G: -4.8101,Loss D:  0.1232\n",
      "Epoch 258, Loss G: -4.3619,Loss D: -0.0860\n",
      "Epoch 259, Loss G: -4.4108,Loss D:  0.2088\n",
      "Epoch 260, Loss G: -4.4472,Loss D:  0.0330\n",
      "Epoch 261, Loss G: -4.6154,Loss D:  0.4857\n",
      "Epoch 262, Loss G: -4.7316,Loss D:  0.0949\n",
      "Epoch 263, Loss G: -4.3219,Loss D:  0.2162\n",
      "Epoch 264, Loss G: -5.1003,Loss D: -0.1454\n",
      "Epoch 265, Loss G: -4.3115,Loss D:  0.1711\n",
      "Epoch 266, Loss G: -4.8223,Loss D:  0.2256\n",
      "Epoch 267, Loss G: -4.1632,Loss D: -0.5710\n",
      "Epoch 268, Loss G: -4.2631,Loss D: -0.0564\n",
      "Epoch 269, Loss G: -4.2474,Loss D: -0.2114\n",
      "Epoch 270, Loss G: -3.8258,Loss D: -0.2019\n",
      "Epoch 271, Loss G: -4.5478,Loss D: -0.3059\n",
      "Epoch 272, Loss G: -4.1210,Loss D: -0.0935\n",
      "Epoch 273, Loss G: -4.2062,Loss D:  0.2425\n",
      "Epoch 274, Loss G: -4.3292,Loss D:  0.2661\n",
      "Epoch 275, Loss G: -3.9945,Loss D:  0.5371\n",
      "Epoch 276, Loss G: -3.9521,Loss D: -0.1705\n",
      "Epoch 277, Loss G: -3.9378,Loss D: -0.0171\n",
      "Epoch 278, Loss G: -4.0359,Loss D:  0.4425\n",
      "Epoch 279, Loss G: -4.3618,Loss D:  0.3879\n",
      "Epoch 280, Loss G: -4.5666,Loss D: -0.4433\n",
      "Epoch 281, Loss G: -4.0114,Loss D:  0.2094\n",
      "Epoch 282, Loss G: -4.0680,Loss D: -0.4205\n",
      "Epoch 283, Loss G: -3.9360,Loss D: -0.1334\n",
      "Epoch 284, Loss G: -4.2271,Loss D:  0.0408\n",
      "Epoch 285, Loss G: -3.9092,Loss D:  0.2310\n",
      "Epoch 286, Loss G: -4.0255,Loss D: -0.1616\n",
      "Epoch 287, Loss G: -3.6953,Loss D: -0.0557\n",
      "Epoch 288, Loss G: -3.8746,Loss D: -0.0120\n",
      "Epoch 289, Loss G: -3.7773,Loss D:  0.0525\n",
      "Epoch 290, Loss G: -3.6840,Loss D:  0.3297\n",
      "Epoch 291, Loss G: -4.3813,Loss D: -0.1733\n",
      "Epoch 292, Loss G: -3.9846,Loss D:  0.0671\n",
      "Epoch 293, Loss G: -3.8724,Loss D:  0.4566\n",
      "Epoch 294, Loss G: -4.0474,Loss D:  0.1292\n",
      "Epoch 295, Loss G: -4.0821,Loss D:  0.4359\n",
      "Epoch 296, Loss G: -4.3798,Loss D: -0.0078\n",
      "Epoch 297, Loss G: -4.2118,Loss D: -0.0702\n",
      "Epoch 298, Loss G: -4.3828,Loss D:  0.0283\n",
      "Epoch 299, Loss G: -4.0351,Loss D:  0.2353\n",
      "Epoch 300, Loss G: -4.4955,Loss D:  0.2259\n",
      "Epoch 301, Loss G: -3.9897,Loss D:  0.0825\n",
      "Epoch 302, Loss G: -4.1655,Loss D: -0.1471\n",
      "Epoch 303, Loss G: -3.7472,Loss D:  0.2079\n",
      "Epoch 304, Loss G: -3.7975,Loss D:  0.0134\n",
      "Epoch 305, Loss G: -3.7954,Loss D: -0.3467\n",
      "Epoch 306, Loss G: -3.6873,Loss D: -0.1000\n",
      "Epoch 307, Loss G: -3.8302,Loss D:  0.0409\n",
      "Epoch 308, Loss G: -3.8975,Loss D:  0.0060\n",
      "Epoch 309, Loss G: -3.8647,Loss D: -0.3022\n",
      "Epoch 310, Loss G: -3.9221,Loss D: -0.0926\n",
      "Epoch 311, Loss G: -3.8255,Loss D: -0.1579\n",
      "Epoch 312, Loss G: -4.0006,Loss D:  0.1798\n",
      "Epoch 313, Loss G: -4.0624,Loss D:  0.1882\n",
      "Epoch 314, Loss G: -3.9120,Loss D:  0.3306\n",
      "Epoch 315, Loss G: -4.2549,Loss D: -0.1738\n",
      "Epoch 316, Loss G: -4.0568,Loss D: -0.3899\n",
      "Epoch 317, Loss G: -4.0684,Loss D:  0.0978\n",
      "Epoch 318, Loss G: -4.3341,Loss D:  0.0995\n",
      "Epoch 319, Loss G: -3.7582,Loss D:  0.1374\n",
      "Epoch 320, Loss G: -4.1473,Loss D:  0.1777\n",
      "Epoch 321, Loss G: -3.7871,Loss D:  0.0400\n",
      "Epoch 322, Loss G: -3.6830,Loss D: -0.2332\n",
      "Epoch 323, Loss G: -4.0629,Loss D:  0.2211\n",
      "Epoch 324, Loss G: -4.1329,Loss D:  0.0576\n",
      "Epoch 325, Loss G: -3.7573,Loss D:  0.0132\n",
      "Epoch 326, Loss G: -4.0676,Loss D: -0.2005\n",
      "Epoch 327, Loss G: -4.2808,Loss D: -0.0339\n",
      "Epoch 328, Loss G: -4.5175,Loss D:  0.2030\n",
      "Epoch 329, Loss G: -4.4996,Loss D: -0.1156\n",
      "Epoch 330, Loss G: -4.3900,Loss D:  0.0101\n",
      "Epoch 331, Loss G: -4.2691,Loss D: -0.1272\n",
      "Epoch 332, Loss G: -4.2720,Loss D: -0.1283\n",
      "Epoch 333, Loss G: -4.1266,Loss D:  0.0850\n",
      "Epoch 334, Loss G: -4.0145,Loss D: -0.0746\n",
      "Epoch 335, Loss G: -3.7756,Loss D: -0.1420\n",
      "Epoch 336, Loss G: -3.5519,Loss D: -0.0741\n",
      "Epoch 337, Loss G: -3.8134,Loss D: -0.0433\n",
      "Epoch 338, Loss G: -4.3386,Loss D:  0.0240\n",
      "Epoch 339, Loss G: -3.7661,Loss D:  0.2887\n",
      "Epoch 340, Loss G: -4.0357,Loss D:  0.0304\n",
      "Epoch 341, Loss G: -3.7182,Loss D: -0.2264\n",
      "Epoch 342, Loss G: -4.1436,Loss D:  0.0311\n",
      "Epoch 343, Loss G: -4.1205,Loss D: -0.0964\n",
      "Epoch 344, Loss G: -3.8922,Loss D:  0.0028\n",
      "Epoch 345, Loss G: -3.5321,Loss D:  0.0020\n",
      "Epoch 346, Loss G: -3.7158,Loss D:  0.0140\n",
      "Epoch 347, Loss G: -3.6962,Loss D:  0.0784\n",
      "Epoch 348, Loss G: -4.2587,Loss D:  0.0331\n",
      "Epoch 349, Loss G: -3.7328,Loss D: -0.0084\n",
      "Epoch 350, Loss G: -3.9038,Loss D:  0.1289\n",
      "Epoch 351, Loss G: -3.5919,Loss D:  0.0022\n",
      "Epoch 352, Loss G: -3.8642,Loss D:  0.2788\n",
      "Epoch 353, Loss G: -3.8235,Loss D:  0.0916\n",
      "Epoch 354, Loss G: -3.8870,Loss D: -0.2579\n",
      "Epoch 355, Loss G: -3.8041,Loss D: -0.0191\n",
      "Epoch 356, Loss G: -3.4568,Loss D: -0.0370\n",
      "Epoch 357, Loss G: -3.7029,Loss D:  0.0003\n",
      "Epoch 358, Loss G: -3.8505,Loss D: -0.1060\n",
      "Epoch 359, Loss G: -3.8684,Loss D: -0.0944\n",
      "Epoch 360, Loss G: -3.3692,Loss D: -0.0947\n",
      "Epoch 361, Loss G: -3.7926,Loss D: -0.0708\n",
      "Epoch 362, Loss G: -3.3646,Loss D: -0.2342\n",
      "Epoch 363, Loss G: -3.8766,Loss D: -0.1484\n",
      "Epoch 364, Loss G: -3.5978,Loss D:  0.0178\n",
      "Epoch 365, Loss G: -3.8330,Loss D: -0.1497\n",
      "Epoch 366, Loss G: -3.7731,Loss D:  0.1695\n",
      "Epoch 367, Loss G: -3.7855,Loss D:  0.0708\n",
      "Epoch 368, Loss G: -3.5506,Loss D: -0.0115\n",
      "Epoch 369, Loss G: -4.0419,Loss D:  0.0817\n",
      "Epoch 370, Loss G: -3.8079,Loss D: -0.1167\n",
      "Epoch 371, Loss G: -3.9708,Loss D:  0.1541\n",
      "Epoch 372, Loss G: -3.6745,Loss D:  0.0060\n",
      "Epoch 373, Loss G: -3.8643,Loss D:  0.0204\n",
      "Epoch 374, Loss G: -3.7905,Loss D: -0.0766\n",
      "Epoch 375, Loss G: -3.4966,Loss D:  0.3210\n",
      "Epoch 376, Loss G: -4.2382,Loss D: -0.0088\n",
      "Epoch 377, Loss G: -3.6433,Loss D:  0.1819\n",
      "Epoch 378, Loss G: -4.2082,Loss D: -0.0582\n",
      "Epoch 379, Loss G: -3.3760,Loss D: -0.2277\n",
      "Epoch 380, Loss G: -3.8304,Loss D:  0.0871\n",
      "Epoch 381, Loss G: -3.7866,Loss D: -0.0580\n",
      "Epoch 382, Loss G: -3.7480,Loss D: -0.1630\n",
      "Epoch 383, Loss G: -3.9402,Loss D: -0.1231\n",
      "Epoch 384, Loss G: -4.2847,Loss D: -0.0054\n",
      "Epoch 385, Loss G: -3.9657,Loss D: -0.0484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 386, Loss G: -3.9070,Loss D: -0.0876\n",
      "Epoch 387, Loss G: -3.9035,Loss D:  0.1134\n",
      "Epoch 388, Loss G: -3.3552,Loss D:  0.3330\n",
      "Epoch 389, Loss G: -3.6454,Loss D: -0.2314\n",
      "Epoch 390, Loss G: -3.8350,Loss D:  0.0536\n",
      "Epoch 391, Loss G: -4.0059,Loss D: -0.1647\n",
      "Epoch 392, Loss G: -3.9435,Loss D: -0.0761\n",
      "Epoch 393, Loss G: -3.8561,Loss D: -0.1877\n",
      "Epoch 394, Loss G: -3.5094,Loss D: -0.0549\n",
      "Epoch 395, Loss G: -3.9113,Loss D: -0.0693\n",
      "Epoch 396, Loss G: -3.7533,Loss D: -0.1191\n",
      "Epoch 397, Loss G: -3.8842,Loss D: -0.0239\n",
      "Epoch 398, Loss G: -3.6971,Loss D: -0.3002\n",
      "Epoch 399, Loss G: -4.2434,Loss D: -0.2540\n",
      "Epoch 400, Loss G: -3.6849,Loss D:  0.1540\n",
      "Epoch 401, Loss G: -3.8089,Loss D:  0.0804\n",
      "Epoch 402, Loss G: -3.7646,Loss D: -0.0808\n",
      "Epoch 403, Loss G: -3.4526,Loss D:  0.1484\n",
      "Epoch 404, Loss G: -3.6785,Loss D: -0.0548\n",
      "Epoch 405, Loss G: -3.4562,Loss D:  0.0338\n",
      "Epoch 406, Loss G: -3.5046,Loss D:  0.0433\n",
      "Epoch 407, Loss G: -3.6302,Loss D: -0.2957\n",
      "Epoch 408, Loss G: -3.8834,Loss D:  0.1779\n",
      "Epoch 409, Loss G: -2.7646,Loss D:  0.1764\n",
      "Epoch 410, Loss G: -4.0456,Loss D:  0.0263\n",
      "Epoch 411, Loss G: -3.3857,Loss D:  0.1090\n",
      "Epoch 412, Loss G: -3.3785,Loss D:  0.0240\n",
      "Epoch 413, Loss G: -3.3927,Loss D:  0.1844\n",
      "Epoch 414, Loss G: -3.6327,Loss D:  0.0595\n",
      "Epoch 415, Loss G: -4.0646,Loss D:  0.3530\n",
      "Epoch 416, Loss G: -3.5115,Loss D:  0.1371\n",
      "Epoch 417, Loss G: -3.8587,Loss D: -0.1966\n",
      "Epoch 418, Loss G: -3.6451,Loss D: -0.2401\n",
      "Epoch 419, Loss G: -3.7153,Loss D:  0.2910\n",
      "Epoch 420, Loss G: -3.8386,Loss D: -0.0548\n",
      "Epoch 421, Loss G: -2.9762,Loss D: -0.2150\n",
      "Epoch 422, Loss G: -3.4602,Loss D:  0.2783\n",
      "Epoch 423, Loss G: -3.6083,Loss D: -0.0617\n",
      "Epoch 424, Loss G: -3.7472,Loss D: -0.1197\n",
      "Epoch 425, Loss G: -3.8728,Loss D: -0.3578\n",
      "Epoch 426, Loss G: -3.2093,Loss D:  0.0123\n",
      "Epoch 427, Loss G: -3.6533,Loss D:  0.1541\n",
      "Epoch 428, Loss G: -3.1777,Loss D:  0.0641\n",
      "Epoch 429, Loss G: -3.6966,Loss D:  0.1226\n",
      "Epoch 430, Loss G: -3.4410,Loss D: -0.1213\n",
      "Epoch 431, Loss G: -3.2664,Loss D: -0.1231\n",
      "Epoch 432, Loss G: -3.2424,Loss D:  0.0465\n",
      "Epoch 433, Loss G: -3.3517,Loss D: -0.0957\n",
      "Epoch 434, Loss G: -3.3281,Loss D: -0.0429\n",
      "Epoch 435, Loss G: -2.7348,Loss D:  0.2328\n",
      "Epoch 436, Loss G: -3.2022,Loss D:  0.0717\n",
      "Epoch 437, Loss G: -2.8679,Loss D: -0.3012\n",
      "Epoch 438, Loss G: -3.5111,Loss D: -0.0870\n",
      "Epoch 439, Loss G: -3.2351,Loss D: -0.0541\n",
      "Epoch 440, Loss G: -2.5936,Loss D: -0.1421\n",
      "Epoch 441, Loss G: -3.2059,Loss D: -0.1195\n",
      "Epoch 442, Loss G: -2.5463,Loss D: -0.1132\n",
      "Epoch 443, Loss G: -3.3701,Loss D:  0.2255\n",
      "Epoch 444, Loss G: -3.0980,Loss D:  0.2328\n",
      "Epoch 445, Loss G: -2.8975,Loss D: -0.0362\n",
      "Epoch 446, Loss G: -3.2930,Loss D: -0.2673\n",
      "Epoch 447, Loss G: -2.7220,Loss D: -0.1015\n",
      "Epoch 448, Loss G: -3.0910,Loss D:  0.2072\n",
      "Epoch 449, Loss G: -2.7731,Loss D:  0.2152\n",
      "Epoch 450, Loss G: -2.9364,Loss D:  0.3073\n",
      "Epoch 451, Loss G: -3.3683,Loss D: -0.1508\n",
      "Epoch 452, Loss G: -3.4322,Loss D: -0.0832\n",
      "Epoch 453, Loss G: -3.3743,Loss D: -0.2219\n",
      "Epoch 454, Loss G: -3.2611,Loss D: -0.0921\n",
      "Epoch 455, Loss G: -3.9311,Loss D: -0.0189\n",
      "Epoch 456, Loss G: -3.1895,Loss D:  0.1563\n",
      "Epoch 457, Loss G: -3.6291,Loss D:  0.0399\n",
      "Epoch 458, Loss G: -3.0455,Loss D: -0.0812\n",
      "Epoch 459, Loss G: -3.3465,Loss D: -0.2131\n",
      "Epoch 460, Loss G: -3.1474,Loss D:  0.1243\n",
      "Epoch 461, Loss G: -3.8075,Loss D: -0.1442\n",
      "Epoch 462, Loss G: -3.4582,Loss D:  0.1188\n",
      "Epoch 463, Loss G: -3.2945,Loss D: -0.0443\n",
      "Epoch 464, Loss G: -3.5777,Loss D: -0.1133\n",
      "Epoch 465, Loss G: -3.2307,Loss D: -0.1335\n",
      "Epoch 466, Loss G: -4.0849,Loss D: -0.1045\n",
      "Epoch 467, Loss G: -3.4665,Loss D:  0.1385\n",
      "Epoch 468, Loss G: -3.7854,Loss D:  0.0348\n",
      "Epoch 469, Loss G: -3.2561,Loss D: -0.1288\n",
      "Epoch 470, Loss G: -3.0578,Loss D:  0.1959\n",
      "Epoch 471, Loss G: -2.9469,Loss D: -0.0492\n",
      "Epoch 472, Loss G: -3.7690,Loss D:  0.4173\n",
      "Epoch 473, Loss G: -2.9737,Loss D:  0.1363\n",
      "Epoch 474, Loss G: -3.6525,Loss D: -0.1584\n",
      "Epoch 475, Loss G: -2.5549,Loss D:  0.1204\n",
      "Epoch 476, Loss G: -4.1139,Loss D: -0.0384\n",
      "Epoch 477, Loss G: -2.9187,Loss D:  0.0194\n",
      "Epoch 478, Loss G: -3.6314,Loss D: -0.2548\n",
      "Epoch 479, Loss G: -3.1568,Loss D: -0.0289\n",
      "Epoch 480, Loss G: -3.2058,Loss D:  0.0240\n",
      "Epoch 481, Loss G: -2.7702,Loss D:  0.2351\n",
      "Epoch 482, Loss G: -3.0513,Loss D: -0.3177\n",
      "Epoch 483, Loss G: -3.4476,Loss D: -0.1763\n",
      "Epoch 484, Loss G: -3.3088,Loss D:  0.2572\n",
      "Epoch 485, Loss G: -2.9554,Loss D:  0.2778\n",
      "Epoch 486, Loss G: -3.6545,Loss D: -0.2326\n",
      "Epoch 487, Loss G: -3.0195,Loss D: -0.0318\n",
      "Epoch 488, Loss G: -3.1483,Loss D:  0.4863\n",
      "Epoch 489, Loss G: -3.6271,Loss D: -0.2926\n",
      "Epoch 490, Loss G: -3.1682,Loss D:  0.3794\n",
      "Epoch 491, Loss G: -4.1013,Loss D: -0.1031\n",
      "Epoch 492, Loss G: -3.1623,Loss D: -0.1031\n",
      "Epoch 493, Loss G: -3.7200,Loss D: -0.1384\n",
      "Epoch 494, Loss G: -3.9682,Loss D: -0.0571\n",
      "Epoch 495, Loss G: -3.3536,Loss D: -0.2267\n",
      "Epoch 496, Loss G: -2.8432,Loss D: -0.1914\n",
      "Epoch 497, Loss G: -3.2101,Loss D: -0.2255\n",
      "Epoch 498, Loss G: -3.0781,Loss D:  0.1689\n",
      "Epoch 499, Loss G: -3.1325,Loss D: -0.3674\n",
      "Epoch 500, Loss G: -3.7627,Loss D:  0.1569\n"
     ]
    }
   ],
   "source": [
    "# train CTGAN and generate fake data\n",
    "ctgan = CTGANSynthesizer(verbose=True)\n",
    "ctgan.fit(GAN_data_train, cov_discrete_columns, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    33311\n",
       "1    16689\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create fake samples\n",
    "samples = ctgan.sample(50000)\n",
    "samples['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adult_data_all.head()\n",
    "fake_X = samples.drop([\"label\"],axis=1)\n",
    "fake_X, fake_y = fake_X,samples.loc[:,\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 54)\n",
      "(50000,)\n",
      "(15000, 54)\n"
     ]
    }
   ],
   "source": [
    "# check the training data shape to agree with the original data (# of features/cols)\n",
    "print(fake_X.shape)\n",
    "print(fake_y.shape)\n",
    "print(orig_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML scores for the GAN generated data:\n",
      "Decision Tree Acc:  0.5123333333333333 f-1:  0.03457832915401874 AUC: 0.7046135806156001\n",
      "Linear SVM Acc:  0.4600666666666667 f-1:  0.5176008100542021 AUC: 0.5448187253054565\n",
      "Random Forest Acc:  0.5467333333333333 f-1:  0.20563149900689331 AUC: 0.6983029294314438\n",
      "Logistic Regression Acc:  0.5231333333333333 f-1:  0.07952644447304079 AUC: 0.6522101501394811\n",
      "MLP Acc:  0.5107333333333334 f-1:  0.00784101662836285 AUC: 0.5912547680994296\n"
     ]
    }
   ],
   "source": [
    "# train a classifier on the CTGAN generated data\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5,random_state=0),\n",
    "    SVC(kernel = 'linear', max_iter=1000, C=0.025, random_state=0, probability=True),\n",
    "    RandomForestClassifier(n_estimators=200, random_state=0),\n",
    "    LogisticRegression(max_iter=1000, random_state=0),\n",
    "    MLPClassifier(alpha=1, max_iter=1000, random_state=0)]\n",
    "\n",
    "print('ML scores for the GAN generated data:')\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(fake_X, fake_y)\n",
    "    score = clf.score(orig_X_test, orig_y_test)\n",
    "    y_pred = clf.predict(orig_X_test)\n",
    "    Acc = accuracy_score(orig_y_test, y_pred)\n",
    "    fscore = f1_score(orig_y_test, y_pred, average='binary')\n",
    "    AUC = roc_auc_score(orig_y_test, clf.predict_proba(orig_X_test)[:, 1])\n",
    "    print(name,'Acc: ', Acc, 'f-1: ', fscore, 'AUC:', AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
